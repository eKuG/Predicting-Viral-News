=================================================
{   'activation': 'relu',
    'batch_size': 64,
    'hidden_layer_width': 16,
    'loss': 'binary_crossentropy',
    'max_epochs': 100,
    'num_hidden': 1,
    'optimizer': 'adam'}

   32/13083 [..............................] - ETA: 1s
  800/13083 [>.............................] - ETA: 0s
 1920/13083 [===>..........................] - ETA: 0s
 3040/13083 [=====>........................] - ETA: 0s
 4224/13083 [========>.....................] - ETA: 0s
 5376/13083 [===========>..................] - ETA: 0s
 6464/13083 [=============>................] - ETA: 0s
 7296/13083 [===============>..............] - ETA: 0s
 8320/13083 [==================>...........] - ETA: 0s
 9440/13083 [====================>.........] - ETA: 0s
10560/13083 [=======================>......] - ETA: 0s
11584/13083 [=========================>....] - ETA: 0s
12640/13083 [===========================>..] - ETA: 0s
13083/13083 [==============================] - 1s 48us/step

   32/26561 [..............................] - ETA: 1s
 1248/26561 [>.............................] - ETA: 1s
 2144/26561 [=>............................] - ETA: 1s
 3360/26561 [==>...........................] - ETA: 1s
 4608/26561 [====>.........................] - ETA: 0s
 5856/26561 [=====>........................] - ETA: 0s
 7104/26561 [=======>......................] - ETA: 0s
 8352/26561 [========>.....................] - ETA: 0s
 9600/26561 [=========>....................] - ETA: 0s
10848/26561 [===========>..................] - ETA: 0s
12096/26561 [============>.................] - ETA: 0s
13344/26561 [==============>...............] - ETA: 0s
14592/26561 [===============>..............] - ETA: 0s
15808/26561 [================>.............] - ETA: 0s
17024/26561 [==================>...........] - ETA: 0s
18272/26561 [===================>..........] - ETA: 0s
19520/26561 [=====================>........] - ETA: 0s
20768/26561 [======================>.......] - ETA: 0s
22016/26561 [=======================>......] - ETA: 0s
23264/26561 [=========================>....] - ETA: 0s
24416/26561 [==========================>...] - ETA: 0s
25152/26561 [===========================>..] - ETA: 0s
25600/26561 [===========================>..] - ETA: 0s
26048/26561 [============================>.] - ETA: 0s
26528/26561 [============================>.] - ETA: 0s
26561/26561 [==============================] - 1s 46us/step

acc: 65.70%
NeuralNet Accuracy: 65.70% (61.62%) with {'loss': 'binary_crossentropy', 'optimizer': 'adam', 'num_hidden': 1, 'activation': 'relu', 'batch_size': 64, 'val_score': 65.69594130196775, 'train_score': 67.58781672376793, 'max_epochs': 100, 'hidden_layer_width': 16}
NEW MAX: 65.695941302
=================================================
{   'activation': 'relu',
    'batch_size': 512,
    'hidden_layer_width': 16,
    'loss': 'binary_crossentropy',
    'max_epochs': 100,
    'num_hidden': 1,
    'optimizer': 'adam'}

   32/13083 [..............................] - ETA: 0s
 1056/13083 [=>............................] - ETA: 0s
 2112/13083 [===>..........................] - ETA: 0s
 3296/13083 [======>.......................] - ETA: 0s
 4160/13083 [========>.....................] - ETA: 0s
 5344/13083 [===========>..................] - ETA: 0s
 6560/13083 [==============>...............] - ETA: 0s
 7808/13083 [================>.............] - ETA: 0s
 9056/13083 [===================>..........] - ETA: 0s
10304/13083 [======================>.......] - ETA: 0s
11552/13083 [=========================>....] - ETA: 0s
12800/13083 [============================>.] - ETA: 0s
13083/13083 [==============================] - 1s 44us/step

   32/26561 [..............................] - ETA: 1s
 1280/26561 [>.............................] - ETA: 1s
 2464/26561 [=>............................] - ETA: 1s
 3680/26561 [===>..........................] - ETA: 0s
 4896/26561 [====>.........................] - ETA: 0s
 6112/26561 [=====>........................] - ETA: 0s
 7328/26561 [=======>......................] - ETA: 0s
 8544/26561 [========>.....................] - ETA: 0s
 9792/26561 [==========>...................] - ETA: 0s
11040/26561 [===========>..................] - ETA: 0s
12288/26561 [============>.................] - ETA: 0s
13408/26561 [==============>...............] - ETA: 0s
14048/26561 [==============>...............] - ETA: 0s
14464/26561 [===============>..............] - ETA: 0s
14912/26561 [===============>..............] - ETA: 0s
15360/26561 [================>.............] - ETA: 0s
15840/26561 [================>.............] - ETA: 0s
16960/26561 [==================>...........] - ETA: 0s
18144/26561 [===================>..........] - ETA: 0s
19328/26561 [====================>.........] - ETA: 0s
20480/26561 [======================>.......] - ETA: 0s
21280/26561 [=======================>......] - ETA: 0s
21664/26561 [=======================>......] - ETA: 0s
22080/26561 [=======================>......] - ETA: 0s
22688/26561 [========================>.....] - ETA: 0s
23488/26561 [=========================>....] - ETA: 0s
24576/26561 [==========================>...] - ETA: 0s
25728/26561 [============================>.] - ETA: 0s
26561/26561 [==============================] - 1s 53us/step

acc: 65.72%
NeuralNet Accuracy: 65.72% (61.81%) with {'loss': 'binary_crossentropy', 'optimizer': 'adam', 'num_hidden': 1, 'activation': 'relu', 'batch_size': 512, 'val_score': 65.71887182385734, 'train_score': 67.91912955084523, 'max_epochs': 100, 'hidden_layer_width': 16}
NEW MAX: 65.7188718239
=================================================
{   'activation': 'relu',
    'batch_size': 64,
    'hidden_layer_width': 32,
    'loss': 'binary_crossentropy',
    'max_epochs': 100,
    'num_hidden': 1,
    'optimizer': 'adam'}

   32/13083 [..............................] - ETA: 2s
  448/13083 [>.............................] - ETA: 1s
 1312/13083 [==>...........................] - ETA: 0s
 2464/13083 [====>.........................] - ETA: 0s
 3616/13083 [=======>......................] - ETA: 0s
 4768/13083 [=========>....................] - ETA: 0s
 5888/13083 [============>.................] - ETA: 0s
 6272/13083 [=============>................] - ETA: 0s
 6688/13083 [==============>...............] - ETA: 0s
 7104/13083 [===============>..............] - ETA: 0s
 8032/13083 [=================>............] - ETA: 0s
 8768/13083 [===================>..........] - ETA: 0s
 9920/13083 [=====================>........] - ETA: 0s
11072/13083 [========================>.....] - ETA: 0s
12224/13083 [===========================>..] - ETA: 0s
13083/13083 [==============================] - 1s 58us/step

   32/26561 [..............................] - ETA: 1s
 1184/26561 [>.............................] - ETA: 1s
 2144/26561 [=>............................] - ETA: 1s
 3296/26561 [==>...........................] - ETA: 1s
 4448/26561 [====>.........................] - ETA: 1s
 5536/26561 [=====>........................] - ETA: 0s
 6496/26561 [======>.......................] - ETA: 0s
 7520/26561 [=======>......................] - ETA: 0s
 8704/26561 [========>.....................] - ETA: 0s
 9568/26561 [=========>....................] - ETA: 0s
10752/26561 [===========>..................] - ETA: 0s
11968/26561 [============>.................] - ETA: 0s
13184/26561 [=============>................] - ETA: 0s
14400/26561 [===============>..............] - ETA: 0s
15616/26561 [================>.............] - ETA: 0s
16832/26561 [==================>...........] - ETA: 0s
18048/26561 [===================>..........] - ETA: 0s
19296/26561 [====================>.........] - ETA: 0s
20512/26561 [======================>.......] - ETA: 0s
21728/26561 [=======================>......] - ETA: 0s
22944/26561 [========================>.....] - ETA: 0s
24128/26561 [==========================>...] - ETA: 0s
25280/26561 [===========================>..] - ETA: 0s
26464/26561 [============================>.] - ETA: 0s
26561/26561 [==============================] - 1s 44us/step

acc: 65.61%
NeuralNet Accuracy: 65.61% (61.80%) with {'loss': 'binary_crossentropy', 'optimizer': 'adam', 'num_hidden': 1, 'activation': 'relu', 'batch_size': 64, 'val_score': 65.61186272216155, 'train_score': 68.12620006776852, 'max_epochs': 100, 'hidden_layer_width': 32}
MAX: 65.7188718239
=================================================
{   'activation': 'relu',
    'batch_size': 512,
    'hidden_layer_width': 32,
    'loss': 'binary_crossentropy',
    'max_epochs': 100,
    'num_hidden': 1,
    'optimizer': 'adam'}

   32/13083 [..............................] - ETA: 0s
 1216/13083 [=>............................] - ETA: 0s
 2400/13083 [====>.........................] - ETA: 0s
 3584/13083 [=======>......................] - ETA: 0s
 4736/13083 [=========>....................] - ETA: 0s
 5408/13083 [===========>..................] - ETA: 0s
 5824/13083 [============>.................] - ETA: 0s
 6208/13083 [=============>................] - ETA: 0s
 6912/13083 [==============>...............] - ETA: 0s
 7712/13083 [================>.............] - ETA: 0s
 8800/13083 [===================>..........] - ETA: 0s
 9952/13083 [=====================>........] - ETA: 0s
11104/13083 [========================>.....] - ETA: 0s
12256/13083 [===========================>..] - ETA: 0s
13083/13083 [==============================] - 1s 54us/step

   32/26561 [..............................] - ETA: 1s
  992/26561 [>.............................] - ETA: 1s
 2144/26561 [=>............................] - ETA: 1s
 3296/26561 [==>...........................] - ETA: 1s
 4416/26561 [===>..........................] - ETA: 1s
 5344/26561 [=====>........................] - ETA: 1s
 6368/26561 [======>.......................] - ETA: 0s
 7552/26561 [=======>......................] - ETA: 0s
 8640/26561 [========>.....................] - ETA: 0s
 9632/26561 [=========>....................] - ETA: 0s
10848/26561 [===========>..................] - ETA: 0s
12064/26561 [============>.................] - ETA: 0s
13280/26561 [=============>................] - ETA: 0s
14496/26561 [===============>..............] - ETA: 0s
15712/26561 [================>.............] - ETA: 0s
16928/26561 [==================>...........] - ETA: 0s
18144/26561 [===================>..........] - ETA: 0s
19360/26561 [====================>.........] - ETA: 0s
20576/26561 [======================>.......] - ETA: 0s
21792/26561 [=======================>......] - ETA: 0s
23008/26561 [========================>.....] - ETA: 0s
24224/26561 [==========================>...] - ETA: 0s
25472/26561 [===========================>..] - ETA: 0s
26561/26561 [==============================] - 1s 44us/step

acc: 66.10%
NeuralNet Accuracy: 66.10% (61.58%) with {'loss': 'binary_crossentropy', 'optimizer': 'adam', 'num_hidden': 1, 'activation': 'relu', 'batch_size': 512, 'val_score': 66.1010471567925, 'train_score': 68.12620006776852, 'max_epochs': 100, 'hidden_layer_width': 32}
NEW MAX: 66.1010471568
=================================================
{   'activation': 'relu',
    'batch_size': 64,
    'hidden_layer_width': 64,
    'loss': 'binary_crossentropy',
    'max_epochs': 100,
    'num_hidden': 1,
    'optimizer': 'adam'}

   32/13083 [..............................] - ETA: 0s
 1120/13083 [=>............................] - ETA: 0s
 2240/13083 [====>.........................] - ETA: 0s
 3360/13083 [======>.......................] - ETA: 0s
 4480/13083 [=========>....................] - ETA: 0s
 5440/13083 [===========>..................] - ETA: 0s
 6624/13083 [==============>...............] - ETA: 0s
 7808/13083 [================>.............] - ETA: 0s
 8928/13083 [===================>..........] - ETA: 0s
 9920/13083 [=====================>........] - ETA: 0s
10976/13083 [========================>.....] - ETA: 0s
12160/13083 [==========================>...] - ETA: 0s
13083/13083 [==============================] - 1s 47us/step

   32/26561 [..............................] - ETA: 2s
 1216/26561 [>.............................] - ETA: 1s
 2432/26561 [=>............................] - ETA: 1s
 3648/26561 [===>..........................] - ETA: 0s
 4864/26561 [====>.........................] - ETA: 0s
 6080/26561 [=====>........................] - ETA: 0s
 7296/26561 [=======>......................] - ETA: 0s
 8512/26561 [========>.....................] - ETA: 0s
 9728/26561 [=========>....................] - ETA: 0s
10944/26561 [===========>..................] - ETA: 0s
12160/26561 [============>.................] - ETA: 0s
13376/26561 [==============>...............] - ETA: 0s
14592/26561 [===============>..............] - ETA: 0s
15808/26561 [================>.............] - ETA: 0s
17024/26561 [==================>...........] - ETA: 0s
18240/26561 [===================>..........] - ETA: 0s
19456/26561 [====================>.........] - ETA: 0s
20672/26561 [======================>.......] - ETA: 0s
21888/26561 [=======================>......] - ETA: 0s
22752/26561 [========================>.....] - ETA: 0s
23200/26561 [=========================>....] - ETA: 0s
23616/26561 [=========================>....] - ETA: 0s
24064/26561 [==========================>...] - ETA: 0s
24448/26561 [==========================>...] - ETA: 0s
25408/26561 [===========================>..] - ETA: 0s
26561/26561 [==============================] - 1s 48us/step

acc: 65.90%
NeuralNet Accuracy: 65.90% (61.99%) with {'loss': 'binary_crossentropy', 'optimizer': 'adam', 'num_hidden': 1, 'activation': 'relu', 'batch_size': 64, 'val_score': 65.90231598348399, 'train_score': 69.32344414743422, 'max_epochs': 100, 'hidden_layer_width': 64}
MAX: 66.1010471568
=================================================
{   'activation': 'relu',
    'batch_size': 512,
    'hidden_layer_width': 64,
    'loss': 'binary_crossentropy',
    'max_epochs': 100,
    'num_hidden': 1,
    'optimizer': 'adam'}

   32/13083 [..............................] - ETA: 2s
  480/13083 [>.............................] - ETA: 1s
  960/13083 [=>............................] - ETA: 1s
 2112/13083 [===>..........................] - ETA: 0s
 3296/13083 [======>.......................] - ETA: 0s
 4480/13083 [=========>....................] - ETA: 0s
 5664/13083 [===========>..................] - ETA: 0s
 6816/13083 [==============>...............] - ETA: 0s
 7264/13083 [===============>..............] - ETA: 0s
 7648/13083 [================>.............] - ETA: 0s
 8032/13083 [=================>............] - ETA: 0s
 8960/13083 [===================>..........] - ETA: 0s
 9696/13083 [=====================>........] - ETA: 0s
10784/13083 [=======================>......] - ETA: 0s
11872/13083 [==========================>...] - ETA: 0s
12960/13083 [============================>.] - ETA: 0s
13083/13083 [==============================] - 1s 60us/step

   32/26561 [..............................] - ETA: 1s
 1120/26561 [>.............................] - ETA: 1s
 2080/26561 [=>............................] - ETA: 1s
 3136/26561 [==>...........................] - ETA: 1s
 4288/26561 [===>..........................] - ETA: 1s
 5440/26561 [=====>........................] - ETA: 0s
 6432/26561 [======>.......................] - ETA: 0s
 7488/26561 [=======>......................] - ETA: 0s
 8608/26561 [========>.....................] - ETA: 0s
 9600/26561 [=========>....................] - ETA: 0s
10560/26561 [==========>...................] - ETA: 0s
11744/26561 [============>.................] - ETA: 0s
12960/26561 [=============>................] - ETA: 0s
14176/26561 [===============>..............] - ETA: 0s
15392/26561 [================>.............] - ETA: 0s
16608/26561 [=================>............] - ETA: 0s
17824/26561 [===================>..........] - ETA: 0s
19040/26561 [====================>.........] - ETA: 0s
20224/26561 [=====================>........] - ETA: 0s
21408/26561 [=======================>......] - ETA: 0s
22592/26561 [========================>.....] - ETA: 0s
23808/26561 [=========================>....] - ETA: 0s
25024/26561 [===========================>..] - ETA: 0s
26240/26561 [============================>.] - ETA: 0s
26561/26561 [==============================] - 1s 45us/step

acc: 65.47%
NeuralNet Accuracy: 65.47% (62.07%) with {'loss': 'binary_crossentropy', 'optimizer': 'adam', 'num_hidden': 1, 'activation': 'relu', 'batch_size': 512, 'val_score': 65.46663609218372, 'train_score': 69.18790708181169, 'max_epochs': 100, 'hidden_layer_width': 64}
MAX: 66.1010471568
=================================================
{   'activation': 'relu',
    'batch_size': 64,
    'hidden_layer_width': 16,
    'loss': 'binary_crossentropy',
    'max_epochs': 100,
    'num_hidden': 2,
    'optimizer': 'adam'}

   32/13083 [..............................] - ETA: 1s
  928/13083 [=>............................] - ETA: 0s
 1984/13083 [===>..........................] - ETA: 0s
 3136/13083 [======>.......................] - ETA: 0s
 4288/13083 [========>.....................] - ETA: 0s
 5472/13083 [===========>..................] - ETA: 0s
 6656/13083 [==============>...............] - ETA: 0s
 7840/13083 [================>.............] - ETA: 0s
 9024/13083 [===================>..........] - ETA: 0s
10208/13083 [======================>.......] - ETA: 0s
11392/13083 [=========================>....] - ETA: 0s
12576/13083 [===========================>..] - ETA: 0s
13083/13083 [==============================] - 1s 45us/step

   32/26561 [..............................] - ETA: 1s
 1216/26561 [>.............................] - ETA: 1s
 2400/26561 [=>............................] - ETA: 1s
 3584/26561 [===>..........................] - ETA: 0s
 4768/26561 [====>.........................] - ETA: 0s
 5952/26561 [=====>........................] - ETA: 0s
 7104/26561 [=======>......................] - ETA: 0s
 8288/26561 [========>.....................] - ETA: 0s
 9344/26561 [=========>....................] - ETA: 0s
 9856/26561 [==========>...................] - ETA: 0s
10272/26561 [==========>...................] - ETA: 0s
10688/26561 [===========>..................] - ETA: 0s
11072/26561 [===========>..................] - ETA: 0s
11584/26561 [============>.................] - ETA: 0s
12704/26561 [=============>................] - ETA: 0s
13856/26561 [==============>...............] - ETA: 0s
15008/26561 [===============>..............] - ETA: 0s
16160/26561 [=================>............] - ETA: 0s
17280/26561 [==================>...........] - ETA: 0s
17856/26561 [===================>..........] - ETA: 0s
18208/26561 [===================>..........] - ETA: 0s
18592/26561 [===================>..........] - ETA: 0s
19584/26561 [=====================>........] - ETA: 0s
20320/26561 [=====================>........] - ETA: 0s
21408/26561 [=======================>......] - ETA: 0s
22496/26561 [========================>.....] - ETA: 0s
23584/26561 [=========================>....] - ETA: 0s
24672/26561 [==========================>...] - ETA: 0s
25728/26561 [============================>.] - ETA: 0s
26561/26561 [==============================] - 1s 56us/step

acc: 66.02%
NeuralNet Accuracy: 66.02% (61.82%) with {'loss': 'binary_crossentropy', 'optimizer': 'adam', 'num_hidden': 2, 'activation': 'relu', 'batch_size': 64, 'val_score': 66.02461209157224, 'train_score': 67.8363013440759, 'max_epochs': 100, 'hidden_layer_width': 16}
MAX: 66.1010471568
=================================================
{   'activation': 'relu',
    'batch_size': 512,
    'hidden_layer_width': 16,
    'loss': 'binary_crossentropy',
    'max_epochs': 100,
    'num_hidden': 2,
    'optimizer': 'adam'}

   32/13083 [..............................] - ETA: 1s
  704/13083 [>.............................] - ETA: 0s
 1408/13083 [==>...........................] - ETA: 0s
 2464/13083 [====>.........................] - ETA: 0s
 3552/13083 [=======>......................] - ETA: 0s
 4672/13083 [=========>....................] - ETA: 0s
 5792/13083 [============>.................] - ETA: 0s
 6912/13083 [==============>...............] - ETA: 0s
 7840/13083 [================>.............] - ETA: 0s
 8960/13083 [===================>..........] - ETA: 0s
10080/13083 [======================>.......] - ETA: 0s
11104/13083 [========================>.....] - ETA: 0s
12032/13083 [==========================>...] - ETA: 0s
13024/13083 [============================>.] - ETA: 0s
13083/13083 [==============================] - 1s 51us/step

   32/26561 [..............................] - ETA: 1s
 1088/26561 [>.............................] - ETA: 1s
 1984/26561 [=>............................] - ETA: 1s
 3136/26561 [==>...........................] - ETA: 1s
 4288/26561 [===>..........................] - ETA: 1s
 5440/26561 [=====>........................] - ETA: 0s
 6624/26561 [======>.......................] - ETA: 0s
 7776/26561 [=======>......................] - ETA: 0s
 8928/26561 [=========>....................] - ETA: 0s
10048/26561 [==========>...................] - ETA: 0s
11168/26561 [===========>..................] - ETA: 0s
12288/26561 [============>.................] - ETA: 0s
13440/26561 [==============>...............] - ETA: 0s
14592/26561 [===============>..............] - ETA: 0s
15744/26561 [================>.............] - ETA: 0s
16896/26561 [==================>...........] - ETA: 0s
18080/26561 [===================>..........] - ETA: 0s
19264/26561 [====================>.........] - ETA: 0s
20448/26561 [======================>.......] - ETA: 0s
21632/26561 [=======================>......] - ETA: 0s
22720/26561 [========================>.....] - ETA: 0s
23392/26561 [=========================>....] - ETA: 0s
23776/26561 [=========================>....] - ETA: 0s
24160/26561 [==========================>...] - ETA: 0s
24576/26561 [==========================>...] - ETA: 0s
25024/26561 [===========================>..] - ETA: 0s
26144/26561 [============================>.] - ETA: 0s
26561/26561 [==============================] - 1s 51us/step

acc: 65.96%
NeuralNet Accuracy: 65.96% (61.74%) with {'loss': 'binary_crossentropy', 'optimizer': 'adam', 'num_hidden': 2, 'activation': 'relu', 'batch_size': 512, 'val_score': 65.95582053455968, 'train_score': 68.07725612740484, 'max_epochs': 100, 'hidden_layer_width': 16}
MAX: 66.1010471568
=================================================
{   'activation': 'relu',
    'batch_size': 64,
    'hidden_layer_width': 32,
    'loss': 'binary_crossentropy',
    'max_epochs': 100,
    'num_hidden': 2,
    'optimizer': 'adam'}

   32/13083 [..............................] - ETA: 2s
  960/13083 [=>............................] - ETA: 0s
 1856/13083 [===>..........................] - ETA: 0s
 2944/13083 [=====>........................] - ETA: 0s
 4032/13083 [========>.....................] - ETA: 0s
 5120/13083 [==========>...................] - ETA: 0s
 6208/13083 [=============>................] - ETA: 0s
 7136/13083 [===============>..............] - ETA: 0s
 8224/13083 [=================>............] - ETA: 0s
 9344/13083 [====================>.........] - ETA: 0s
10368/13083 [======================>.......] - ETA: 0s
11296/13083 [========================>.....] - ETA: 0s
12288/13083 [===========================>..] - ETA: 0s
13083/13083 [==============================] - 1s 50us/step

   32/26561 [..............................] - ETA: 1s
  864/26561 [..............................] - ETA: 1s
 2016/26561 [=>............................] - ETA: 1s
 3168/26561 [==>...........................] - ETA: 1s
 4320/26561 [===>..........................] - ETA: 1s
 5472/26561 [=====>........................] - ETA: 0s
 6624/26561 [======>.......................] - ETA: 0s
 7776/26561 [=======>......................] - ETA: 0s
 8896/26561 [=========>....................] - ETA: 0s
10048/26561 [==========>...................] - ETA: 0s
11200/26561 [===========>..................] - ETA: 0s
12352/26561 [============>.................] - ETA: 0s
13504/26561 [==============>...............] - ETA: 0s
14656/26561 [===============>..............] - ETA: 0s
15808/26561 [================>.............] - ETA: 0s
16960/26561 [==================>...........] - ETA: 0s
18112/26561 [===================>..........] - ETA: 0s
19264/26561 [====================>.........] - ETA: 0s
20416/26561 [======================>.......] - ETA: 0s
21568/26561 [=======================>......] - ETA: 0s
22304/26561 [========================>.....] - ETA: 0s
22688/26561 [========================>.....] - ETA: 0s
23104/26561 [=========================>....] - ETA: 0s
23456/26561 [=========================>....] - ETA: 0s
23840/26561 [=========================>....] - ETA: 0s
24736/26561 [==========================>...] - ETA: 0s
25856/26561 [============================>.] - ETA: 0s
26561/26561 [==============================] - 1s 51us/step

acc: 66.40%
NeuralNet Accuracy: 66.40% (61.65%) with {'loss': 'binary_crossentropy', 'optimizer': 'adam', 'num_hidden': 2, 'activation': 'relu', 'batch_size': 64, 'val_score': 66.39914393133411, 'train_score': 68.97707164639885, 'max_epochs': 100, 'hidden_layer_width': 32}
NEW MAX: 66.3991439313
=================================================
{   'activation': 'relu',
    'batch_size': 512,
    'hidden_layer_width': 32,
    'loss': 'binary_crossentropy',
    'max_epochs': 100,
    'num_hidden': 2,
    'optimizer': 'adam'}

   32/13083 [..............................] - ETA: 1s
 1056/13083 [=>............................] - ETA: 0s
 2080/13083 [===>..........................] - ETA: 0s
 3040/13083 [=====>........................] - ETA: 0s
 4192/13083 [========>.....................] - ETA: 0s
 5344/13083 [===========>..................] - ETA: 0s
 6496/13083 [=============>................] - ETA: 0s
 7648/13083 [================>.............] - ETA: 0s
 8800/13083 [===================>..........] - ETA: 0s
 9952/13083 [=====================>........] - ETA: 0s
11104/13083 [========================>.....] - ETA: 0s
12256/13083 [===========================>..] - ETA: 0s
13083/13083 [==============================] - 1s 46us/step

   32/26561 [..............................] - ETA: 1s
 1184/26561 [>.............................] - ETA: 1s
 2336/26561 [=>............................] - ETA: 1s
 3456/26561 [==>...........................] - ETA: 1s
 4608/26561 [====>.........................] - ETA: 0s
 5760/26561 [=====>........................] - ETA: 0s
 6912/26561 [======>.......................] - ETA: 0s
 8064/26561 [========>.....................] - ETA: 0s
 9216/26561 [=========>....................] - ETA: 0s
10272/26561 [==========>...................] - ETA: 0s
10944/26561 [===========>..................] - ETA: 0s
11328/26561 [===========>..................] - ETA: 0s
11712/26561 [============>.................] - ETA: 0s
12096/26561 [============>.................] - ETA: 0s
12544/26561 [=============>................] - ETA: 0s
13632/26561 [==============>...............] - ETA: 0s
14240/26561 [===============>..............] - ETA: 0s
14560/26561 [===============>..............] - ETA: 0s
14944/26561 [===============>..............] - ETA: 0s
15712/26561 [================>.............] - ETA: 0s
16416/26561 [=================>............] - ETA: 0s
17472/26561 [==================>...........] - ETA: 0s
18528/26561 [===================>..........] - ETA: 0s
19616/26561 [=====================>........] - ETA: 0s
20704/26561 [======================>.......] - ETA: 0s
21792/26561 [=======================>......] - ETA: 0s
22688/26561 [========================>.....] - ETA: 0s
23776/26561 [=========================>....] - ETA: 0s
24896/26561 [===========================>..] - ETA: 0s
25920/26561 [============================>.] - ETA: 0s
26561/26561 [==============================] - 2s 57us/step

acc: 65.83%
NeuralNet Accuracy: 65.83% (61.91%) with {'loss': 'binary_crossentropy', 'optimizer': 'adam', 'num_hidden': 2, 'activation': 'relu', 'batch_size': 512, 'val_score': 65.8258809182637, 'train_score': 68.7097624336433, 'max_epochs': 100, 'hidden_layer_width': 32}
MAX: 66.3991439313
=================================================
{   'activation': 'relu',
    'batch_size': 64,
    'hidden_layer_width': 64,
    'loss': 'binary_crossentropy',
    'max_epochs': 100,
    'num_hidden': 2,
    'optimizer': 'adam'}

   32/13083 [..............................] - ETA: 1s
 1056/13083 [=>............................] - ETA: 0s
 2016/13083 [===>..........................] - ETA: 0s
 3040/13083 [=====>........................] - ETA: 0s
 3968/13083 [========>.....................] - ETA: 0s
 4864/13083 [==========>...................] - ETA: 0s
 5984/13083 [============>.................] - ETA: 0s
 7104/13083 [===============>..............] - ETA: 0s
 8224/13083 [=================>............] - ETA: 0s
 9344/13083 [====================>.........] - ETA: 0s
10464/13083 [======================>.......] - ETA: 0s
11584/13083 [=========================>....] - ETA: 0s
12704/13083 [============================>.] - ETA: 0s
13083/13083 [==============================] - 1s 49us/step

   32/26561 [..............................] - ETA: 1s
 1152/26561 [>.............................] - ETA: 1s
 2272/26561 [=>............................] - ETA: 1s
 3392/26561 [==>...........................] - ETA: 1s
 4512/26561 [====>.........................] - ETA: 1s
 5632/26561 [=====>........................] - ETA: 0s
 6752/26561 [======>.......................] - ETA: 0s
 7872/26561 [=======>......................] - ETA: 0s
 8992/26561 [=========>....................] - ETA: 0s
10112/26561 [==========>...................] - ETA: 0s
11200/26561 [===========>..................] - ETA: 0s
12032/26561 [============>.................] - ETA: 0s
12416/26561 [=============>................] - ETA: 0s
12832/26561 [=============>................] - ETA: 0s
13216/26561 [=============>................] - ETA: 0s
13568/26561 [==============>...............] - ETA: 0s
14496/26561 [===============>..............] - ETA: 0s
15616/26561 [================>.............] - ETA: 0s
16736/26561 [=================>............] - ETA: 0s
17856/26561 [===================>..........] - ETA: 0s
18976/26561 [====================>.........] - ETA: 0s
19744/26561 [=====================>........] - ETA: 0s
20064/26561 [=====================>........] - ETA: 0s
20416/26561 [======================>.......] - ETA: 0s
21024/26561 [======================>.......] - ETA: 0s
21728/26561 [=======================>......] - ETA: 0s
22752/26561 [========================>.....] - ETA: 0s
23808/26561 [=========================>....] - ETA: 0s
24896/26561 [===========================>..] - ETA: 0s
25952/26561 [============================>.] - ETA: 0s
26561/26561 [==============================] - 2s 57us/step

acc: 65.22%
NeuralNet Accuracy: 65.22% (62.88%) with {'loss': 'binary_crossentropy', 'optimizer': 'adam', 'num_hidden': 2, 'activation': 'relu', 'batch_size': 64, 'val_score': 65.22204387327368, 'train_score': 70.30608787319754, 'max_epochs': 100, 'hidden_layer_width': 64}
MAX: 66.3991439313
=================================================
{   'activation': 'relu',
    'batch_size': 512,
    'hidden_layer_width': 64,
    'loss': 'binary_crossentropy',
    'max_epochs': 100,
    'num_hidden': 2,
    'optimizer': 'adam'}

   32/13083 [..............................] - ETA: 1s
  448/13083 [>.............................] - ETA: 1s
  832/13083 [>.............................] - ETA: 1s
 1280/13083 [=>............................] - ETA: 1s
 1728/13083 [==>...........................] - ETA: 1s
 2656/13083 [=====>........................] - ETA: 1s
 3776/13083 [=======>......................] - ETA: 0s
 4896/13083 [==========>...................] - ETA: 0s
 5984/13083 [============>.................] - ETA: 0s
 7072/13083 [===============>..............] - ETA: 0s
 7808/13083 [================>.............] - ETA: 0s
 8160/13083 [=================>............] - ETA: 0s
 8576/13083 [==================>...........] - ETA: 0s
 9248/13083 [====================>.........] - ETA: 0s
 9984/13083 [=====================>........] - ETA: 0s
11040/13083 [========================>.....] - ETA: 0s
12128/13083 [==========================>...] - ETA: 0s
13083/13083 [==============================] - 1s 67us/step

   32/26561 [..............................] - ETA: 1s
 1152/26561 [>.............................] - ETA: 1s
 2272/26561 [=>............................] - ETA: 1s
 3200/26561 [==>...........................] - ETA: 1s
 4320/26561 [===>..........................] - ETA: 1s
 5440/26561 [=====>........................] - ETA: 1s
 6496/26561 [======>.......................] - ETA: 0s
 7424/26561 [=======>......................] - ETA: 0s
 8416/26561 [========>.....................] - ETA: 0s
 9536/26561 [=========>....................] - ETA: 0s
10368/26561 [==========>...................] - ETA: 0s
11488/26561 [===========>..................] - ETA: 0s
12640/26561 [=============>................] - ETA: 0s
13792/26561 [==============>...............] - ETA: 0s
14944/26561 [===============>..............] - ETA: 0s
16096/26561 [=================>............] - ETA: 0s
17248/26561 [==================>...........] - ETA: 0s
18400/26561 [===================>..........] - ETA: 0s
19552/26561 [=====================>........] - ETA: 0s
20704/26561 [======================>.......] - ETA: 0s
21856/26561 [=======================>......] - ETA: 0s
23008/26561 [========================>.....] - ETA: 0s
24160/26561 [==========================>...] - ETA: 0s
25312/26561 [===========================>..] - ETA: 0s
26464/26561 [============================>.] - ETA: 0s
26561/26561 [==============================] - 1s 46us/step

acc: 66.20%
NeuralNet Accuracy: 66.20% (62.04%) with {'loss': 'binary_crossentropy', 'optimizer': 'adam', 'num_hidden': 2, 'activation': 'relu', 'batch_size': 512, 'val_score': 66.20041274891382, 'train_score': 69.76017469221792, 'max_epochs': 100, 'hidden_layer_width': 64}
MAX: 66.3991439313
=================================================
{   'activation': 'relu',
    'batch_size': 64,
    'hidden_layer_width': 16,
    'loss': 'binary_crossentropy',
    'max_epochs': 100,
    'num_hidden': 3,
    'optimizer': 'adam'}

   32/13083 [..............................] - ETA: 1s
 1152/13083 [=>............................] - ETA: 0s
 2272/13083 [====>.........................] - ETA: 0s
 3360/13083 [======>.......................] - ETA: 0s
 4448/13083 [=========>....................] - ETA: 0s
 5536/13083 [===========>..................] - ETA: 0s
 6656/13083 [==============>...............] - ETA: 0s
 7776/13083 [================>.............] - ETA: 0s
 8896/13083 [===================>..........] - ETA: 0s
10016/13083 [=====================>........] - ETA: 0s
11136/13083 [========================>.....] - ETA: 0s
12256/13083 [===========================>..] - ETA: 0s
13083/13083 [==============================] - 1s 46us/step

   32/26561 [..............................] - ETA: 1s
  640/26561 [..............................] - ETA: 2s
 1024/26561 [>.............................] - ETA: 2s
 1472/26561 [>.............................] - ETA: 2s
 1856/26561 [=>............................] - ETA: 2s
 2400/26561 [=>............................] - ETA: 2s
 3456/26561 [==>...........................] - ETA: 2s
 4512/26561 [====>.........................] - ETA: 1s
 5600/26561 [=====>........................] - ETA: 1s
 6656/26561 [======>.......................] - ETA: 1s
 7712/26561 [=======>......................] - ETA: 1s
 8224/26561 [========>.....................] - ETA: 1s
 8544/26561 [========>.....................] - ETA: 1s
 8864/26561 [=========>....................] - ETA: 1s
 9504/26561 [=========>....................] - ETA: 1s
10176/26561 [==========>...................] - ETA: 1s
11168/26561 [===========>..................] - ETA: 1s
12192/26561 [============>.................] - ETA: 1s
13216/26561 [=============>................] - ETA: 0s
14240/26561 [===============>..............] - ETA: 0s
15264/26561 [================>.............] - ETA: 0s
16096/26561 [=================>............] - ETA: 0s
17120/26561 [==================>...........] - ETA: 0s
18144/26561 [===================>..........] - ETA: 0s
19104/26561 [====================>.........] - ETA: 0s
19968/26561 [=====================>........] - ETA: 0s
20896/26561 [======================>.......] - ETA: 0s
21952/26561 [=======================>......] - ETA: 0s
22784/26561 [========================>.....] - ETA: 0s
23840/26561 [=========================>....] - ETA: 0s
24928/26561 [===========================>..] - ETA: 0s
25952/26561 [============================>.] - ETA: 0s
26561/26561 [==============================] - 2s 61us/step

acc: 65.68%
NeuralNet Accuracy: 65.68% (61.68%) with {'loss': 'binary_crossentropy', 'optimizer': 'adam', 'num_hidden': 3, 'activation': 'relu', 'batch_size': 64, 'val_score': 65.68065428054088, 'train_score': 68.37844960656602, 'max_epochs': 100, 'hidden_layer_width': 16}
MAX: 66.3991439313
=================================================
{   'activation': 'relu',
    'batch_size': 512,
    'hidden_layer_width': 16,
    'loss': 'binary_crossentropy',
    'max_epochs': 100,
    'num_hidden': 3,
    'optimizer': 'adam'}

   32/13083 [..............................] - ETA: 2s
  384/13083 [..............................] - ETA: 1s
  736/13083 [>.............................] - ETA: 1s
 1600/13083 [==>...........................] - ETA: 1s
 2656/13083 [=====>........................] - ETA: 0s
 3712/13083 [=======>......................] - ETA: 0s
 4768/13083 [=========>....................] - ETA: 0s
 5824/13083 [============>.................] - ETA: 0s
 6848/13083 [==============>...............] - ETA: 0s
 7296/13083 [===============>..............] - ETA: 0s
 7616/13083 [================>.............] - ETA: 0s
 8032/13083 [=================>............] - ETA: 0s
 8768/13083 [===================>..........] - ETA: 0s
 9440/13083 [====================>.........] - ETA: 0s
10464/13083 [======================>.......] - ETA: 0s
11488/13083 [=========================>....] - ETA: 0s
12512/13083 [===========================>..] - ETA: 0s
13083/13083 [==============================] - 1s 65us/step

   32/26561 [..............................] - ETA: 1s
 1056/26561 [>.............................] - ETA: 1s
 1920/26561 [=>............................] - ETA: 1s
 2944/26561 [==>...........................] - ETA: 1s
 3968/26561 [===>..........................] - ETA: 1s
 4960/26561 [====>.........................] - ETA: 1s
 5824/26561 [=====>........................] - ETA: 1s
 6752/26561 [======>.......................] - ETA: 1s
 7808/26561 [=======>......................] - ETA: 0s
 8768/26561 [========>.....................] - ETA: 0s
 9728/26561 [=========>....................] - ETA: 0s
10848/26561 [===========>..................] - ETA: 0s
11968/26561 [============>.................] - ETA: 0s
13088/26561 [=============>................] - ETA: 0s
14208/26561 [===============>..............] - ETA: 0s
15328/26561 [================>.............] - ETA: 0s
16448/26561 [=================>............] - ETA: 0s
17568/26561 [==================>...........] - ETA: 0s
18688/26561 [====================>.........] - ETA: 0s
19808/26561 [=====================>........] - ETA: 0s
20864/26561 [======================>.......] - ETA: 0s
21920/26561 [=======================>......] - ETA: 0s
23008/26561 [========================>.....] - ETA: 0s
24096/26561 [==========================>...] - ETA: 0s
25184/26561 [===========================>..] - ETA: 0s
26304/26561 [============================>.] - ETA: 0s
26561/26561 [==============================] - 1s 49us/step

acc: 66.11%
NeuralNet Accuracy: 66.11% (61.72%) with {'loss': 'binary_crossentropy', 'optimizer': 'adam', 'num_hidden': 3, 'activation': 'relu', 'batch_size': 512, 'val_score': 66.10869066363344, 'train_score': 68.08478596438388, 'max_epochs': 100, 'hidden_layer_width': 16}
MAX: 66.3991439313
=================================================
{   'activation': 'relu',
    'batch_size': 64,
    'hidden_layer_width': 32,
    'loss': 'binary_crossentropy',
    'max_epochs': 100,
    'num_hidden': 3,
    'optimizer': 'adam'}

   32/13083 [..............................] - ETA: 1s
 1120/13083 [=>............................] - ETA: 0s
 2208/13083 [====>.........................] - ETA: 0s
 3296/13083 [======>.......................] - ETA: 0s
 4384/13083 [=========>....................] - ETA: 0s
 5472/13083 [===========>..................] - ETA: 0s
 6560/13083 [==============>...............] - ETA: 0s
 7648/13083 [================>.............] - ETA: 0s
 8736/13083 [===================>..........] - ETA: 0s
 9824/13083 [=====================>........] - ETA: 0s
10912/13083 [========================>.....] - ETA: 0s
12000/13083 [==========================>...] - ETA: 0s
13083/13083 [==============================] - 1s 46us/step

   32/26561 [..............................] - ETA: 1s
 1088/26561 [>.............................] - ETA: 1s
 2112/26561 [=>............................] - ETA: 1s
 2784/26561 [==>...........................] - ETA: 1s
 3168/26561 [==>...........................] - ETA: 1s
 3552/26561 [===>..........................] - ETA: 1s
 3904/26561 [===>..........................] - ETA: 1s
 4320/26561 [===>..........................] - ETA: 1s
 5280/26561 [====>.........................] - ETA: 1s
 6336/26561 [======>.......................] - ETA: 1s
 7392/26561 [=======>......................] - ETA: 1s
 8448/26561 [========>.....................] - ETA: 1s
 9504/26561 [=========>....................] - ETA: 1s
10368/26561 [==========>...................] - ETA: 1s
10752/26561 [===========>..................] - ETA: 1s
11072/26561 [===========>..................] - ETA: 1s
11392/26561 [===========>..................] - ETA: 1s
12320/26561 [============>.................] - ETA: 1s
13184/26561 [=============>................] - ETA: 0s
14176/26561 [===============>..............] - ETA: 0s
15168/26561 [================>.............] - ETA: 0s
16160/26561 [=================>............] - ETA: 0s
17152/26561 [==================>...........] - ETA: 0s
17984/26561 [===================>..........] - ETA: 0s
19008/26561 [====================>.........] - ETA: 0s
20032/26561 [=====================>........] - ETA: 0s
20992/26561 [======================>.......] - ETA: 0s
21888/26561 [=======================>......] - ETA: 0s
22816/26561 [========================>.....] - ETA: 0s
23872/26561 [=========================>....] - ETA: 0s
24672/26561 [==========================>...] - ETA: 0s
25760/26561 [============================>.] - ETA: 0s
26561/26561 [==============================] - 2s 61us/step

acc: 65.63%
NeuralNet Accuracy: 65.63% (62.12%) with {'loss': 'binary_crossentropy', 'optimizer': 'adam', 'num_hidden': 3, 'activation': 'relu', 'batch_size': 64, 'val_score': 65.63479324268437, 'train_score': 68.97707164639885, 'max_epochs': 100, 'hidden_layer_width': 32}
MAX: 66.3991439313
=================================================
{   'activation': 'relu',
    'batch_size': 512,
    'hidden_layer_width': 32,
    'loss': 'binary_crossentropy',
    'max_epochs': 100,
    'num_hidden': 3,
    'optimizer': 'adam'}

   32/13083 [..............................] - ETA: 1s
 1120/13083 [=>............................] - ETA: 0s
 2208/13083 [====>.........................] - ETA: 0s
 3296/13083 [======>.......................] - ETA: 0s
 4384/13083 [=========>....................] - ETA: 0s
 5472/13083 [===========>..................] - ETA: 0s
 6560/13083 [==============>...............] - ETA: 0s
 7648/13083 [================>.............] - ETA: 0s
 8736/13083 [===================>..........] - ETA: 0s
 9824/13083 [=====================>........] - ETA: 0s
10912/13083 [========================>.....] - ETA: 0s
12000/13083 [==========================>...] - ETA: 0s
13083/13083 [==============================] - 1s 47us/step

   32/26561 [..............................] - ETA: 1s
 1120/26561 [>.............................] - ETA: 1s
 2080/26561 [=>............................] - ETA: 1s
 2432/26561 [=>............................] - ETA: 1s
 2784/26561 [==>...........................] - ETA: 1s
 3168/26561 [==>...........................] - ETA: 1s
 3520/26561 [==>...........................] - ETA: 2s
 4192/26561 [===>..........................] - ETA: 1s
 5216/26561 [====>.........................] - ETA: 1s
 6240/26561 [======>.......................] - ETA: 1s
 7264/26561 [=======>......................] - ETA: 1s
 8288/26561 [========>.....................] - ETA: 1s
 9312/26561 [=========>....................] - ETA: 1s
 9632/26561 [=========>....................] - ETA: 1s
 9952/26561 [==========>...................] - ETA: 1s
10336/26561 [==========>...................] - ETA: 1s
11136/26561 [===========>..................] - ETA: 1s
12000/26561 [============>.................] - ETA: 1s
13024/26561 [=============>................] - ETA: 0s
14048/26561 [==============>...............] - ETA: 0s
15072/26561 [================>.............] - ETA: 0s
16096/26561 [=================>............] - ETA: 0s
16928/26561 [==================>...........] - ETA: 0s
17952/26561 [===================>..........] - ETA: 0s
18976/26561 [====================>.........] - ETA: 0s
19968/26561 [=====================>........] - ETA: 0s
20800/26561 [======================>.......] - ETA: 0s
21728/26561 [=======================>......] - ETA: 0s
22752/26561 [========================>.....] - ETA: 0s
23680/26561 [=========================>....] - ETA: 0s
24576/26561 [==========================>...] - ETA: 0s
25664/26561 [===========================>..] - ETA: 0s
26561/26561 [==============================] - 2s 62us/step

acc: 65.33%
NeuralNet Accuracy: 65.33% (61.87%) with {'loss': 'binary_crossentropy', 'optimizer': 'adam', 'num_hidden': 3, 'activation': 'relu', 'batch_size': 512, 'val_score': 65.3290529658577, 'train_score': 68.27679680734911, 'max_epochs': 100, 'hidden_layer_width': 32}
MAX: 66.3991439313
=================================================
{   'activation': 'relu',
    'batch_size': 64,
    'hidden_layer_width': 64,
    'loss': 'binary_crossentropy',
    'max_epochs': 100,
    'num_hidden': 3,
    'optimizer': 'adam'}

   32/13083 [..............................] - ETA: 0s
 1088/13083 [=>............................] - ETA: 0s
 2144/13083 [===>..........................] - ETA: 0s
 3200/13083 [======>.......................] - ETA: 0s
 4288/13083 [========>.....................] - ETA: 0s
 5376/13083 [===========>..................] - ETA: 0s
 6336/13083 [=============>................] - ETA: 0s
 6784/13083 [==============>...............] - ETA: 0s
 7168/13083 [===============>..............] - ETA: 0s
 7552/13083 [================>.............] - ETA: 0s
 7904/13083 [=================>............] - ETA: 0s
 8544/13083 [==================>...........] - ETA: 0s
 9600/13083 [=====================>........] - ETA: 0s
10656/13083 [=======================>......] - ETA: 0s
11712/13083 [=========================>....] - ETA: 0s
12768/13083 [============================>.] - ETA: 0s
13083/13083 [==============================] - 1s 61us/step

   32/26561 [..............................] - ETA: 1s
  800/26561 [..............................] - ETA: 1s
 1184/26561 [>.............................] - ETA: 2s
 1600/26561 [>.............................] - ETA: 2s
 2240/26561 [=>............................] - ETA: 2s
 2912/26561 [==>...........................] - ETA: 2s
 3936/26561 [===>..........................] - ETA: 1s
 4960/26561 [====>.........................] - ETA: 1s
 5952/26561 [=====>........................] - ETA: 1s
 6912/26561 [======>.......................] - ETA: 1s
 7872/26561 [=======>......................] - ETA: 1s
 8704/26561 [========>.....................] - ETA: 1s
 9728/26561 [=========>....................] - ETA: 1s
10752/26561 [===========>..................] - ETA: 0s
11712/26561 [============>.................] - ETA: 0s
12576/26561 [=============>................] - ETA: 0s
13504/26561 [==============>...............] - ETA: 0s
14560/26561 [===============>..............] - ETA: 0s
15328/26561 [================>.............] - ETA: 0s
16384/26561 [=================>............] - ETA: 0s
17472/26561 [==================>...........] - ETA: 0s
18560/26561 [===================>..........] - ETA: 0s
19648/26561 [=====================>........] - ETA: 0s
20736/26561 [======================>.......] - ETA: 0s
21824/26561 [=======================>......] - ETA: 0s
22912/26561 [========================>.....] - ETA: 0s
24000/26561 [==========================>...] - ETA: 0s
25088/26561 [===========================>..] - ETA: 0s
26144/26561 [============================>.] - ETA: 0s
26561/26561 [==============================] - 1s 55us/step

acc: 66.04%
NeuralNet Accuracy: 66.04% (61.83%) with {'loss': 'binary_crossentropy', 'optimizer': 'adam', 'num_hidden': 3, 'activation': 'relu', 'batch_size': 64, 'val_score': 66.039899102065, 'train_score': 69.39121268024547, 'max_epochs': 100, 'hidden_layer_width': 64}
MAX: 66.3991439313
=================================================
{   'activation': 'relu',
    'batch_size': 512,
    'hidden_layer_width': 64,
    'loss': 'binary_crossentropy',
    'max_epochs': 100,
    'num_hidden': 3,
    'optimizer': 'adam'}

   32/13083 [..............................] - ETA: 0s
 1088/13083 [=>............................] - ETA: 0s
 2144/13083 [===>..........................] - ETA: 0s
 3232/13083 [======>.......................] - ETA: 0s
 4320/13083 [========>.....................] - ETA: 0s
 5280/13083 [===========>..................] - ETA: 0s
 5696/13083 [============>.................] - ETA: 0s
 6016/13083 [============>.................] - ETA: 0s
 6368/13083 [=============>................] - ETA: 0s
 6720/13083 [==============>...............] - ETA: 0s
 7328/13083 [===============>..............] - ETA: 0s
 8352/13083 [==================>...........] - ETA: 0s
 9408/13083 [====================>.........] - ETA: 0s
10464/13083 [======================>.......] - ETA: 0s
11520/13083 [=========================>....] - ETA: 0s
12544/13083 [===========================>..] - ETA: 0s
12864/13083 [============================>.] - ETA: 0s
13083/13083 [==============================] - 1s 66us/step

   32/26561 [..............................] - ETA: 5s
  384/26561 [..............................] - ETA: 4s
 1184/26561 [>.............................] - ETA: 2s
 1824/26561 [=>............................] - ETA: 2s
 2816/26561 [==>...........................] - ETA: 1s
 3808/26561 [===>..........................] - ETA: 1s
 4800/26561 [====>.........................] - ETA: 1s
 5792/26561 [=====>........................] - ETA: 1s
 6784/26561 [======>.......................] - ETA: 1s
 7584/26561 [=======>......................] - ETA: 1s
 8544/26561 [========>.....................] - ETA: 1s
 9536/26561 [=========>....................] - ETA: 1s
10464/26561 [==========>...................] - ETA: 0s
11296/26561 [===========>..................] - ETA: 0s
12192/26561 [============>.................] - ETA: 0s
13216/26561 [=============>................] - ETA: 0s
13984/26561 [==============>...............] - ETA: 0s
15040/26561 [===============>..............] - ETA: 0s
16128/26561 [=================>............] - ETA: 0s
17216/26561 [==================>...........] - ETA: 0s
18304/26561 [===================>..........] - ETA: 0s
19392/26561 [====================>.........] - ETA: 0s
20480/26561 [======================>.......] - ETA: 0s
21568/26561 [=======================>......] - ETA: 0s
22656/26561 [========================>.....] - ETA: 0s
23744/26561 [=========================>....] - ETA: 0s
24832/26561 [===========================>..] - ETA: 0s
25920/26561 [============================>.] - ETA: 0s
26561/26561 [==============================] - 1s 53us/step

acc: 65.80%
NeuralNet Accuracy: 65.80% (62.11%) with {'loss': 'binary_crossentropy', 'optimizer': 'adam', 'num_hidden': 3, 'activation': 'relu', 'batch_size': 512, 'val_score': 65.80295039318501, 'train_score': 69.1954369187907, 'max_epochs': 100, 'hidden_layer_width': 64}
MAX: 66.3991439313
=================================================
{   'activation': 'tanh',
    'batch_size': 64,
    'hidden_layer_width': 16,
    'loss': 'binary_crossentropy',
    'max_epochs': 100,
    'num_hidden': 1,
    'optimizer': 'adam'}

   32/13083 [..............................] - ETA: 0s
 1152/13083 [=>............................] - ETA: 0s
 2272/13083 [====>.........................] - ETA: 0s
 3392/13083 [======>.......................] - ETA: 0s
 4480/13083 [=========>....................] - ETA: 0s
 5568/13083 [===========>..................] - ETA: 0s
 6656/13083 [==============>...............] - ETA: 0s
 7744/13083 [================>.............] - ETA: 0s
 8832/13083 [===================>..........] - ETA: 0s
 9920/13083 [=====================>........] - ETA: 0s
11008/13083 [========================>.....] - ETA: 0s
12096/13083 [==========================>...] - ETA: 0s
13083/13083 [==============================] - 1s 46us/step

   32/26561 [..............................] - ETA: 1s
 1152/26561 [>.............................] - ETA: 1s
 2240/26561 [=>............................] - ETA: 1s
 3328/26561 [==>...........................] - ETA: 1s
 4416/26561 [===>..........................] - ETA: 1s
 5408/26561 [=====>........................] - ETA: 1s
 5760/26561 [=====>........................] - ETA: 1s
 6112/26561 [=====>........................] - ETA: 1s
 6464/26561 [======>.......................] - ETA: 1s
 6848/26561 [======>.......................] - ETA: 1s
 7520/26561 [=======>......................] - ETA: 1s
 8576/26561 [========>.....................] - ETA: 1s
 9664/26561 [=========>....................] - ETA: 1s
10752/26561 [===========>..................] - ETA: 0s
11840/26561 [============>.................] - ETA: 0s
12832/26561 [=============>................] - ETA: 0s
13120/26561 [=============>................] - ETA: 0s
13472/26561 [==============>...............] - ETA: 0s
13920/26561 [==============>...............] - ETA: 0s
14720/26561 [===============>..............] - ETA: 0s
15648/26561 [================>.............] - ETA: 0s
16640/26561 [=================>............] - ETA: 0s
17664/26561 [==================>...........] - ETA: 0s
18688/26561 [====================>.........] - ETA: 0s
19712/26561 [=====================>........] - ETA: 0s
20544/26561 [======================>.......] - ETA: 0s
21600/26561 [=======================>......] - ETA: 0s
22656/26561 [========================>.....] - ETA: 0s
23680/26561 [=========================>....] - ETA: 0s
24544/26561 [==========================>...] - ETA: 0s
25472/26561 [===========================>..] - ETA: 0s
26496/26561 [============================>.] - ETA: 0s
26561/26561 [==============================] - 2s 60us/step

acc: 65.61%
NeuralNet Accuracy: 65.61% (62.03%) with {'loss': 'binary_crossentropy', 'optimizer': 'adam', 'num_hidden': 1, 'activation': 'tanh', 'batch_size': 64, 'val_score': 65.61186272671743, 'train_score': 67.30921275554384, 'max_epochs': 100, 'hidden_layer_width': 16}
MAX: 66.3991439313
=================================================
{   'activation': 'tanh',
    'batch_size': 512,
    'hidden_layer_width': 16,
    'loss': 'binary_crossentropy',
    'max_epochs': 100,
    'num_hidden': 1,
    'optimizer': 'adam'}

   32/13083 [..............................] - ETA: 2s
 1088/13083 [=>............................] - ETA: 0s
 2176/13083 [===>..........................] - ETA: 0s
 3264/13083 [======>.......................] - ETA: 0s
 4320/13083 [========>.....................] - ETA: 0s
 5376/13083 [===========>..................] - ETA: 0s
 6048/13083 [============>.................] - ETA: 0s
 6368/13083 [=============>................] - ETA: 0s
 6688/13083 [==============>...............] - ETA: 0s
 7328/13083 [===============>..............] - ETA: 0s
 8000/13083 [=================>............] - ETA: 0s
 9056/13083 [===================>..........] - ETA: 0s
10112/13083 [======================>.......] - ETA: 0s
11168/13083 [========================>.....] - ETA: 0s
12224/13083 [===========================>..] - ETA: 0s
13083/13083 [==============================] - 1s 58us/step

   32/26561 [..............................] - ETA: 1s
  896/26561 [>.............................] - ETA: 1s
 1920/26561 [=>............................] - ETA: 1s
 2944/26561 [==>...........................] - ETA: 1s
 3904/26561 [===>..........................] - ETA: 1s
 4768/26561 [====>.........................] - ETA: 1s
 5696/26561 [=====>........................] - ETA: 1s
 6784/26561 [======>.......................] - ETA: 1s
 7712/26561 [=======>......................] - ETA: 1s
 8736/26561 [========>.....................] - ETA: 0s
 9824/26561 [==========>...................] - ETA: 0s
10944/26561 [===========>..................] - ETA: 0s
12064/26561 [============>.................] - ETA: 0s
13184/26561 [=============>................] - ETA: 0s
14304/26561 [===============>..............] - ETA: 0s
15424/26561 [================>.............] - ETA: 0s
16544/26561 [=================>............] - ETA: 0s
17632/26561 [==================>...........] - ETA: 0s
18688/26561 [====================>.........] - ETA: 0s
19776/26561 [=====================>........] - ETA: 0s
20864/26561 [======================>.......] - ETA: 0s
21952/26561 [=======================>......] - ETA: 0s
23040/26561 [=========================>....] - ETA: 0s
24128/26561 [==========================>...] - ETA: 0s
25216/26561 [===========================>..] - ETA: 0s
26304/26561 [============================>.] - ETA: 0s
26561/26561 [==============================] - 1s 49us/step

acc: 65.59%
NeuralNet Accuracy: 65.59% (61.93%) with {'loss': 'binary_crossentropy', 'optimizer': 'adam', 'num_hidden': 1, 'activation': 'tanh', 'batch_size': 512, 'val_score': 65.58893220163873, 'train_score': 67.53887278340423, 'max_epochs': 100, 'hidden_layer_width': 16}
MAX: 66.3991439313
=================================================
{   'activation': 'tanh',
    'batch_size': 64,
    'hidden_layer_width': 32,
    'loss': 'binary_crossentropy',
    'max_epochs': 100,
    'num_hidden': 1,
    'optimizer': 'adam'}

   32/13083 [..............................] - ETA: 0s
 1152/13083 [=>............................] - ETA: 0s
 2272/13083 [====>.........................] - ETA: 0s
 3296/13083 [======>.......................] - ETA: 0s
 4000/13083 [========>.....................] - ETA: 0s
 4352/13083 [========>.....................] - ETA: 0s
 4736/13083 [=========>....................] - ETA: 0s
 5120/13083 [==========>...................] - ETA: 0s
 5504/13083 [===========>..................] - ETA: 0s
 6560/13083 [==============>...............] - ETA: 0s
 7648/13083 [================>.............] - ETA: 0s
 8736/13083 [===================>..........] - ETA: 0s
 9824/13083 [=====================>........] - ETA: 0s
10880/13083 [=======================>......] - ETA: 0s
11392/13083 [=========================>....] - ETA: 0s
11808/13083 [==========================>...] - ETA: 0s
12256/13083 [===========================>..] - ETA: 0s
13083/13083 [==============================] - 1s 68us/step

   32/26561 [..............................] - ETA: 1s
  704/26561 [..............................] - ETA: 1s
 1728/26561 [>.............................] - ETA: 1s
 2752/26561 [==>...........................] - ETA: 1s
 3808/26561 [===>..........................] - ETA: 1s
 4864/26561 [====>.........................] - ETA: 1s
 5920/26561 [=====>........................] - ETA: 1s
 6784/26561 [======>.......................] - ETA: 1s
 7776/26561 [=======>......................] - ETA: 0s
 8864/26561 [=========>....................] - ETA: 0s
 9856/26561 [==========>...................] - ETA: 0s
10784/26561 [===========>..................] - ETA: 0s
11840/26561 [============>.................] - ETA: 0s
12704/26561 [=============>................] - ETA: 0s
13792/26561 [==============>...............] - ETA: 0s
14912/26561 [===============>..............] - ETA: 0s
16032/26561 [=================>............] - ETA: 0s
17152/26561 [==================>...........] - ETA: 0s
18272/26561 [===================>..........] - ETA: 0s
19392/26561 [====================>.........] - ETA: 0s
20512/26561 [======================>.......] - ETA: 0s
21632/26561 [=======================>......] - ETA: 0s
22752/26561 [========================>.....] - ETA: 0s
23872/26561 [=========================>....] - ETA: 0s
24992/26561 [===========================>..] - ETA: 0s
26112/26561 [============================>.] - ETA: 0s
26561/26561 [==============================] - 1s 49us/step

acc: 65.53%
NeuralNet Accuracy: 65.53% (61.91%) with {'loss': 'binary_crossentropy', 'optimizer': 'adam', 'num_hidden': 1, 'activation': 'tanh', 'batch_size': 64, 'val_score': 65.5277841514671, 'train_score': 67.38827604382365, 'max_epochs': 100, 'hidden_layer_width': 32}
MAX: 66.3991439313
=================================================
{   'activation': 'tanh',
    'batch_size': 512,
    'hidden_layer_width': 32,
    'loss': 'binary_crossentropy',
    'max_epochs': 100,
    'num_hidden': 1,
    'optimizer': 'adam'}

   32/13083 [..............................] - ETA: 1s
 1088/13083 [=>............................] - ETA: 0s
 2176/13083 [===>..........................] - ETA: 0s
 3264/13083 [======>.......................] - ETA: 0s
 4352/13083 [========>.....................] - ETA: 0s
 5408/13083 [===========>..................] - ETA: 0s
 6464/13083 [=============>................] - ETA: 0s
 7552/13083 [================>.............] - ETA: 0s
 8640/13083 [==================>...........] - ETA: 0s
 9728/13083 [=====================>........] - ETA: 0s
10848/13083 [=======================>......] - ETA: 0s
11968/13083 [==========================>...] - ETA: 0s
13083/13083 [==============================] - 1s 47us/step

   32/26561 [..............................] - ETA: 1s
 1152/26561 [>.............................] - ETA: 1s
 2272/26561 [=>............................] - ETA: 1s
 3392/26561 [==>...........................] - ETA: 1s
 4512/26561 [====>.........................] - ETA: 0s
 5568/26561 [=====>........................] - ETA: 0s
 6528/26561 [======>.......................] - ETA: 0s
 7040/26561 [======>.......................] - ETA: 1s
 7392/26561 [=======>......................] - ETA: 1s
 7744/26561 [=======>......................] - ETA: 1s
 8096/26561 [========>.....................] - ETA: 1s
 8608/26561 [========>.....................] - ETA: 1s
 9664/26561 [=========>....................] - ETA: 1s
10720/26561 [===========>..................] - ETA: 0s
11808/26561 [============>.................] - ETA: 0s
12896/26561 [=============>................] - ETA: 0s
13952/26561 [==============>...............] - ETA: 0s
14496/26561 [===============>..............] - ETA: 0s
14816/26561 [===============>..............] - ETA: 0s
15200/26561 [================>.............] - ETA: 0s
16032/26561 [=================>............] - ETA: 0s
16704/26561 [=================>............] - ETA: 0s
17696/26561 [==================>...........] - ETA: 0s
18688/26561 [====================>.........] - ETA: 0s
19680/26561 [=====================>........] - ETA: 0s
20704/26561 [======================>.......] - ETA: 0s
21728/26561 [=======================>......] - ETA: 0s
22560/26561 [========================>.....] - ETA: 0s
23584/26561 [=========================>....] - ETA: 0s
24608/26561 [==========================>...] - ETA: 0s
25568/26561 [===========================>..] - ETA: 0s
26432/26561 [============================>.] - ETA: 0s
26561/26561 [==============================] - 2s 60us/step

acc: 65.58%
NeuralNet Accuracy: 65.58% (62.04%) with {'loss': 'binary_crossentropy', 'optimizer': 'adam', 'num_hidden': 1, 'activation': 'tanh', 'batch_size': 512, 'val_score': 65.58128869479779, 'train_score': 67.80241707767027, 'max_epochs': 100, 'hidden_layer_width': 32}
MAX: 66.3991439313
=================================================
{   'activation': 'tanh',
    'batch_size': 64,
    'hidden_layer_width': 64,
    'loss': 'binary_crossentropy',
    'max_epochs': 100,
    'num_hidden': 1,
    'optimizer': 'adam'}

   32/13083 [..............................] - ETA: 0s
 1152/13083 [=>............................] - ETA: 0s
 2272/13083 [====>.........................] - ETA: 0s
 3392/13083 [======>.......................] - ETA: 0s
 4480/13083 [=========>....................] - ETA: 0s
 5536/13083 [===========>..................] - ETA: 0s
 6528/13083 [=============>................] - ETA: 0s
 7168/13083 [===============>..............] - ETA: 0s
 7520/13083 [================>.............] - ETA: 0s
 7904/13083 [=================>............] - ETA: 0s
 8256/13083 [=================>............] - ETA: 0s
 8768/13083 [===================>..........] - ETA: 0s
 9824/13083 [=====================>........] - ETA: 0s
10880/13083 [=======================>......] - ETA: 0s
11936/13083 [==========================>...] - ETA: 0s
13024/13083 [============================>.] - ETA: 0s
13083/13083 [==============================] - 1s 60us/step

   32/26561 [..............................] - ETA: 1s
 1088/26561 [>.............................] - ETA: 1s
 1728/26561 [>.............................] - ETA: 1s
 2016/26561 [=>............................] - ETA: 1s
 2336/26561 [=>............................] - ETA: 2s
 2976/26561 [==>...........................] - ETA: 2s
 3648/26561 [===>..........................] - ETA: 1s
 4672/26561 [====>.........................] - ETA: 1s
 5664/26561 [=====>........................] - ETA: 1s
 6656/26561 [======>.......................] - ETA: 1s
 7648/26561 [=======>......................] - ETA: 1s
 8640/26561 [========>.....................] - ETA: 1s
 9472/26561 [=========>....................] - ETA: 1s
10496/26561 [==========>...................] - ETA: 1s
11520/26561 [============>.................] - ETA: 0s
12448/26561 [=============>................] - ETA: 0s
13312/26561 [==============>...............] - ETA: 0s
14208/26561 [===============>..............] - ETA: 0s
15232/26561 [================>.............] - ETA: 0s
15968/26561 [=================>............] - ETA: 0s
17024/26561 [==================>...........] - ETA: 0s
18112/26561 [===================>..........] - ETA: 0s
19232/26561 [====================>.........] - ETA: 0s
20352/26561 [=====================>........] - ETA: 0s
21472/26561 [=======================>......] - ETA: 0s
22592/26561 [========================>.....] - ETA: 0s
23712/26561 [=========================>....] - ETA: 0s
24832/26561 [===========================>..] - ETA: 0s
25952/26561 [============================>.] - ETA: 0s
26561/26561 [==============================] - 1s 55us/step

acc: 65.51%
NeuralNet Accuracy: 65.51% (61.85%) with {'loss': 'binary_crossentropy', 'optimizer': 'adam', 'num_hidden': 1, 'activation': 'tanh', 'batch_size': 64, 'val_score': 65.51249713778523, 'train_score': 67.41463047325026, 'max_epochs': 100, 'hidden_layer_width': 64}
MAX: 66.3991439313
=================================================
{   'activation': 'tanh',
    'batch_size': 512,
    'hidden_layer_width': 64,
    'loss': 'binary_crossentropy',
    'max_epochs': 100,
    'num_hidden': 1,
    'optimizer': 'adam'}

   32/13083 [..............................] - ETA: 1s
 1024/13083 [=>............................] - ETA: 0s
 2048/13083 [===>..........................] - ETA: 0s
 2848/13083 [=====>........................] - ETA: 0s
 3872/13083 [=======>......................] - ETA: 0s
 4896/13083 [==========>...................] - ETA: 0s
 5888/13083 [============>.................] - ETA: 0s
 6784/13083 [==============>...............] - ETA: 0s
 7712/13083 [================>.............] - ETA: 0s
 8768/13083 [===================>..........] - ETA: 0s
 9632/13083 [=====================>........] - ETA: 0s
10688/13083 [=======================>......] - ETA: 0s
11776/13083 [==========================>...] - ETA: 0s
12896/13083 [============================>.] - ETA: 0s
13083/13083 [==============================] - 1s 52us/step

   32/26561 [..............................] - ETA: 1s
 1152/26561 [>.............................] - ETA: 1s
 2272/26561 [=>............................] - ETA: 1s
 3360/26561 [==>...........................] - ETA: 1s
 4448/26561 [====>.........................] - ETA: 1s
 5536/26561 [=====>........................] - ETA: 0s
 6624/26561 [======>.......................] - ETA: 0s
 7680/26561 [=======>......................] - ETA: 0s
 8768/26561 [========>.....................] - ETA: 0s
 9856/26561 [==========>...................] - ETA: 0s
10944/26561 [===========>..................] - ETA: 0s
12000/26561 [============>.................] - ETA: 0s
13056/26561 [=============>................] - ETA: 0s
14112/26561 [==============>...............] - ETA: 0s
15168/26561 [================>.............] - ETA: 0s
16128/26561 [=================>............] - ETA: 0s
16544/26561 [=================>............] - ETA: 0s
16896/26561 [==================>...........] - ETA: 0s
17248/26561 [==================>...........] - ETA: 0s
17600/26561 [==================>...........] - ETA: 0s
18112/26561 [===================>..........] - ETA: 0s
19168/26561 [====================>.........] - ETA: 0s
20224/26561 [=====================>........] - ETA: 0s
21280/26561 [=======================>......] - ETA: 0s
22336/26561 [========================>.....] - ETA: 0s
23360/26561 [=========================>....] - ETA: 0s
23680/26561 [=========================>....] - ETA: 0s
24032/26561 [==========================>...] - ETA: 0s
24352/26561 [==========================>...] - ETA: 0s
25184/26561 [===========================>..] - ETA: 0s
25856/26561 [============================>.] - ETA: 0s
26561/26561 [==============================] - 2s 59us/step

acc: 65.54%
NeuralNet Accuracy: 65.54% (61.92%) with {'loss': 'binary_crossentropy', 'optimizer': 'adam', 'num_hidden': 1, 'activation': 'tanh', 'batch_size': 512, 'val_score': 65.54307116514899, 'train_score': 68.15255449719514, 'max_epochs': 100, 'hidden_layer_width': 64}
MAX: 66.3991439313
=================================================
{   'activation': 'tanh',
    'batch_size': 64,
    'hidden_layer_width': 16,
    'loss': 'binary_crossentropy',
    'max_epochs': 100,
    'num_hidden': 2,
    'optimizer': 'adam'}

   32/13083 [..............................] - ETA: 1s
 1056/13083 [=>............................] - ETA: 0s
 2080/13083 [===>..........................] - ETA: 0s
 2592/13083 [====>.........................] - ETA: 0s
 2912/13083 [=====>........................] - ETA: 0s
 3296/13083 [======>.......................] - ETA: 0s
 4064/13083 [========>.....................] - ETA: 0s
 4704/13083 [=========>....................] - ETA: 0s
 5696/13083 [============>.................] - ETA: 0s
 6688/13083 [==============>...............] - ETA: 0s
 7712/13083 [================>.............] - ETA: 0s
 8736/13083 [===================>..........] - ETA: 0s
 9760/13083 [=====================>........] - ETA: 0s
10624/13083 [=======================>......] - ETA: 0s
11648/13083 [=========================>....] - ETA: 0s
12704/13083 [============================>.] - ETA: 0s
13083/13083 [==============================] - 1s 60us/step

   32/26561 [..............................] - ETA: 1s
  896/26561 [>.............................] - ETA: 1s
 1824/26561 [=>............................] - ETA: 1s
 2880/26561 [==>...........................] - ETA: 1s
 3648/26561 [===>..........................] - ETA: 1s
 4736/26561 [====>.........................] - ETA: 1s
 5824/26561 [=====>........................] - ETA: 1s
 6912/26561 [======>.......................] - ETA: 1s
 7968/26561 [=======>......................] - ETA: 0s
 9056/26561 [=========>....................] - ETA: 0s
10112/26561 [==========>...................] - ETA: 0s
11104/26561 [===========>..................] - ETA: 0s
12128/26561 [============>.................] - ETA: 0s
13184/26561 [=============>................] - ETA: 0s
14272/26561 [===============>..............] - ETA: 0s
15360/26561 [================>.............] - ETA: 0s
16448/26561 [=================>............] - ETA: 0s
17536/26561 [==================>...........] - ETA: 0s
18624/26561 [====================>.........] - ETA: 0s
19712/26561 [=====================>........] - ETA: 0s
20800/26561 [======================>.......] - ETA: 0s
21888/26561 [=======================>......] - ETA: 0s
22848/26561 [========================>.....] - ETA: 0s
23264/26561 [=========================>....] - ETA: 0s
23616/26561 [=========================>....] - ETA: 0s
24000/26561 [==========================>...] - ETA: 0s
24384/26561 [==========================>...] - ETA: 0s
24928/26561 [===========================>..] - ETA: 0s
25952/26561 [============================>.] - ETA: 0s
26561/26561 [==============================] - 1s 55us/step

acc: 65.57%
NeuralNet Accuracy: 65.57% (61.93%) with {'loss': 'binary_crossentropy', 'optimizer': 'adam', 'num_hidden': 2, 'activation': 'tanh', 'batch_size': 64, 'val_score': 65.57364518476774, 'train_score': 67.36568653288656, 'max_epochs': 100, 'hidden_layer_width': 16}
MAX: 66.3991439313
=================================================
{   'activation': 'tanh',
    'batch_size': 512,
    'hidden_layer_width': 16,
    'loss': 'binary_crossentropy',
    'max_epochs': 100,
    'num_hidden': 2,
    'optimizer': 'adam'}

   32/13083 [..............................] - ETA: 2s
  352/13083 [..............................] - ETA: 2s
  992/13083 [=>............................] - ETA: 1s
 1984/13083 [===>..........................] - ETA: 0s
 2976/13083 [=====>........................] - ETA: 0s
 3968/13083 [========>.....................] - ETA: 0s
 4960/13083 [==========>...................] - ETA: 0s
 5856/13083 [============>.................] - ETA: 0s
 6208/13083 [=============>................] - ETA: 0s
 6528/13083 [=============>................] - ETA: 0s
 6912/13083 [==============>...............] - ETA: 0s
 7712/13083 [================>.............] - ETA: 0s
 8640/13083 [==================>...........] - ETA: 0s
 9664/13083 [=====================>........] - ETA: 0s
10688/13083 [=======================>......] - ETA: 0s
11712/13083 [=========================>....] - ETA: 0s
12736/13083 [============================>.] - ETA: 0s
13083/13083 [==============================] - 1s 64us/step

   32/26561 [..............................] - ETA: 1s
  896/26561 [>.............................] - ETA: 1s
 1920/26561 [=>............................] - ETA: 1s
 2944/26561 [==>...........................] - ETA: 1s
 3904/26561 [===>..........................] - ETA: 1s
 4800/26561 [====>.........................] - ETA: 1s
 5824/26561 [=====>........................] - ETA: 1s
 6656/26561 [======>.......................] - ETA: 1s
 7712/26561 [=======>......................] - ETA: 1s
 8800/26561 [========>.....................] - ETA: 0s
 9856/26561 [==========>...................] - ETA: 0s
10912/26561 [===========>..................] - ETA: 0s
11936/26561 [============>.................] - ETA: 0s
12992/26561 [=============>................] - ETA: 0s
14048/26561 [==============>...............] - ETA: 0s
15104/26561 [================>.............] - ETA: 0s
16160/26561 [=================>............] - ETA: 0s
17248/26561 [==================>...........] - ETA: 0s
18304/26561 [===================>..........] - ETA: 0s
19360/26561 [====================>.........] - ETA: 0s
20448/26561 [======================>.......] - ETA: 0s
21504/26561 [=======================>......] - ETA: 0s
22592/26561 [========================>.....] - ETA: 0s
23648/26561 [=========================>....] - ETA: 0s
24736/26561 [==========================>...] - ETA: 0s
25728/26561 [============================>.] - ETA: 0s
26368/26561 [============================>.] - ETA: 0s
26561/26561 [==============================] - 1s 52us/step

acc: 65.61%
NeuralNet Accuracy: 65.61% (62.04%) with {'loss': 'binary_crossentropy', 'optimizer': 'adam', 'num_hidden': 2, 'activation': 'tanh', 'batch_size': 512, 'val_score': 65.61186272671743, 'train_score': 67.44851473965589, 'max_epochs': 100, 'hidden_layer_width': 16}
MAX: 66.3991439313
=================================================
{   'activation': 'tanh',
    'batch_size': 64,
    'hidden_layer_width': 32,
    'loss': 'binary_crossentropy',
    'max_epochs': 100,
    'num_hidden': 2,
    'optimizer': 'adam'}

   32/13083 [..............................] - ETA: 1s
 1056/13083 [=>............................] - ETA: 0s
 1824/13083 [===>..........................] - ETA: 0s
 2880/13083 [=====>........................] - ETA: 0s
 3936/13083 [========>.....................] - ETA: 0s
 4992/13083 [==========>...................] - ETA: 0s
 6048/13083 [============>.................] - ETA: 0s
 7104/13083 [===============>..............] - ETA: 0s
 8160/13083 [=================>............] - ETA: 0s
 9216/13083 [====================>.........] - ETA: 0s
10272/13083 [======================>.......] - ETA: 0s
11328/13083 [========================>.....] - ETA: 0s
12384/13083 [===========================>..] - ETA: 0s
13083/13083 [==============================] - 1s 49us/step

   32/26561 [..............................] - ETA: 1s
 1088/26561 [>.............................] - ETA: 1s
 2144/26561 [=>............................] - ETA: 1s
 3200/26561 [==>...........................] - ETA: 1s
 4256/26561 [===>..........................] - ETA: 1s
 5312/26561 [====>.........................] - ETA: 1s
 6368/26561 [======>.......................] - ETA: 0s
 7424/26561 [=======>......................] - ETA: 0s
 8160/26561 [========>.....................] - ETA: 0s
 8512/26561 [========>.....................] - ETA: 0s
 8864/26561 [=========>....................] - ETA: 1s
 9216/26561 [=========>....................] - ETA: 1s
 9568/26561 [=========>....................] - ETA: 1s
10496/26561 [==========>...................] - ETA: 1s
11200/26561 [===========>..................] - ETA: 0s
11520/26561 [============>.................] - ETA: 1s
11840/26561 [============>.................] - ETA: 1s
12448/26561 [=============>................] - ETA: 0s
13120/26561 [=============>................] - ETA: 0s
14112/26561 [==============>...............] - ETA: 0s
15104/26561 [================>.............] - ETA: 0s
16096/26561 [=================>............] - ETA: 0s
17088/26561 [==================>...........] - ETA: 0s
18080/26561 [===================>..........] - ETA: 0s
18912/26561 [====================>.........] - ETA: 0s
19936/26561 [=====================>........] - ETA: 0s
20960/26561 [======================>.......] - ETA: 0s
21888/26561 [=======================>......] - ETA: 0s
22720/26561 [========================>.....] - ETA: 0s
23616/26561 [=========================>....] - ETA: 0s
24608/26561 [==========================>...] - ETA: 0s
25344/26561 [===========================>..] - ETA: 0s
26368/26561 [============================>.] - ETA: 0s
26561/26561 [==============================] - 2s 62us/step

acc: 65.73%
NeuralNet Accuracy: 65.73% (62.25%) with {'loss': 'binary_crossentropy', 'optimizer': 'adam', 'num_hidden': 2, 'activation': 'tanh', 'batch_size': 64, 'val_score': 65.7265153261424, 'train_score': 67.70076427845338, 'max_epochs': 100, 'hidden_layer_width': 32}
MAX: 66.3991439313
=================================================
{   'activation': 'tanh',
    'batch_size': 512,
    'hidden_layer_width': 32,
    'loss': 'binary_crossentropy',
    'max_epochs': 100,
    'num_hidden': 2,
    'optimizer': 'adam'}

   32/13083 [..............................] - ETA: 1s
 1088/13083 [=>............................] - ETA: 0s
 2144/13083 [===>..........................] - ETA: 0s
 3200/13083 [======>.......................] - ETA: 0s
 4256/13083 [========>.....................] - ETA: 0s
 5312/13083 [===========>..................] - ETA: 0s
 6368/13083 [=============>................] - ETA: 0s
 7392/13083 [===============>..............] - ETA: 0s
 8448/13083 [==================>...........] - ETA: 0s
 9472/13083 [====================>.........] - ETA: 0s
10496/13083 [=======================>......] - ETA: 0s
11520/13083 [=========================>....] - ETA: 0s
12544/13083 [===========================>..] - ETA: 0s
13083/13083 [==============================] - 1s 49us/step

   32/26561 [..............................] - ETA: 1s
  928/26561 [>.............................] - ETA: 1s
 1248/26561 [>.............................] - ETA: 2s
 1600/26561 [>.............................] - ETA: 2s
 1952/26561 [=>............................] - ETA: 2s
 2272/26561 [=>............................] - ETA: 2s
 2976/26561 [==>...........................] - ETA: 2s
 3872/26561 [===>..........................] - ETA: 2s
 4224/26561 [===>..........................] - ETA: 2s
 4544/26561 [====>.........................] - ETA: 2s
 4832/26561 [====>.........................] - ETA: 2s
 5760/26561 [=====>........................] - ETA: 2s
 6496/26561 [======>.......................] - ETA: 1s
 7488/26561 [=======>......................] - ETA: 1s
 8448/26561 [========>.....................] - ETA: 1s
 9440/26561 [=========>....................] - ETA: 1s
10432/26561 [==========>...................] - ETA: 1s
11264/26561 [===========>..................] - ETA: 1s
12224/26561 [============>.................] - ETA: 1s
13216/26561 [=============>................] - ETA: 0s
14208/26561 [===============>..............] - ETA: 0s
14976/26561 [===============>..............] - ETA: 0s
15872/26561 [================>.............] - ETA: 0s
16896/26561 [==================>...........] - ETA: 0s
17792/26561 [===================>..........] - ETA: 0s
18656/26561 [====================>.........] - ETA: 0s
19712/26561 [=====================>........] - ETA: 0s
20768/26561 [======================>.......] - ETA: 0s
21824/26561 [=======================>......] - ETA: 0s
22880/26561 [========================>.....] - ETA: 0s
23936/26561 [==========================>...] - ETA: 0s
24992/26561 [===========================>..] - ETA: 0s
26048/26561 [============================>.] - ETA: 0s
26561/26561 [==============================] - 2s 63us/step

acc: 65.48%
NeuralNet Accuracy: 65.48% (62.19%) with {'loss': 'binary_crossentropy', 'optimizer': 'adam', 'num_hidden': 2, 'activation': 'tanh', 'batch_size': 512, 'val_score': 65.48192310586559, 'train_score': 67.24520914122209, 'max_epochs': 100, 'hidden_layer_width': 32}
MAX: 66.3991439313
=================================================
{   'activation': 'tanh',
    'batch_size': 64,
    'hidden_layer_width': 64,
    'loss': 'binary_crossentropy',
    'max_epochs': 100,
    'num_hidden': 2,
    'optimizer': 'adam'}

   32/13083 [..............................] - ETA: 1s
  640/13083 [>.............................] - ETA: 1s
 1632/13083 [==>...........................] - ETA: 0s
 2624/13083 [=====>........................] - ETA: 0s
 3616/13083 [=======>......................] - ETA: 0s
 4608/13083 [=========>....................] - ETA: 0s
 5568/13083 [===========>..................] - ETA: 0s
 6336/13083 [=============>................] - ETA: 0s
 7296/13083 [===============>..............] - ETA: 0s
 8256/13083 [=================>............] - ETA: 0s
 9152/13083 [===================>..........] - ETA: 0s
 9952/13083 [=====================>........] - ETA: 0s
10816/13083 [=======================>......] - ETA: 0s
11808/13083 [==========================>...] - ETA: 0s
12544/13083 [===========================>..] - ETA: 0s
13083/13083 [==============================] - 1s 57us/step

   32/26561 [..............................] - ETA: 1s
 1088/26561 [>.............................] - ETA: 1s
 2144/26561 [=>............................] - ETA: 1s
 3200/26561 [==>...........................] - ETA: 1s
 4256/26561 [===>..........................] - ETA: 1s
 5312/26561 [====>.........................] - ETA: 1s
 6368/26561 [======>.......................] - ETA: 0s
 7424/26561 [=======>......................] - ETA: 0s
 8480/26561 [========>.....................] - ETA: 0s
 9536/26561 [=========>....................] - ETA: 0s
10592/26561 [==========>...................] - ETA: 0s
11648/26561 [============>.................] - ETA: 0s
12704/26561 [=============>................] - ETA: 0s
13760/26561 [==============>...............] - ETA: 0s
14816/26561 [===============>..............] - ETA: 0s
15872/26561 [================>.............] - ETA: 0s
16928/26561 [==================>...........] - ETA: 0s
17984/26561 [===================>..........] - ETA: 0s
18816/26561 [====================>.........] - ETA: 0s
19168/26561 [====================>.........] - ETA: 0s
19488/26561 [=====================>........] - ETA: 0s
19840/26561 [=====================>........] - ETA: 0s
20160/26561 [=====================>........] - ETA: 0s
20800/26561 [======================>.......] - ETA: 0s
21760/26561 [=======================>......] - ETA: 0s
22080/26561 [=======================>......] - ETA: 0s
22400/26561 [========================>.....] - ETA: 0s
22752/26561 [========================>.....] - ETA: 0s
23520/26561 [=========================>....] - ETA: 0s
24320/26561 [==========================>...] - ETA: 0s
25312/26561 [===========================>..] - ETA: 0s
26304/26561 [============================>.] - ETA: 0s
26561/26561 [==============================] - 2s 61us/step

acc: 65.60%
NeuralNet Accuracy: 65.60% (62.17%) with {'loss': 'binary_crossentropy', 'optimizer': 'adam', 'num_hidden': 2, 'activation': 'tanh', 'batch_size': 64, 'val_score': 65.59657570984643, 'train_score': 67.46357441361394, 'max_epochs': 100, 'hidden_layer_width': 64}
MAX: 66.3991439313
=================================================
{   'activation': 'tanh',
    'batch_size': 512,
    'hidden_layer_width': 64,
    'loss': 'binary_crossentropy',
    'max_epochs': 100,
    'num_hidden': 2,
    'optimizer': 'adam'}

   32/13083 [..............................] - ETA: 1s
 1024/13083 [=>............................] - ETA: 0s
 1760/13083 [===>..........................] - ETA: 0s
 2784/13083 [=====>........................] - ETA: 0s
 3840/13083 [=======>......................] - ETA: 0s
 4896/13083 [==========>...................] - ETA: 0s
 5952/13083 [============>.................] - ETA: 0s
 7008/13083 [===============>..............] - ETA: 0s
 8064/13083 [=================>............] - ETA: 0s
 9120/13083 [===================>..........] - ETA: 0s
10176/13083 [======================>.......] - ETA: 0s
11232/13083 [========================>.....] - ETA: 0s
12288/13083 [===========================>..] - ETA: 0s
13083/13083 [==============================] - 1s 50us/step

   32/26561 [..............................] - ETA: 1s
 1088/26561 [>.............................] - ETA: 1s
 2144/26561 [=>............................] - ETA: 1s
 3200/26561 [==>...........................] - ETA: 1s
 4256/26561 [===>..........................] - ETA: 1s
 5312/26561 [====>.........................] - ETA: 1s
 6368/26561 [======>.......................] - ETA: 0s
 7424/26561 [=======>......................] - ETA: 0s
 8096/26561 [========>.....................] - ETA: 0s
 8416/26561 [========>.....................] - ETA: 0s
 8768/26561 [========>.....................] - ETA: 1s
 9088/26561 [=========>....................] - ETA: 1s
 9408/26561 [=========>....................] - ETA: 1s
10240/26561 [==========>...................] - ETA: 1s
10944/26561 [===========>..................] - ETA: 1s
11200/26561 [===========>..................] - ETA: 1s
11488/26561 [===========>..................] - ETA: 1s
12064/26561 [============>.................] - ETA: 1s
12704/26561 [=============>................] - ETA: 1s
13664/26561 [==============>...............] - ETA: 0s
14656/26561 [===============>..............] - ETA: 0s
15648/26561 [================>.............] - ETA: 0s
16608/26561 [=================>............] - ETA: 0s
17600/26561 [==================>...........] - ETA: 0s
18400/26561 [===================>..........] - ETA: 0s
19392/26561 [====================>.........] - ETA: 0s
20384/26561 [======================>.......] - ETA: 0s
21312/26561 [=======================>......] - ETA: 0s
22144/26561 [========================>.....] - ETA: 0s
23040/26561 [=========================>....] - ETA: 0s
24032/26561 [==========================>...] - ETA: 0s
24768/26561 [==========================>...] - ETA: 0s
25792/26561 [============================>.] - ETA: 0s
26561/26561 [==============================] - 2s 63us/step

acc: 65.88%
NeuralNet Accuracy: 65.88% (61.87%) with {'loss': 'binary_crossentropy', 'optimizer': 'adam', 'num_hidden': 2, 'activation': 'tanh', 'batch_size': 512, 'val_score': 65.87938546751705, 'train_score': 67.29415308158579, 'max_epochs': 100, 'hidden_layer_width': 64}
MAX: 66.3991439313
=================================================
{   'activation': 'tanh',
    'batch_size': 64,
    'hidden_layer_width': 16,
    'loss': 'binary_crossentropy',
    'max_epochs': 100,
    'num_hidden': 3,
    'optimizer': 'adam'}

   32/13083 [..............................] - ETA: 1s
  544/13083 [>.............................] - ETA: 1s
  896/13083 [=>............................] - ETA: 1s
 1248/13083 [=>............................] - ETA: 1s
 1600/13083 [==>...........................] - ETA: 1s
 2080/13083 [===>..........................] - ETA: 1s
 3040/13083 [=====>........................] - ETA: 1s
 3488/13083 [======>.......................] - ETA: 1s
 3840/13083 [=======>......................] - ETA: 1s
 4192/13083 [========>.....................] - ETA: 0s
 4896/13083 [==========>...................] - ETA: 0s
 5536/13083 [===========>..................] - ETA: 0s
 6496/13083 [=============>................] - ETA: 0s
 7456/13083 [================>.............] - ETA: 0s
 8416/13083 [==================>...........] - ETA: 0s
 9376/13083 [====================>.........] - ETA: 0s
10336/13083 [======================>.......] - ETA: 0s
11104/13083 [========================>.....] - ETA: 0s
12064/13083 [==========================>...] - ETA: 0s
13024/13083 [============================>.] - ETA: 0s
13083/13083 [==============================] - 1s 75us/step

   32/26561 [..............................] - ETA: 1s
  928/26561 [>.............................] - ETA: 1s
 1792/26561 [=>............................] - ETA: 1s
 2752/26561 [==>...........................] - ETA: 1s
 3648/26561 [===>..........................] - ETA: 1s
 4480/26561 [====>.........................] - ETA: 1s
 5504/26561 [=====>........................] - ETA: 1s
 6528/26561 [======>.......................] - ETA: 1s
 7552/26561 [=======>......................] - ETA: 1s
 8576/26561 [========>.....................] - ETA: 0s
 9600/26561 [=========>....................] - ETA: 0s
10624/26561 [==========>...................] - ETA: 0s
11648/26561 [============>.................] - ETA: 0s
12672/26561 [=============>................] - ETA: 0s
13696/26561 [==============>...............] - ETA: 0s
14720/26561 [===============>..............] - ETA: 0s
15744/26561 [================>.............] - ETA: 0s
16768/26561 [=================>............] - ETA: 0s
17792/26561 [===================>..........] - ETA: 0s
18816/26561 [====================>.........] - ETA: 0s
19840/26561 [=====================>........] - ETA: 0s
20832/26561 [======================>.......] - ETA: 0s
21856/26561 [=======================>......] - ETA: 0s
22688/26561 [========================>.....] - ETA: 0s
23040/26561 [=========================>....] - ETA: 0s
23424/26561 [=========================>....] - ETA: 0s
23808/26561 [=========================>....] - ETA: 0s
24160/26561 [==========================>...] - ETA: 0s
24864/26561 [===========================>..] - ETA: 0s
25792/26561 [============================>.] - ETA: 0s
26561/26561 [==============================] - 2s 58us/step

acc: 65.45%
NeuralNet Accuracy: 65.45% (62.27%) with {'loss': 'binary_crossentropy', 'optimizer': 'adam', 'num_hidden': 3, 'activation': 'tanh', 'batch_size': 64, 'val_score': 65.45134907394595, 'train_score': 66.69553104175294, 'max_epochs': 100, 'hidden_layer_width': 16}
MAX: 66.3991439313
=================================================
{   'activation': 'tanh',
    'batch_size': 512,
    'hidden_layer_width': 16,
    'loss': 'binary_crossentropy',
    'max_epochs': 100,
    'num_hidden': 3,
    'optimizer': 'adam'}

   32/13083 [..............................] - ETA: 2s
  320/13083 [..............................] - ETA: 2s
  672/13083 [>.............................] - ETA: 2s
 1504/13083 [==>...........................] - ETA: 1s
 2176/13083 [===>..........................] - ETA: 1s
 2464/13083 [====>.........................] - ETA: 1s
 2720/13083 [=====>........................] - ETA: 1s
 3232/13083 [======>.......................] - ETA: 1s
 3840/13083 [=======>......................] - ETA: 1s
 4736/13083 [=========>....................] - ETA: 0s
 5632/13083 [===========>..................] - ETA: 0s
 6528/13083 [=============>................] - ETA: 0s
 7456/13083 [================>.............] - ETA: 0s
 8416/13083 [==================>...........] - ETA: 0s
 9184/13083 [====================>.........] - ETA: 0s
10112/13083 [======================>.......] - ETA: 0s
11040/13083 [========================>.....] - ETA: 0s
11904/13083 [==========================>...] - ETA: 0s
12672/13083 [============================>.] - ETA: 0s
13083/13083 [==============================] - 1s 73us/step

   32/26561 [..............................] - ETA: 3s
  896/26561 [>.............................] - ETA: 1s
 1728/26561 [>.............................] - ETA: 1s
 2528/26561 [=>............................] - ETA: 1s
 3520/26561 [==>...........................] - ETA: 1s
 4544/26561 [====>.........................] - ETA: 1s
 5568/26561 [=====>........................] - ETA: 1s
 6592/26561 [======>.......................] - ETA: 1s
 7616/26561 [=======>......................] - ETA: 1s
 8608/26561 [========>.....................] - ETA: 0s
 9632/26561 [=========>....................] - ETA: 0s
10624/26561 [==========>...................] - ETA: 0s
11616/26561 [============>.................] - ETA: 0s
12640/26561 [=============>................] - ETA: 0s
13632/26561 [==============>...............] - ETA: 0s
14624/26561 [===============>..............] - ETA: 0s
15616/26561 [================>.............] - ETA: 0s
16576/26561 [=================>............] - ETA: 0s
17568/26561 [==================>...........] - ETA: 0s
18560/26561 [===================>..........] - ETA: 0s
19552/26561 [=====================>........] - ETA: 0s
20448/26561 [======================>.......] - ETA: 0s
20832/26561 [======================>.......] - ETA: 0s
21184/26561 [======================>.......] - ETA: 0s
21504/26561 [=======================>......] - ETA: 0s
21824/26561 [=======================>......] - ETA: 0s
22208/26561 [========================>.....] - ETA: 0s
23136/26561 [=========================>....] - ETA: 0s
23616/26561 [=========================>....] - ETA: 0s
23936/26561 [==========================>...] - ETA: 0s
24256/26561 [==========================>...] - ETA: 0s
24832/26561 [===========================>..] - ETA: 0s
25440/26561 [===========================>..] - ETA: 0s
26368/26561 [============================>.] - ETA: 0s
26561/26561 [==============================] - 2s 64us/step

acc: 65.75%
NeuralNet Accuracy: 65.75% (62.15%) with {'loss': 'binary_crossentropy', 'optimizer': 'adam', 'num_hidden': 3, 'activation': 'tanh', 'batch_size': 512, 'val_score': 65.74944584210932, 'train_score': 67.74970821881706, 'max_epochs': 100, 'hidden_layer_width': 16}
MAX: 66.3991439313
=================================================
{   'activation': 'tanh',
    'batch_size': 64,
    'hidden_layer_width': 32,
    'loss': 'binary_crossentropy',
    'max_epochs': 100,
    'num_hidden': 3,
    'optimizer': 'adam'}

   32/13083 [..............................] - ETA: 1s
  800/13083 [>.............................] - ETA: 0s
 1760/13083 [===>..........................] - ETA: 0s
 2752/13083 [=====>........................] - ETA: 0s
 3680/13083 [=======>......................] - ETA: 0s
 4608/13083 [=========>....................] - ETA: 0s
 5536/13083 [===========>..................] - ETA: 0s
 6464/13083 [=============>................] - ETA: 0s
 7392/13083 [===============>..............] - ETA: 0s
 8352/13083 [==================>...........] - ETA: 0s
 9312/13083 [====================>.........] - ETA: 0s
10240/13083 [======================>.......] - ETA: 0s
11168/13083 [========================>.....] - ETA: 0s
12064/13083 [==========================>...] - ETA: 0s
13024/13083 [============================>.] - ETA: 0s
13083/13083 [==============================] - 1s 55us/step

   32/26561 [..............................] - ETA: 1s
 1024/26561 [>.............................] - ETA: 1s
 2016/26561 [=>............................] - ETA: 1s
 3008/26561 [==>...........................] - ETA: 1s
 4000/26561 [===>..........................] - ETA: 1s
 4896/26561 [====>.........................] - ETA: 1s
 5376/26561 [=====>........................] - ETA: 1s
 5696/26561 [=====>........................] - ETA: 1s
 6016/26561 [=====>........................] - ETA: 1s
 6336/26561 [======>.......................] - ETA: 1s
 6720/26561 [======>.......................] - ETA: 1s
 7648/26561 [=======>......................] - ETA: 1s
 8096/26561 [========>.....................] - ETA: 1s
 8384/26561 [========>.....................] - ETA: 1s
 8672/26561 [========>.....................] - ETA: 1s
 9216/26561 [=========>....................] - ETA: 1s
 9792/26561 [==========>...................] - ETA: 1s
10720/26561 [===========>..................] - ETA: 1s
11648/26561 [============>.................] - ETA: 1s
12576/26561 [=============>................] - ETA: 1s
13504/26561 [==============>...............] - ETA: 0s
14432/26561 [===============>..............] - ETA: 0s
15168/26561 [================>.............] - ETA: 0s
16096/26561 [=================>............] - ETA: 0s
17024/26561 [==================>...........] - ETA: 0s
17888/26561 [===================>..........] - ETA: 0s
18656/26561 [====================>.........] - ETA: 0s
19488/26561 [=====================>........] - ETA: 0s
20448/26561 [======================>.......] - ETA: 0s
21120/26561 [======================>.......] - ETA: 0s
22080/26561 [=======================>......] - ETA: 0s
23072/26561 [=========================>....] - ETA: 0s
24064/26561 [==========================>...] - ETA: 0s
25056/26561 [===========================>..] - ETA: 0s
26048/26561 [============================>.] - ETA: 0s
26561/26561 [==============================] - 2s 67us/step

acc: 65.35%
NeuralNet Accuracy: 65.35% (62.29%) with {'loss': 'binary_crossentropy', 'optimizer': 'adam', 'num_hidden': 3, 'activation': 'tanh', 'batch_size': 64, 'val_score': 65.35198348956963, 'train_score': 66.81977335190693, 'max_epochs': 100, 'hidden_layer_width': 32}
MAX: 66.3991439313
=================================================
{   'activation': 'tanh',
    'batch_size': 512,
    'hidden_layer_width': 32,
    'loss': 'binary_crossentropy',
    'max_epochs': 100,
    'num_hidden': 3,
    'optimizer': 'adam'}

   32/13083 [..............................] - ETA: 1s
  608/13083 [>.............................] - ETA: 1s
 1504/13083 [==>...........................] - ETA: 0s
 2432/13083 [====>.........................] - ETA: 0s
 3360/13083 [======>.......................] - ETA: 0s
 4288/13083 [========>.....................] - ETA: 0s
 5216/13083 [==========>...................] - ETA: 0s
 5984/13083 [============>.................] - ETA: 0s
 6912/13083 [==============>...............] - ETA: 0s
 7840/13083 [================>.............] - ETA: 0s
 8704/13083 [==================>...........] - ETA: 0s
 9504/13083 [====================>.........] - ETA: 0s
10336/13083 [======================>.......] - ETA: 0s
11296/13083 [========================>.....] - ETA: 0s
12000/13083 [==========================>...] - ETA: 0s
12928/13083 [============================>.] - ETA: 0s
13083/13083 [==============================] - 1s 59us/step

   32/26561 [..............................] - ETA: 1s
  992/26561 [>.............................] - ETA: 1s
 1952/26561 [=>............................] - ETA: 1s
 2912/26561 [==>...........................] - ETA: 1s
 3904/26561 [===>..........................] - ETA: 1s
 4896/26561 [====>.........................] - ETA: 1s
 5888/26561 [=====>........................] - ETA: 1s
 6880/26561 [======>.......................] - ETA: 1s
 7872/26561 [=======>......................] - ETA: 0s
 8864/26561 [=========>....................] - ETA: 0s
 9856/26561 [==========>...................] - ETA: 0s
10848/26561 [===========>..................] - ETA: 0s
11840/26561 [============>.................] - ETA: 0s
12832/26561 [=============>................] - ETA: 0s
13824/26561 [==============>...............] - ETA: 0s
14816/26561 [===============>..............] - ETA: 0s
15808/26561 [================>.............] - ETA: 0s
16736/26561 [=================>............] - ETA: 0s
17184/26561 [==================>...........] - ETA: 0s
17536/26561 [==================>...........] - ETA: 0s
17920/26561 [===================>..........] - ETA: 0s
18240/26561 [===================>..........] - ETA: 0s
18816/26561 [====================>.........] - ETA: 0s
19744/26561 [=====================>........] - ETA: 0s
20032/26561 [=====================>........] - ETA: 0s
20416/26561 [======================>.......] - ETA: 0s
20768/26561 [======================>.......] - ETA: 0s
21568/26561 [=======================>......] - ETA: 0s
22240/26561 [========================>.....] - ETA: 0s
23168/26561 [=========================>....] - ETA: 0s
24096/26561 [==========================>...] - ETA: 0s
25024/26561 [===========================>..] - ETA: 0s
25952/26561 [============================>.] - ETA: 0s
26561/26561 [==============================] - 2s 63us/step

acc: 65.47%
NeuralNet Accuracy: 65.47% (62.02%) with {'loss': 'binary_crossentropy', 'optimizer': 'adam', 'num_hidden': 3, 'activation': 'tanh', 'batch_size': 512, 'val_score': 65.47427960358054, 'train_score': 67.18497044538985, 'max_epochs': 100, 'hidden_layer_width': 32}
MAX: 66.3991439313
=================================================
{   'activation': 'tanh',
    'batch_size': 64,
    'hidden_layer_width': 64,
    'loss': 'binary_crossentropy',
    'max_epochs': 100,
    'num_hidden': 3,
    'optimizer': 'adam'}

   32/13083 [..............................] - ETA: 1s
  928/13083 [=>............................] - ETA: 0s
 1920/13083 [===>..........................] - ETA: 0s
 2912/13083 [=====>........................] - ETA: 0s
 3904/13083 [=======>......................] - ETA: 0s
 4896/13083 [==========>...................] - ETA: 0s
 5888/13083 [============>.................] - ETA: 0s
 6880/13083 [==============>...............] - ETA: 0s
 7840/13083 [================>.............] - ETA: 0s
 8832/13083 [===================>..........] - ETA: 0s
 9824/13083 [=====================>........] - ETA: 0s
10816/13083 [=======================>......] - ETA: 0s
11808/13083 [==========================>...] - ETA: 0s
12832/13083 [============================>.] - ETA: 0s
13083/13083 [==============================] - 1s 52us/step

   32/26561 [..............................] - ETA: 4s
  640/26561 [..............................] - ETA: 2s
  992/26561 [>.............................] - ETA: 2s
 1312/26561 [>.............................] - ETA: 3s
 1632/26561 [>.............................] - ETA: 3s
 1984/26561 [=>............................] - ETA: 3s
 2912/26561 [==>...........................] - ETA: 2s
 3360/26561 [==>...........................] - ETA: 2s
 3648/26561 [===>..........................] - ETA: 2s
 3968/26561 [===>..........................] - ETA: 2s
 4640/26561 [====>.........................] - ETA: 2s
 5216/26561 [====>.........................] - ETA: 2s
 6112/26561 [=====>........................] - ETA: 2s
 7040/26561 [======>.......................] - ETA: 1s
 7968/26561 [=======>......................] - ETA: 1s
 8896/26561 [=========>....................] - ETA: 1s
 9824/26561 [==========>...................] - ETA: 1s
10560/26561 [==========>...................] - ETA: 1s
11456/26561 [===========>..................] - ETA: 1s
12384/26561 [============>.................] - ETA: 1s
13248/26561 [=============>................] - ETA: 1s
14016/26561 [==============>...............] - ETA: 0s
14848/26561 [===============>..............] - ETA: 0s
15808/26561 [================>.............] - ETA: 0s
16512/26561 [=================>............] - ETA: 0s
17504/26561 [==================>...........] - ETA: 0s
18496/26561 [===================>..........] - ETA: 0s
19488/26561 [=====================>........] - ETA: 0s
20480/26561 [======================>.......] - ETA: 0s
21472/26561 [=======================>......] - ETA: 0s
22464/26561 [========================>.....] - ETA: 0s
23456/26561 [=========================>....] - ETA: 0s
24448/26561 [==========================>...] - ETA: 0s
25440/26561 [===========================>..] - ETA: 0s
26432/26561 [============================>.] - ETA: 0s
26561/26561 [==============================] - 2s 66us/step

acc: 65.42%
NeuralNet Accuracy: 65.42% (62.04%) with {'loss': 'binary_crossentropy', 'optimizer': 'adam', 'num_hidden': 3, 'activation': 'tanh', 'batch_size': 64, 'val_score': 65.42077505706074, 'train_score': 67.2753284891382, 'max_epochs': 100, 'hidden_layer_width': 64}
MAX: 66.3991439313
=================================================
{   'activation': 'tanh',
    'batch_size': 512,
    'hidden_layer_width': 64,
    'loss': 'binary_crossentropy',
    'max_epochs': 100,
    'num_hidden': 3,
    'optimizer': 'adam'}

   32/13083 [..............................] - ETA: 1s
  768/13083 [>.............................] - ETA: 0s
 1696/13083 [==>...........................] - ETA: 0s
 2624/13083 [=====>........................] - ETA: 0s
 3488/13083 [======>.......................] - ETA: 0s
 4256/13083 [========>.....................] - ETA: 0s
 5088/13083 [==========>...................] - ETA: 0s
 6016/13083 [============>.................] - ETA: 0s
 6688/13083 [==============>...............] - ETA: 0s
 7648/13083 [================>.............] - ETA: 0s
 8640/13083 [==================>...........] - ETA: 0s
 9632/13083 [=====================>........] - ETA: 0s
10624/13083 [=======================>......] - ETA: 0s
11616/13083 [=========================>....] - ETA: 0s
12608/13083 [===========================>..] - ETA: 0s
13083/13083 [==============================] - 1s 57us/step

   32/26561 [..............................] - ETA: 1s
 1024/26561 [>.............................] - ETA: 1s
 2016/26561 [=>............................] - ETA: 1s
 3008/26561 [==>...........................] - ETA: 1s
 4000/26561 [===>..........................] - ETA: 1s
 4992/26561 [====>.........................] - ETA: 1s
 5984/26561 [=====>........................] - ETA: 1s
 6976/26561 [======>.......................] - ETA: 1s
 7968/26561 [=======>......................] - ETA: 0s
 8960/26561 [=========>....................] - ETA: 0s
 9952/26561 [==========>...................] - ETA: 0s
10944/26561 [===========>..................] - ETA: 0s
11808/26561 [============>.................] - ETA: 0s
12096/26561 [============>.................] - ETA: 0s
12416/26561 [=============>................] - ETA: 0s
12736/26561 [=============>................] - ETA: 0s
13056/26561 [=============>................] - ETA: 0s
13568/26561 [==============>...............] - ETA: 0s
14496/26561 [===============>..............] - ETA: 0s
14848/26561 [===============>..............] - ETA: 0s
15136/26561 [================>.............] - ETA: 0s
15456/26561 [================>.............] - ETA: 0s
16128/26561 [=================>............] - ETA: 0s
16704/26561 [=================>............] - ETA: 0s
17632/26561 [==================>...........] - ETA: 0s
18560/26561 [===================>..........] - ETA: 0s
19488/26561 [=====================>........] - ETA: 0s
20416/26561 [======================>.......] - ETA: 0s
21344/26561 [=======================>......] - ETA: 0s
22080/26561 [=======================>......] - ETA: 0s
22976/26561 [========================>.....] - ETA: 0s
23872/26561 [=========================>....] - ETA: 0s
24736/26561 [==========================>...] - ETA: 0s
25536/26561 [===========================>..] - ETA: 0s
26336/26561 [============================>.] - ETA: 0s
26561/26561 [==============================] - 2s 66us/step

acc: 65.37%
NeuralNet Accuracy: 65.37% (62.12%) with {'loss': 'binary_crossentropy', 'optimizer': 'adam', 'num_hidden': 3, 'activation': 'tanh', 'batch_size': 512, 'val_score': 65.36727050325152, 'train_score': 66.81600843341742, 'max_epochs': 100, 'hidden_layer_width': 64}
MAX: 66.3991439313
=================================================
{   'activation': 'relu',
    'batch_size': 64,
    'hidden_layer_width': 16,
    'loss': 'mse',
    'max_epochs': 100,
    'num_hidden': 1,
    'optimizer': 'adam'}

   32/13083 [..............................] - ETA: 0s
 1024/13083 [=>............................] - ETA: 0s
 1408/13083 [==>...........................] - ETA: 0s
 1792/13083 [===>..........................] - ETA: 1s
 2144/13083 [===>..........................] - ETA: 1s
 2496/13083 [====>.........................] - ETA: 1s
 3168/13083 [======>.......................] - ETA: 0s
 4224/13083 [========>.....................] - ETA: 0s
 4544/13083 [=========>....................] - ETA: 0s
 4832/13083 [==========>...................] - ETA: 0s
 5248/13083 [===========>..................] - ETA: 0s
 6144/13083 [=============>................] - ETA: 0s
 6880/13083 [==============>...............] - ETA: 0s
 7936/13083 [=================>............] - ETA: 0s
 8992/13083 [===================>..........] - ETA: 0s
10016/13083 [=====================>........] - ETA: 0s
11040/13083 [========================>.....] - ETA: 0s
11936/13083 [==========================>...] - ETA: 0s
12928/13083 [============================>.] - ETA: 0s
13083/13083 [==============================] - 1s 73us/step

   32/26561 [..............................] - ETA: 1s
 1088/26561 [>.............................] - ETA: 1s
 2080/26561 [=>............................] - ETA: 1s
 2944/26561 [==>...........................] - ETA: 1s
 3840/26561 [===>..........................] - ETA: 1s
 4928/26561 [====>.........................] - ETA: 1s
 5920/26561 [=====>........................] - ETA: 1s
 6816/26561 [======>.......................] - ETA: 1s
 7968/26561 [=======>......................] - ETA: 0s
 9120/26561 [=========>....................] - ETA: 0s
10272/26561 [==========>...................] - ETA: 0s
11392/26561 [===========>..................] - ETA: 0s
12512/26561 [=============>................] - ETA: 0s
13664/26561 [==============>...............] - ETA: 0s
14816/26561 [===============>..............] - ETA: 0s
15936/26561 [================>.............] - ETA: 0s
17088/26561 [==================>...........] - ETA: 0s
18240/26561 [===================>..........] - ETA: 0s
19360/26561 [====================>.........] - ETA: 0s
20512/26561 [======================>.......] - ETA: 0s
21664/26561 [=======================>......] - ETA: 0s
22816/26561 [========================>.....] - ETA: 0s
23968/26561 [==========================>...] - ETA: 0s
25120/26561 [===========================>..] - ETA: 0s
26272/26561 [============================>.] - ETA: 0s
26561/26561 [==============================] - 1s 47us/step

acc: 65.77%
NeuralNet Accuracy: 65.77% (21.43%) with {'loss': 'mse', 'optimizer': 'adam', 'num_hidden': 1, 'activation': 'relu', 'batch_size': 64, 'val_score': 65.77237637037715, 'train_score': 68.00195775761455, 'max_epochs': 100, 'hidden_layer_width': 16}
MAX: 66.3991439313
=================================================
{   'activation': 'relu',
    'batch_size': 512,
    'hidden_layer_width': 16,
    'loss': 'mse',
    'max_epochs': 100,
    'num_hidden': 1,
    'optimizer': 'adam'}

   32/13083 [..............................] - ETA: 1s
 1152/13083 [=>............................] - ETA: 0s
 2272/13083 [====>.........................] - ETA: 0s
 3424/13083 [======>.......................] - ETA: 0s
 4576/13083 [=========>....................] - ETA: 0s
 5728/13083 [============>.................] - ETA: 0s
 6880/13083 [==============>...............] - ETA: 0s
 8032/13083 [=================>............] - ETA: 0s
 9184/13083 [====================>.........] - ETA: 0s
10304/13083 [======================>.......] - ETA: 0s
11424/13083 [=========================>....] - ETA: 0s
12544/13083 [===========================>..] - ETA: 0s
13083/13083 [==============================] - 1s 45us/step

   32/26561 [..............................] - ETA: 1s
 1152/26561 [>.............................] - ETA: 1s
 2272/26561 [=>............................] - ETA: 1s
 3392/26561 [==>...........................] - ETA: 1s
 4512/26561 [====>.........................] - ETA: 0s
 5632/26561 [=====>........................] - ETA: 0s
 6304/26561 [======>.......................] - ETA: 0s
 6656/26561 [======>.......................] - ETA: 1s
 7008/26561 [======>.......................] - ETA: 1s
 7328/26561 [=======>......................] - ETA: 1s
 7648/26561 [=======>......................] - ETA: 1s
 8512/26561 [========>.....................] - ETA: 1s
 9344/26561 [=========>....................] - ETA: 1s
 9664/26561 [=========>....................] - ETA: 1s
 9984/26561 [==========>...................] - ETA: 1s
10432/26561 [==========>...................] - ETA: 1s
11200/26561 [===========>..................] - ETA: 1s
12160/26561 [============>.................] - ETA: 1s
13184/26561 [=============>................] - ETA: 0s
14208/26561 [===============>..............] - ETA: 0s
15232/26561 [================>.............] - ETA: 0s
16288/26561 [=================>............] - ETA: 0s
17120/26561 [==================>...........] - ETA: 0s
18144/26561 [===================>..........] - ETA: 0s
19168/26561 [====================>.........] - ETA: 0s
20128/26561 [=====================>........] - ETA: 0s
20960/26561 [======================>.......] - ETA: 0s
21888/26561 [=======================>......] - ETA: 0s
22976/26561 [========================>.....] - ETA: 0s
23936/26561 [==========================>...] - ETA: 0s
24832/26561 [===========================>..] - ETA: 0s
25952/26561 [============================>.] - ETA: 0s
26561/26561 [==============================] - 2s 61us/step

acc: 65.73%
NeuralNet Accuracy: 65.73% (21.50%) with {'loss': 'mse', 'optimizer': 'adam', 'num_hidden': 1, 'activation': 'relu', 'batch_size': 512, 'val_score': 65.73415883617245, 'train_score': 68.28809156281767, 'max_epochs': 100, 'hidden_layer_width': 16}
MAX: 66.3991439313
=================================================
{   'activation': 'relu',
    'batch_size': 64,
    'hidden_layer_width': 32,
    'loss': 'mse',
    'max_epochs': 100,
    'num_hidden': 1,
    'optimizer': 'adam'}

   32/13083 [..............................] - ETA: 2s
  672/13083 [>.............................] - ETA: 1s
 1696/13083 [==>...........................] - ETA: 0s
 2016/13083 [===>..........................] - ETA: 0s
 2304/13083 [====>.........................] - ETA: 1s
 2656/13083 [=====>........................] - ETA: 1s
 3520/13083 [=======>......................] - ETA: 0s
 4384/13083 [=========>....................] - ETA: 0s
 5408/13083 [===========>..................] - ETA: 0s
 6432/13083 [=============>................] - ETA: 0s
 7456/13083 [================>.............] - ETA: 0s
 8512/13083 [==================>...........] - ETA: 0s
 9408/13083 [====================>.........] - ETA: 0s
10496/13083 [=======================>......] - ETA: 0s
11584/13083 [=========================>....] - ETA: 0s
12608/13083 [===========================>..] - ETA: 0s
13083/13083 [==============================] - 1s 61us/step

   32/26561 [..............................] - ETA: 1s
  960/26561 [>.............................] - ETA: 1s
 1888/26561 [=>............................] - ETA: 1s
 2944/26561 [==>...........................] - ETA: 1s
 3808/26561 [===>..........................] - ETA: 1s
 4928/26561 [====>.........................] - ETA: 1s
 6048/26561 [=====>........................] - ETA: 1s
 7168/26561 [=======>......................] - ETA: 0s
 8288/26561 [========>.....................] - ETA: 0s
 9408/26561 [=========>....................] - ETA: 0s
10528/26561 [==========>...................] - ETA: 0s
11680/26561 [============>.................] - ETA: 0s
12800/26561 [=============>................] - ETA: 0s
13920/26561 [==============>...............] - ETA: 0s
15040/26561 [===============>..............] - ETA: 0s
16160/26561 [=================>............] - ETA: 0s
17312/26561 [==================>...........] - ETA: 0s
18464/26561 [===================>..........] - ETA: 0s
19616/26561 [=====================>........] - ETA: 0s
20736/26561 [======================>.......] - ETA: 0s
21856/26561 [=======================>......] - ETA: 0s
22976/26561 [========================>.....] - ETA: 0s
24000/26561 [==========================>...] - ETA: 0s
24544/26561 [==========================>...] - ETA: 0s
24864/26561 [===========================>..] - ETA: 0s
25248/26561 [===========================>..] - ETA: 0s
25600/26561 [===========================>..] - ETA: 0s
26048/26561 [============================>.] - ETA: 0s
26561/26561 [==============================] - 1s 53us/step

acc: 65.74%
NeuralNet Accuracy: 65.74% (21.53%) with {'loss': 'mse', 'optimizer': 'adam', 'num_hidden': 1, 'activation': 'relu', 'batch_size': 64, 'val_score': 65.74180233526839, 'train_score': 68.84529949926585, 'max_epochs': 100, 'hidden_layer_width': 32}
MAX: 66.3991439313
=================================================
{   'activation': 'relu',
    'batch_size': 512,
    'hidden_layer_width': 32,
    'loss': 'mse',
    'max_epochs': 100,
    'num_hidden': 1,
    'optimizer': 'adam'}

   32/13083 [..............................] - ETA: 1s
  992/13083 [=>............................] - ETA: 0s
 1888/13083 [===>..........................] - ETA: 0s
 2848/13083 [=====>........................] - ETA: 0s
 3808/13083 [=======>......................] - ETA: 0s
 4736/13083 [=========>....................] - ETA: 0s
 5856/13083 [============>.................] - ETA: 0s
 6976/13083 [==============>...............] - ETA: 0s
 8096/13083 [=================>............] - ETA: 0s
 9216/13083 [====================>.........] - ETA: 0s
10336/13083 [======================>.......] - ETA: 0s
11456/13083 [=========================>....] - ETA: 0s
12576/13083 [===========================>..] - ETA: 0s
13083/13083 [==============================] - 1s 48us/step

   32/26561 [..............................] - ETA: 1s
 1152/26561 [>.............................] - ETA: 1s
 2272/26561 [=>............................] - ETA: 1s
 3392/26561 [==>...........................] - ETA: 1s
 4512/26561 [====>.........................] - ETA: 1s
 5632/26561 [=====>........................] - ETA: 0s
 6688/26561 [======>.......................] - ETA: 0s
 7808/26561 [=======>......................] - ETA: 0s
 8928/26561 [=========>....................] - ETA: 0s
10016/26561 [==========>...................] - ETA: 0s
11040/26561 [===========>..................] - ETA: 0s
11616/26561 [============>.................] - ETA: 0s
11936/26561 [============>.................] - ETA: 0s
12288/26561 [============>.................] - ETA: 0s
12640/26561 [=============>................] - ETA: 0s
13120/26561 [=============>................] - ETA: 0s
14176/26561 [===============>..............] - ETA: 0s
14656/26561 [===============>..............] - ETA: 0s
14976/26561 [===============>..............] - ETA: 0s
15328/26561 [================>.............] - ETA: 0s
16096/26561 [=================>............] - ETA: 0s
16736/26561 [=================>............] - ETA: 0s
17760/26561 [===================>..........] - ETA: 0s
18784/26561 [====================>.........] - ETA: 0s
19840/26561 [=====================>........] - ETA: 0s
20896/26561 [======================>.......] - ETA: 0s
21952/26561 [=======================>......] - ETA: 0s
22784/26561 [========================>.....] - ETA: 0s
23840/26561 [=========================>....] - ETA: 0s
24896/26561 [===========================>..] - ETA: 0s
25888/26561 [============================>.] - ETA: 0s
26561/26561 [==============================] - 2s 60us/step

acc: 65.87%
NeuralNet Accuracy: 65.87% (21.48%) with {'loss': 'mse', 'optimizer': 'adam', 'num_hidden': 1, 'activation': 'relu', 'batch_size': 512, 'val_score': 65.87174195475346, 'train_score': 68.78882572192312, 'max_epochs': 100, 'hidden_layer_width': 32}
MAX: 66.3991439313
=================================================
{   'activation': 'relu',
    'batch_size': 64,
    'hidden_layer_width': 64,
    'loss': 'mse',
    'max_epochs': 100,
    'num_hidden': 1,
    'optimizer': 'adam'}

   32/13083 [..............................] - ETA: 1s
 1152/13083 [=>............................] - ETA: 0s
 2272/13083 [====>.........................] - ETA: 0s
 3392/13083 [======>.......................] - ETA: 0s
 4512/13083 [=========>....................] - ETA: 0s
 5632/13083 [===========>..................] - ETA: 0s
 6752/13083 [==============>...............] - ETA: 0s
 7872/13083 [=================>............] - ETA: 0s
 8992/13083 [===================>..........] - ETA: 0s
10112/13083 [======================>.......] - ETA: 0s
11232/13083 [========================>.....] - ETA: 0s
12352/13083 [===========================>..] - ETA: 0s
13083/13083 [==============================] - 1s 45us/step

   32/26561 [..............................] - ETA: 1s
 1152/26561 [>.............................] - ETA: 1s
 2272/26561 [=>............................] - ETA: 1s
 3392/26561 [==>...........................] - ETA: 1s
 4512/26561 [====>.........................] - ETA: 0s
 5216/26561 [====>.........................] - ETA: 1s
 5568/26561 [=====>........................] - ETA: 1s
 5952/26561 [=====>........................] - ETA: 1s
 6304/26561 [======>.......................] - ETA: 1s
 6624/26561 [======>.......................] - ETA: 1s
 7488/26561 [=======>......................] - ETA: 1s
 8320/26561 [========>.....................] - ETA: 1s
 8608/26561 [========>.....................] - ETA: 1s
 8896/26561 [=========>....................] - ETA: 1s
 9344/26561 [=========>....................] - ETA: 1s
10080/26561 [==========>...................] - ETA: 1s
11008/26561 [===========>..................] - ETA: 1s
12064/26561 [============>.................] - ETA: 1s
13120/26561 [=============>................] - ETA: 0s
14176/26561 [===============>..............] - ETA: 0s
15232/26561 [================>.............] - ETA: 0s
16096/26561 [=================>............] - ETA: 0s
17152/26561 [==================>...........] - ETA: 0s
18208/26561 [===================>..........] - ETA: 0s
19200/26561 [====================>.........] - ETA: 0s
20096/26561 [=====================>........] - ETA: 0s
21056/26561 [======================>.......] - ETA: 0s
22144/26561 [========================>.....] - ETA: 0s
23040/26561 [=========================>....] - ETA: 0s
24096/26561 [==========================>...] - ETA: 0s
25216/26561 [===========================>..] - ETA: 0s
26336/26561 [============================>.] - ETA: 0s
26561/26561 [==============================] - 2s 60us/step

acc: 65.97%
NeuralNet Accuracy: 65.97% (21.43%) with {'loss': 'mse', 'optimizer': 'adam', 'num_hidden': 1, 'activation': 'relu', 'batch_size': 64, 'val_score': 65.97110754049655, 'train_score': 69.9634802906517, 'max_epochs': 100, 'hidden_layer_width': 64}
MAX: 66.3991439313
=================================================
{   'activation': 'relu',
    'batch_size': 512,
    'hidden_layer_width': 64,
    'loss': 'mse',
    'max_epochs': 100,
    'num_hidden': 1,
    'optimizer': 'adam'}

   32/13083 [..............................] - ETA: 1s
 1056/13083 [=>............................] - ETA: 0s
 2016/13083 [===>..........................] - ETA: 0s
 3040/13083 [=====>........................] - ETA: 0s
 4064/13083 [========>.....................] - ETA: 0s
 4896/13083 [==========>...................] - ETA: 0s
 5920/13083 [============>.................] - ETA: 0s
 6944/13083 [==============>...............] - ETA: 0s
 7904/13083 [=================>............] - ETA: 0s
 8768/13083 [===================>..........] - ETA: 0s
 9664/13083 [=====================>........] - ETA: 0s
10752/13083 [=======================>......] - ETA: 0s
11520/13083 [=========================>....] - ETA: 0s
12576/13083 [===========================>..] - ETA: 0s
13083/13083 [==============================] - 1s 52us/step

   32/26561 [..............................] - ETA: 1s
 1152/26561 [>.............................] - ETA: 1s
 2272/26561 [=>............................] - ETA: 1s
 3392/26561 [==>...........................] - ETA: 1s
 4512/26561 [====>.........................] - ETA: 0s
 5632/26561 [=====>........................] - ETA: 0s
 6752/26561 [======>.......................] - ETA: 0s
 7872/26561 [=======>......................] - ETA: 0s
 8992/26561 [=========>....................] - ETA: 0s
10112/26561 [==========>...................] - ETA: 0s
11232/26561 [===========>..................] - ETA: 0s
12352/26561 [============>.................] - ETA: 0s
13472/26561 [==============>...............] - ETA: 0s
14592/26561 [===============>..............] - ETA: 0s
15712/26561 [================>.............] - ETA: 0s
16832/26561 [==================>...........] - ETA: 0s
17952/26561 [===================>..........] - ETA: 0s
18944/26561 [====================>.........] - ETA: 0s
19328/26561 [====================>.........] - ETA: 0s
19648/26561 [=====================>........] - ETA: 0s
20000/26561 [=====================>........] - ETA: 0s
20352/26561 [=====================>........] - ETA: 0s
20992/26561 [======================>.......] - ETA: 0s
22016/26561 [=======================>......] - ETA: 0s
22336/26561 [========================>.....] - ETA: 0s
22656/26561 [========================>.....] - ETA: 0s
22976/26561 [========================>.....] - ETA: 0s
23904/26561 [=========================>....] - ETA: 0s
24800/26561 [===========================>..] - ETA: 0s
25824/26561 [============================>.] - ETA: 0s
26561/26561 [==============================] - 2s 58us/step

acc: 65.56%
NeuralNet Accuracy: 65.56% (21.48%) with {'loss': 'mse', 'optimizer': 'adam', 'num_hidden': 1, 'activation': 'relu', 'batch_size': 512, 'val_score': 65.55835817883087, 'train_score': 69.48533564248334, 'max_epochs': 100, 'hidden_layer_width': 64}
MAX: 66.3991439313
=================================================
{   'activation': 'relu',
    'batch_size': 64,
    'hidden_layer_width': 16,
    'loss': 'mse',
    'max_epochs': 100,
    'num_hidden': 2,
    'optimizer': 'adam'}

   32/13083 [..............................] - ETA: 1s
  640/13083 [>.............................] - ETA: 1s
 1600/13083 [==>...........................] - ETA: 0s
 2592/13083 [====>.........................] - ETA: 0s
 3584/13083 [=======>......................] - ETA: 0s
 4544/13083 [=========>....................] - ETA: 0s
 5568/13083 [===========>..................] - ETA: 0s
 6400/13083 [=============>................] - ETA: 0s
 7424/13083 [================>.............] - ETA: 0s
 8448/13083 [==================>...........] - ETA: 0s
 9408/13083 [====================>.........] - ETA: 0s
10272/13083 [======================>.......] - ETA: 0s
11200/13083 [========================>.....] - ETA: 0s
12128/13083 [==========================>...] - ETA: 0s
12992/13083 [============================>.] - ETA: 0s
13083/13083 [==============================] - 1s 55us/step

   32/26561 [..............................] - ETA: 1s
 1120/26561 [>.............................] - ETA: 1s
 2208/26561 [=>............................] - ETA: 1s
 3296/26561 [==>...........................] - ETA: 1s
 4384/26561 [===>..........................] - ETA: 1s
 5472/26561 [=====>........................] - ETA: 0s
 6560/26561 [======>.......................] - ETA: 0s
 7648/26561 [=======>......................] - ETA: 0s
 8736/26561 [========>.....................] - ETA: 0s
 9824/26561 [==========>...................] - ETA: 0s
10912/26561 [===========>..................] - ETA: 0s
12000/26561 [============>.................] - ETA: 0s
13088/26561 [=============>................] - ETA: 0s
14144/26561 [==============>...............] - ETA: 0s
15200/26561 [================>.............] - ETA: 0s
16256/26561 [=================>............] - ETA: 0s
17312/26561 [==================>...........] - ETA: 0s
18400/26561 [===================>..........] - ETA: 0s
19264/26561 [====================>.........] - ETA: 0s
19584/26561 [=====================>........] - ETA: 0s
19904/26561 [=====================>........] - ETA: 0s
20224/26561 [=====================>........] - ETA: 0s
20544/26561 [======================>.......] - ETA: 0s
21216/26561 [======================>.......] - ETA: 0s
22144/26561 [========================>.....] - ETA: 0s
22496/26561 [========================>.....] - ETA: 0s
22784/26561 [========================>.....] - ETA: 0s
23104/26561 [=========================>....] - ETA: 0s
24000/26561 [==========================>...] - ETA: 0s
24832/26561 [===========================>..] - ETA: 0s
25856/26561 [============================>.] - ETA: 0s
26561/26561 [==============================] - 2s 60us/step

acc: 65.95%
NeuralNet Accuracy: 65.95% (21.42%) with {'loss': 'mse', 'optimizer': 'adam', 'num_hidden': 2, 'activation': 'relu', 'batch_size': 64, 'val_score': 65.94817702316284, 'train_score': 68.00948759459358, 'max_epochs': 100, 'hidden_layer_width': 16}
MAX: 66.3991439313
=================================================
{   'activation': 'relu',
    'batch_size': 512,
    'hidden_layer_width': 16,
    'loss': 'mse',
    'max_epochs': 100,
    'num_hidden': 2,
    'optimizer': 'adam'}

   32/13083 [..............................] - ETA: 1s
 1120/13083 [=>............................] - ETA: 0s
 2208/13083 [====>.........................] - ETA: 0s
 3296/13083 [======>.......................] - ETA: 0s
 4384/13083 [=========>....................] - ETA: 0s
 5472/13083 [===========>..................] - ETA: 0s
 6560/13083 [==============>...............] - ETA: 0s
 7648/13083 [================>.............] - ETA: 0s
 8736/13083 [===================>..........] - ETA: 0s
 9824/13083 [=====================>........] - ETA: 0s
10912/13083 [========================>.....] - ETA: 0s
12000/13083 [==========================>...] - ETA: 0s
12992/13083 [============================>.] - ETA: 0s
13083/13083 [==============================] - 1s 47us/step

   32/26561 [..............................] - ETA: 1s
  416/26561 [..............................] - ETA: 3s
  800/26561 [..............................] - ETA: 3s
 1184/26561 [>.............................] - ETA: 3s
 1568/26561 [>.............................] - ETA: 3s
 2112/26561 [=>............................] - ETA: 3s
 3136/26561 [==>...........................] - ETA: 2s
 3520/26561 [==>...........................] - ETA: 2s
 3808/26561 [===>..........................] - ETA: 2s
 4192/26561 [===>..........................] - ETA: 2s
 4960/26561 [====>.........................] - ETA: 2s
 5568/26561 [=====>........................] - ETA: 2s
 6560/26561 [======>.......................] - ETA: 1s
 7584/26561 [=======>......................] - ETA: 1s
 8576/26561 [========>.....................] - ETA: 1s
 9568/26561 [=========>....................] - ETA: 1s
10560/26561 [==========>...................] - ETA: 1s
11360/26561 [===========>..................] - ETA: 1s
12384/26561 [============>.................] - ETA: 1s
13408/26561 [==============>...............] - ETA: 0s
14368/26561 [===============>..............] - ETA: 0s
15200/26561 [================>.............] - ETA: 0s
16096/26561 [=================>............] - ETA: 0s
17152/26561 [==================>...........] - ETA: 0s
17920/26561 [===================>..........] - ETA: 0s
18976/26561 [====================>.........] - ETA: 0s
20064/26561 [=====================>........] - ETA: 0s
21152/26561 [======================>.......] - ETA: 0s
22240/26561 [========================>.....] - ETA: 0s
23360/26561 [=========================>....] - ETA: 0s
24448/26561 [==========================>...] - ETA: 0s
25536/26561 [===========================>..] - ETA: 0s
26561/26561 [==============================] - 2s 61us/step

acc: 66.02%
NeuralNet Accuracy: 66.02% (21.36%) with {'loss': 'mse', 'optimizer': 'adam', 'num_hidden': 2, 'activation': 'relu', 'batch_size': 512, 'val_score': 66.02461209612812, 'train_score': 68.14502466021611, 'max_epochs': 100, 'hidden_layer_width': 16}
MAX: 66.3991439313
=================================================
{   'activation': 'relu',
    'batch_size': 64,
    'hidden_layer_width': 32,
    'loss': 'mse',
    'max_epochs': 100,
    'num_hidden': 2,
    'optimizer': 'adam'}

   32/13083 [..............................] - ETA: 1s
  928/13083 [=>............................] - ETA: 0s
 1984/13083 [===>..........................] - ETA: 0s
 2880/13083 [=====>........................] - ETA: 0s
 3872/13083 [=======>......................] - ETA: 0s
 4928/13083 [==========>...................] - ETA: 0s
 5984/13083 [============>.................] - ETA: 0s
 7040/13083 [===============>..............] - ETA: 0s
 8096/13083 [=================>............] - ETA: 0s
 9184/13083 [====================>.........] - ETA: 0s
10240/13083 [======================>.......] - ETA: 0s
11296/13083 [========================>.....] - ETA: 0s
12352/13083 [===========================>..] - ETA: 0s
13083/13083 [==============================] - 1s 50us/step

   32/26561 [..............................] - ETA: 1s
 1088/26561 [>.............................] - ETA: 1s
 2144/26561 [=>............................] - ETA: 1s
 3200/26561 [==>...........................] - ETA: 1s
 4256/26561 [===>..........................] - ETA: 1s
 5312/26561 [====>.........................] - ETA: 1s
 6368/26561 [======>.......................] - ETA: 0s
 7424/26561 [=======>......................] - ETA: 0s
 8480/26561 [========>.....................] - ETA: 0s
 9408/26561 [=========>....................] - ETA: 0s
 9760/26561 [==========>...................] - ETA: 0s
10048/26561 [==========>...................] - ETA: 0s
10400/26561 [==========>...................] - ETA: 0s
10720/26561 [===========>..................] - ETA: 0s
11168/26561 [===========>..................] - ETA: 0s
12160/26561 [============>.................] - ETA: 0s
12640/26561 [=============>................] - ETA: 0s
12960/26561 [=============>................] - ETA: 0s
13312/26561 [==============>...............] - ETA: 0s
14048/26561 [==============>...............] - ETA: 0s
14656/26561 [===============>..............] - ETA: 0s
15680/26561 [================>.............] - ETA: 0s
16704/26561 [=================>............] - ETA: 0s
17728/26561 [===================>..........] - ETA: 0s
18752/26561 [====================>.........] - ETA: 0s
19776/26561 [=====================>........] - ETA: 0s
20608/26561 [======================>.......] - ETA: 0s
21632/26561 [=======================>......] - ETA: 0s
22624/26561 [========================>.....] - ETA: 0s
23552/26561 [=========================>....] - ETA: 0s
24352/26561 [==========================>...] - ETA: 0s
25216/26561 [===========================>..] - ETA: 0s
26208/26561 [============================>.] - ETA: 0s
26561/26561 [==============================] - 2s 63us/step

acc: 65.32%
NeuralNet Accuracy: 65.32% (21.76%) with {'loss': 'mse', 'optimizer': 'adam', 'num_hidden': 2, 'activation': 'relu', 'batch_size': 64, 'val_score': 65.32140946220588, 'train_score': 69.58698844170024, 'max_epochs': 100, 'hidden_layer_width': 32}
MAX: 66.3991439313
=================================================
{   'activation': 'relu',
    'batch_size': 512,
    'hidden_layer_width': 32,
    'loss': 'mse',
    'max_epochs': 100,
    'num_hidden': 2,
    'optimizer': 'adam'}

   32/13083 [..............................] - ETA: 1s
  960/13083 [=>............................] - ETA: 0s
 1984/13083 [===>..........................] - ETA: 0s
 2752/13083 [=====>........................] - ETA: 0s
 3840/13083 [=======>......................] - ETA: 0s
 4928/13083 [==========>...................] - ETA: 0s
 6016/13083 [============>.................] - ETA: 0s
 7072/13083 [===============>..............] - ETA: 0s
 8128/13083 [=================>............] - ETA: 0s
 9184/13083 [====================>.........] - ETA: 0s
10240/13083 [======================>.......] - ETA: 0s
11328/13083 [========================>.....] - ETA: 0s
12416/13083 [===========================>..] - ETA: 0s
13083/13083 [==============================] - 1s 49us/step

   32/26561 [..............................] - ETA: 1s
 1120/26561 [>.............................] - ETA: 1s
 2208/26561 [=>............................] - ETA: 1s
 3296/26561 [==>...........................] - ETA: 1s
 4384/26561 [===>..........................] - ETA: 1s
 5472/26561 [=====>........................] - ETA: 0s
 6560/26561 [======>.......................] - ETA: 0s
 7648/26561 [=======>......................] - ETA: 0s
 8736/26561 [========>.....................] - ETA: 0s
 9536/26561 [=========>....................] - ETA: 0s
 9856/26561 [==========>...................] - ETA: 0s
10176/26561 [==========>...................] - ETA: 0s
10528/26561 [==========>...................] - ETA: 0s
10880/26561 [===========>..................] - ETA: 0s
11648/26561 [============>.................] - ETA: 0s
12544/26561 [=============>................] - ETA: 0s
12832/26561 [=============>................] - ETA: 0s
13120/26561 [=============>................] - ETA: 0s
13568/26561 [==============>...............] - ETA: 0s
14272/26561 [===============>..............] - ETA: 0s
15168/26561 [================>.............] - ETA: 0s
16160/26561 [=================>............] - ETA: 0s
17152/26561 [==================>...........] - ETA: 0s
18144/26561 [===================>..........] - ETA: 0s
19136/26561 [====================>.........] - ETA: 0s
19936/26561 [=====================>........] - ETA: 0s
20928/26561 [======================>.......] - ETA: 0s
21952/26561 [=======================>......] - ETA: 0s
22880/26561 [========================>.....] - ETA: 0s
23712/26561 [=========================>....] - ETA: 0s
24608/26561 [==========================>...] - ETA: 0s
25664/26561 [===========================>..] - ETA: 0s
26560/26561 [============================>.] - ETA: 0s
26561/26561 [==============================] - 2s 62us/step

acc: 65.75%
NeuralNet Accuracy: 65.75% (21.46%) with {'loss': 'mse', 'optimizer': 'adam', 'num_hidden': 2, 'activation': 'relu', 'batch_size': 512, 'val_score': 65.74944585122108, 'train_score': 69.22179134821731, 'max_epochs': 100, 'hidden_layer_width': 32}
MAX: 66.3991439313
=================================================
{   'activation': 'relu',
    'batch_size': 64,
    'hidden_layer_width': 64,
    'loss': 'mse',
    'max_epochs': 100,
    'num_hidden': 2,
    'optimizer': 'adam'}

   32/13083 [..............................] - ETA: 0s
 1120/13083 [=>............................] - ETA: 0s
 2208/13083 [====>.........................] - ETA: 0s
 3296/13083 [======>.......................] - ETA: 0s
 4288/13083 [========>.....................] - ETA: 0s
 4800/13083 [==========>...................] - ETA: 0s
 5120/13083 [==========>...................] - ETA: 0s
 5440/13083 [===========>..................] - ETA: 0s
 5760/13083 [============>.................] - ETA: 0s
 6176/13083 [=============>................] - ETA: 0s
 7200/13083 [===============>..............] - ETA: 0s
 8288/13083 [==================>...........] - ETA: 0s
 9248/13083 [====================>.........] - ETA: 0s
 9568/13083 [====================>.........] - ETA: 0s
 9888/13083 [=====================>........] - ETA: 0s
10336/13083 [======================>.......] - ETA: 0s
11040/13083 [========================>.....] - ETA: 0s
11904/13083 [==========================>...] - ETA: 0s
12896/13083 [============================>.] - ETA: 0s
13083/13083 [==============================] - 1s 72us/step

   32/26561 [..............................] - ETA: 1s
  992/26561 [>.............................] - ETA: 1s
 1952/26561 [=>............................] - ETA: 1s
 2816/26561 [==>...........................] - ETA: 1s
 3616/26561 [===>..........................] - ETA: 1s
 4640/26561 [====>.........................] - ETA: 1s
 5664/26561 [=====>........................] - ETA: 1s
 6624/26561 [======>.......................] - ETA: 1s
 7488/26561 [=======>......................] - ETA: 1s
 8416/26561 [========>.....................] - ETA: 0s
 9440/26561 [=========>....................] - ETA: 0s
10176/26561 [==========>...................] - ETA: 0s
11264/26561 [===========>..................] - ETA: 0s
12352/26561 [============>.................] - ETA: 0s
13440/26561 [==============>...............] - ETA: 0s
14528/26561 [===============>..............] - ETA: 0s
15616/26561 [================>.............] - ETA: 0s
16704/26561 [=================>............] - ETA: 0s
17792/26561 [===================>..........] - ETA: 0s
18880/26561 [====================>.........] - ETA: 0s
19968/26561 [=====================>........] - ETA: 0s
21056/26561 [======================>.......] - ETA: 0s
22144/26561 [========================>.....] - ETA: 0s
23232/26561 [=========================>....] - ETA: 0s
24320/26561 [==========================>...] - ETA: 0s
25408/26561 [===========================>..] - ETA: 0s
26496/26561 [============================>.] - ETA: 0s
26561/26561 [==============================] - 1s 50us/step

acc: 65.80%
NeuralNet Accuracy: 65.80% (21.45%) with {'loss': 'mse', 'optimizer': 'adam', 'num_hidden': 2, 'activation': 'relu', 'batch_size': 64, 'val_score': 65.8029503899959, 'train_score': 69.50416023493091, 'max_epochs': 100, 'hidden_layer_width': 64}
MAX: 66.3991439313
=================================================
{   'activation': 'relu',
    'batch_size': 512,
    'hidden_layer_width': 64,
    'loss': 'mse',
    'max_epochs': 100,
    'num_hidden': 2,
    'optimizer': 'adam'}

   32/13083 [..............................] - ETA: 1s
 1088/13083 [=>............................] - ETA: 0s
 2016/13083 [===>..........................] - ETA: 0s
 2912/13083 [=====>........................] - ETA: 0s
 4000/13083 [========>.....................] - ETA: 0s
 5088/13083 [==========>...................] - ETA: 0s
 6208/13083 [=============>................] - ETA: 0s
 7296/13083 [===============>..............] - ETA: 0s
 8384/13083 [==================>...........] - ETA: 0s
 9472/13083 [====================>.........] - ETA: 0s
10560/13083 [=======================>......] - ETA: 0s
11648/13083 [=========================>....] - ETA: 0s
12736/13083 [============================>.] - ETA: 0s
13083/13083 [==============================] - 1s 48us/step

   32/26561 [..............................] - ETA: 1s
 1120/26561 [>.............................] - ETA: 1s
 2208/26561 [=>............................] - ETA: 1s
 3264/26561 [==>...........................] - ETA: 1s
 4320/26561 [===>..........................] - ETA: 1s
 5376/26561 [=====>........................] - ETA: 1s
 6432/26561 [======>.......................] - ETA: 0s
 7488/26561 [=======>......................] - ETA: 0s
 8512/26561 [========>.....................] - ETA: 0s
 9152/26561 [=========>....................] - ETA: 0s
 9472/26561 [=========>....................] - ETA: 0s
 9824/26561 [==========>...................] - ETA: 0s
10176/26561 [==========>...................] - ETA: 1s
10560/26561 [==========>...................] - ETA: 1s
11520/26561 [============>.................] - ETA: 0s
12064/26561 [============>.................] - ETA: 0s
12352/26561 [============>.................] - ETA: 0s
12640/26561 [=============>................] - ETA: 0s
13248/26561 [=============>................] - ETA: 0s
13856/26561 [==============>...............] - ETA: 0s
14816/26561 [===============>..............] - ETA: 0s
15776/26561 [================>.............] - ETA: 0s
16768/26561 [=================>............] - ETA: 0s
17728/26561 [===================>..........] - ETA: 0s
18688/26561 [====================>.........] - ETA: 0s
19520/26561 [=====================>........] - ETA: 0s
20544/26561 [======================>.......] - ETA: 0s
21568/26561 [=======================>......] - ETA: 0s
22496/26561 [========================>.....] - ETA: 0s
23328/26561 [=========================>....] - ETA: 0s
24224/26561 [==========================>...] - ETA: 0s
25280/26561 [===========================>..] - ETA: 0s
26016/26561 [============================>.] - ETA: 0s
26561/26561 [==============================] - 2s 63us/step

acc: 65.55%
NeuralNet Accuracy: 65.55% (21.58%) with {'loss': 'mse', 'optimizer': 'adam', 'num_hidden': 2, 'activation': 'relu', 'batch_size': 512, 'val_score': 65.55071466424492, 'train_score': 70.50186363465231, 'max_epochs': 100, 'hidden_layer_width': 64}
MAX: 66.3991439313
=================================================
{   'activation': 'relu',
    'batch_size': 64,
    'hidden_layer_width': 16,
    'loss': 'mse',
    'max_epochs': 100,
    'num_hidden': 3,
    'optimizer': 'adam'}

   32/13083 [..............................] - ETA: 1s
 1024/13083 [=>............................] - ETA: 0s
 1920/13083 [===>..........................] - ETA: 0s
 2816/13083 [=====>........................] - ETA: 0s
 3808/13083 [=======>......................] - ETA: 0s
 4800/13083 [==========>...................] - ETA: 0s
 5696/13083 [============>.................] - ETA: 0s
 6592/13083 [==============>...............] - ETA: 0s
 7584/13083 [================>.............] - ETA: 0s
 8480/13083 [==================>...........] - ETA: 0s
 9344/13083 [====================>.........] - ETA: 0s
10432/13083 [======================>.......] - ETA: 0s
11520/13083 [=========================>....] - ETA: 0s
12608/13083 [===========================>..] - ETA: 0s
13083/13083 [==============================] - 1s 53us/step

   32/26561 [..............................] - ETA: 1s
 1120/26561 [>.............................] - ETA: 1s
 2208/26561 [=>............................] - ETA: 1s
 3264/26561 [==>...........................] - ETA: 1s
 4320/26561 [===>..........................] - ETA: 1s
 5408/26561 [=====>........................] - ETA: 1s
 6464/26561 [======>.......................] - ETA: 0s
 7520/26561 [=======>......................] - ETA: 0s
 8576/26561 [========>.....................] - ETA: 0s
 9632/26561 [=========>....................] - ETA: 0s
10688/26561 [===========>..................] - ETA: 0s
11776/26561 [============>.................] - ETA: 0s
12832/26561 [=============>................] - ETA: 0s
13888/26561 [==============>...............] - ETA: 0s
14880/26561 [===============>..............] - ETA: 0s
15520/26561 [================>.............] - ETA: 0s
15872/26561 [================>.............] - ETA: 0s
16224/26561 [=================>............] - ETA: 0s
16544/26561 [=================>............] - ETA: 0s
16928/26561 [==================>...........] - ETA: 0s
17856/26561 [===================>..........] - ETA: 0s
18496/26561 [===================>..........] - ETA: 0s
18848/26561 [====================>.........] - ETA: 0s
19168/26561 [====================>.........] - ETA: 0s
19808/26561 [=====================>........] - ETA: 0s
20480/26561 [======================>.......] - ETA: 0s
21440/26561 [=======================>......] - ETA: 0s
22432/26561 [========================>.....] - ETA: 0s
23424/26561 [=========================>....] - ETA: 0s
24448/26561 [==========================>...] - ETA: 0s
25440/26561 [===========================>..] - ETA: 0s
26240/26561 [============================>.] - ETA: 0s
26561/26561 [==============================] - 2s 60us/step

acc: 65.77%
NeuralNet Accuracy: 65.77% (21.50%) with {'loss': 'mse', 'optimizer': 'adam', 'num_hidden': 3, 'activation': 'relu', 'batch_size': 64, 'val_score': 65.77237637037715, 'train_score': 68.3219758292233, 'max_epochs': 100, 'hidden_layer_width': 16}
MAX: 66.3991439313
=================================================
{   'activation': 'relu',
    'batch_size': 512,
    'hidden_layer_width': 16,
    'loss': 'mse',
    'max_epochs': 100,
    'num_hidden': 3,
    'optimizer': 'adam'}

   32/13083 [..............................] - ETA: 1s
 1056/13083 [=>............................] - ETA: 0s
 2080/13083 [===>..........................] - ETA: 0s
 3104/13083 [======>.......................] - ETA: 0s
 4128/13083 [========>.....................] - ETA: 0s
 5152/13083 [==========>...................] - ETA: 0s
 6176/13083 [=============>................] - ETA: 0s
 7200/13083 [===============>..............] - ETA: 0s
 8256/13083 [=================>............] - ETA: 0s
 9312/13083 [====================>.........] - ETA: 0s
10368/13083 [======================>.......] - ETA: 0s
11424/13083 [=========================>....] - ETA: 0s
12480/13083 [===========================>..] - ETA: 0s
13083/13083 [==============================] - 1s 49us/step

   32/26561 [..............................] - ETA: 1s
 1088/26561 [>.............................] - ETA: 1s
 2048/26561 [=>............................] - ETA: 1s
 2624/26561 [=>............................] - ETA: 1s
 2944/26561 [==>...........................] - ETA: 1s
 3264/26561 [==>...........................] - ETA: 1s
 3584/26561 [===>..........................] - ETA: 2s
 3936/26561 [===>..........................] - ETA: 2s
 4928/26561 [====>.........................] - ETA: 1s
 5504/26561 [=====>........................] - ETA: 1s
 5760/26561 [=====>........................] - ETA: 1s
 6016/26561 [=====>........................] - ETA: 1s
 6624/26561 [======>.......................] - ETA: 1s
 7232/26561 [=======>......................] - ETA: 1s
 8192/26561 [========>.....................] - ETA: 1s
 9152/26561 [=========>....................] - ETA: 1s
10144/26561 [==========>...................] - ETA: 1s
11136/26561 [===========>..................] - ETA: 1s
12128/26561 [============>.................] - ETA: 1s
12928/26561 [=============>................] - ETA: 1s
13920/26561 [==============>...............] - ETA: 0s
14912/26561 [===============>..............] - ETA: 0s
15808/26561 [================>.............] - ETA: 0s
16608/26561 [=================>............] - ETA: 0s
17504/26561 [==================>...........] - ETA: 0s
18528/26561 [===================>..........] - ETA: 0s
19264/26561 [====================>.........] - ETA: 0s
20256/26561 [=====================>........] - ETA: 0s
21312/26561 [=======================>......] - ETA: 0s
22368/26561 [========================>.....] - ETA: 0s
23424/26561 [=========================>....] - ETA: 0s
24480/26561 [==========================>...] - ETA: 0s
25536/26561 [===========================>..] - ETA: 0s
26561/26561 [==============================] - 2s 64us/step

acc: 66.00%
NeuralNet Accuracy: 66.00% (21.48%) with {'loss': 'mse', 'optimizer': 'adam', 'num_hidden': 3, 'activation': 'relu', 'batch_size': 512, 'val_score': 66.00168157104942, 'train_score': 67.85512593652348, 'max_epochs': 100, 'hidden_layer_width': 16}
MAX: 66.3991439313
=================================================
{   'activation': 'relu',
    'batch_size': 64,
    'hidden_layer_width': 32,
    'loss': 'mse',
    'max_epochs': 100,
    'num_hidden': 3,
    'optimizer': 'adam'}

   32/13083 [..............................] - ETA: 0s
 1088/13083 [=>............................] - ETA: 0s
 2144/13083 [===>..........................] - ETA: 0s
 3200/13083 [======>.......................] - ETA: 0s
 4256/13083 [========>.....................] - ETA: 0s
 4896/13083 [==========>...................] - ETA: 0s
 5184/13083 [==========>...................] - ETA: 0s
 5504/13083 [===========>..................] - ETA: 0s
 5824/13083 [============>.................] - ETA: 0s
 6176/13083 [=============>................] - ETA: 0s
 7104/13083 [===============>..............] - ETA: 0s
 7808/13083 [================>.............] - ETA: 0s
 8096/13083 [=================>............] - ETA: 0s
 8448/13083 [==================>...........] - ETA: 0s
 9056/13083 [===================>..........] - ETA: 0s
 9664/13083 [=====================>........] - ETA: 0s
10656/13083 [=======================>......] - ETA: 0s
11648/13083 [=========================>....] - ETA: 0s
12608/13083 [===========================>..] - ETA: 0s
13083/13083 [==============================] - 1s 74us/step

   32/26561 [..............................] - ETA: 1s
  992/26561 [>.............................] - ETA: 1s
 1792/26561 [=>............................] - ETA: 1s
 2752/26561 [==>...........................] - ETA: 1s
 3744/26561 [===>..........................] - ETA: 1s
 4704/26561 [====>.........................] - ETA: 1s
 5504/26561 [=====>........................] - ETA: 1s
 6368/26561 [======>.......................] - ETA: 1s
 7360/26561 [=======>......................] - ETA: 1s
 8224/26561 [========>.....................] - ETA: 1s
 9024/26561 [=========>....................] - ETA: 0s
10080/26561 [==========>...................] - ETA: 0s
11136/26561 [===========>..................] - ETA: 0s
12192/26561 [============>.................] - ETA: 0s
13248/26561 [=============>................] - ETA: 0s
14304/26561 [===============>..............] - ETA: 0s
15360/26561 [================>.............] - ETA: 0s
16416/26561 [=================>............] - ETA: 0s
17472/26561 [==================>...........] - ETA: 0s
18464/26561 [===================>..........] - ETA: 0s
19456/26561 [====================>.........] - ETA: 0s
20480/26561 [======================>.......] - ETA: 0s
21504/26561 [=======================>......] - ETA: 0s
22528/26561 [========================>.....] - ETA: 0s
23584/26561 [=========================>....] - ETA: 0s
24640/26561 [==========================>...] - ETA: 0s
25696/26561 [============================>.] - ETA: 0s
26561/26561 [==============================] - 1s 52us/step

acc: 66.13%
NeuralNet Accuracy: 66.13% (21.47%) with {'loss': 'mse', 'optimizer': 'adam', 'num_hidden': 3, 'activation': 'relu', 'batch_size': 64, 'val_score': 66.13162119190126, 'train_score': 68.80012047739167, 'max_epochs': 100, 'hidden_layer_width': 32}
MAX: 66.3991439313
=================================================
{   'activation': 'relu',
    'batch_size': 512,
    'hidden_layer_width': 32,
    'loss': 'mse',
    'max_epochs': 100,
    'num_hidden': 3,
    'optimizer': 'adam'}

   32/13083 [..............................] - ETA: 0s
 1088/13083 [=>............................] - ETA: 0s
 2144/13083 [===>..........................] - ETA: 0s
 3200/13083 [======>.......................] - ETA: 0s
 4256/13083 [========>.....................] - ETA: 0s
 5312/13083 [===========>..................] - ETA: 0s
 6336/13083 [=============>................] - ETA: 0s
 7392/13083 [===============>..............] - ETA: 0s
 8448/13083 [==================>...........] - ETA: 0s
 9504/13083 [====================>.........] - ETA: 0s
10432/13083 [======================>.......] - ETA: 0s
10720/13083 [=======================>......] - ETA: 0s
11008/13083 [========================>.....] - ETA: 0s
11328/13083 [========================>.....] - ETA: 0s
11616/13083 [=========================>....] - ETA: 0s
12288/13083 [===========================>..] - ETA: 0s
13083/13083 [==============================] - 1s 62us/step

   32/26561 [..............................] - ETA: 1s
  352/26561 [..............................] - ETA: 4s
  608/26561 [..............................] - ETA: 4s
  896/26561 [>.............................] - ETA: 4s
 1760/26561 [>.............................] - ETA: 2s
 2368/26561 [=>............................] - ETA: 2s
 3328/26561 [==>...........................] - ETA: 2s
 4288/26561 [===>..........................] - ETA: 1s
 5280/26561 [====>.........................] - ETA: 1s
 6272/26561 [======>.......................] - ETA: 1s
 7232/26561 [=======>......................] - ETA: 1s
 8160/26561 [========>.....................] - ETA: 1s
 9152/26561 [=========>....................] - ETA: 1s
10144/26561 [==========>...................] - ETA: 1s
10944/26561 [===========>..................] - ETA: 1s
11808/26561 [============>.................] - ETA: 0s
12800/26561 [=============>................] - ETA: 0s
13664/26561 [==============>...............] - ETA: 0s
14496/26561 [===============>..............] - ETA: 0s
15552/26561 [================>.............] - ETA: 0s
16608/26561 [=================>............] - ETA: 0s
17632/26561 [==================>...........] - ETA: 0s
18688/26561 [====================>.........] - ETA: 0s
19744/26561 [=====================>........] - ETA: 0s
20800/26561 [======================>.......] - ETA: 0s
21856/26561 [=======================>......] - ETA: 0s
22912/26561 [========================>.....] - ETA: 0s
23968/26561 [==========================>...] - ETA: 0s
25024/26561 [===========================>..] - ETA: 0s
26080/26561 [============================>.] - ETA: 0s
26561/26561 [==============================] - 2s 57us/step

acc: 65.72%
NeuralNet Accuracy: 65.72% (21.48%) with {'loss': 'mse', 'optimizer': 'adam', 'num_hidden': 3, 'activation': 'relu', 'batch_size': 512, 'val_score': 65.71887182249057, 'train_score': 68.99213132035692, 'max_epochs': 100, 'hidden_layer_width': 32}
MAX: 66.3991439313
=================================================
{   'activation': 'relu',
    'batch_size': 64,
    'hidden_layer_width': 64,
    'loss': 'mse',
    'max_epochs': 100,
    'num_hidden': 3,
    'optimizer': 'adam'}

   32/13083 [..............................] - ETA: 1s
 1056/13083 [=>............................] - ETA: 0s
 2112/13083 [===>..........................] - ETA: 0s
 3168/13083 [======>.......................] - ETA: 0s
 4224/13083 [========>.....................] - ETA: 0s
 5280/13083 [===========>..................] - ETA: 0s
 6336/13083 [=============>................] - ETA: 0s
 7392/13083 [===============>..............] - ETA: 0s
 8416/13083 [==================>...........] - ETA: 0s
 9472/13083 [====================>.........] - ETA: 0s
10496/13083 [=======================>......] - ETA: 0s
11552/13083 [=========================>....] - ETA: 0s
12576/13083 [===========================>..] - ETA: 0s
13083/13083 [==============================] - 1s 49us/step

   32/26561 [..............................] - ETA: 1s
 1088/26561 [>.............................] - ETA: 1s
 2144/26561 [=>............................] - ETA: 1s
 3168/26561 [==>...........................] - ETA: 1s
 4224/26561 [===>..........................] - ETA: 1s
 5280/26561 [====>.........................] - ETA: 1s
 5920/26561 [=====>........................] - ETA: 1s
 6208/26561 [======>.......................] - ETA: 1s
 6560/26561 [======>.......................] - ETA: 1s
 6880/26561 [======>.......................] - ETA: 1s
 7232/26561 [=======>......................] - ETA: 1s
 8096/26561 [========>.....................] - ETA: 1s
 8800/26561 [========>.....................] - ETA: 1s
 8992/26561 [=========>....................] - ETA: 1s
 9280/26561 [=========>....................] - ETA: 1s
10048/26561 [==========>...................] - ETA: 1s
10656/26561 [===========>..................] - ETA: 1s
11616/26561 [============>.................] - ETA: 1s
12576/26561 [=============>................] - ETA: 1s
13536/26561 [==============>...............] - ETA: 0s
14496/26561 [===============>..............] - ETA: 0s
15392/26561 [================>.............] - ETA: 0s
16320/26561 [=================>............] - ETA: 0s
17280/26561 [==================>...........] - ETA: 0s
18272/26561 [===================>..........] - ETA: 0s
19104/26561 [====================>.........] - ETA: 0s
19968/26561 [=====================>........] - ETA: 0s
20928/26561 [======================>.......] - ETA: 0s
21792/26561 [=======================>......] - ETA: 0s
22592/26561 [========================>.....] - ETA: 0s
23616/26561 [=========================>....] - ETA: 0s
24672/26561 [==========================>...] - ETA: 0s
25728/26561 [============================>.] - ETA: 0s
26561/26561 [==============================] - 2s 64us/step

acc: 65.95%
NeuralNet Accuracy: 65.95% (21.78%) with {'loss': 'mse', 'optimizer': 'adam', 'num_hidden': 3, 'activation': 'relu', 'batch_size': 64, 'val_score': 65.94817702452961, 'train_score': 69.85053273596627, 'max_epochs': 100, 'hidden_layer_width': 64}
MAX: 66.3991439313
=================================================
{   'activation': 'relu',
    'batch_size': 512,
    'hidden_layer_width': 64,
    'loss': 'mse',
    'max_epochs': 100,
    'num_hidden': 3,
    'optimizer': 'adam'}

   32/13083 [..............................] - ETA: 0s
 1056/13083 [=>............................] - ETA: 0s
 2080/13083 [===>..........................] - ETA: 0s
 3104/13083 [======>.......................] - ETA: 0s
 4128/13083 [========>.....................] - ETA: 0s
 5024/13083 [==========>...................] - ETA: 0s
 5408/13083 [===========>..................] - ETA: 0s
 5728/13083 [============>.................] - ETA: 0s
 6048/13083 [============>.................] - ETA: 0s
 6368/13083 [=============>................] - ETA: 0s
 6944/13083 [==============>...............] - ETA: 0s
 7904/13083 [=================>............] - ETA: 0s
 8256/13083 [=================>............] - ETA: 0s
 8544/13083 [==================>...........] - ETA: 0s
 8832/13083 [===================>..........] - ETA: 0s
 9600/13083 [=====================>........] - ETA: 0s
10176/13083 [======================>.......] - ETA: 0s
11136/13083 [========================>.....] - ETA: 0s
12096/13083 [==========================>...] - ETA: 0s
13056/13083 [============================>.] - ETA: 0s
13083/13083 [==============================] - 1s 75us/step

   32/26561 [..............................] - ETA: 1s
  992/26561 [>.............................] - ETA: 1s
 1856/26561 [=>............................] - ETA: 1s
 2784/26561 [==>...........................] - ETA: 1s
 3744/26561 [===>..........................] - ETA: 1s
 4736/26561 [====>.........................] - ETA: 1s
 5536/26561 [=====>........................] - ETA: 1s
 6368/26561 [======>.......................] - ETA: 1s
 7328/26561 [=======>......................] - ETA: 1s
 8192/26561 [========>.....................] - ETA: 1s
 9024/26561 [=========>....................] - ETA: 1s
10048/26561 [==========>...................] - ETA: 0s
11072/26561 [===========>..................] - ETA: 0s
12096/26561 [============>.................] - ETA: 0s
13120/26561 [=============>................] - ETA: 0s
14144/26561 [==============>...............] - ETA: 0s
15200/26561 [================>.............] - ETA: 0s
16256/26561 [=================>............] - ETA: 0s
17312/26561 [==================>...........] - ETA: 0s
18368/26561 [===================>..........] - ETA: 0s
19392/26561 [====================>.........] - ETA: 0s
20448/26561 [======================>.......] - ETA: 0s
21504/26561 [=======================>......] - ETA: 0s
22560/26561 [========================>.....] - ETA: 0s
23616/26561 [=========================>....] - ETA: 0s
24672/26561 [==========================>...] - ETA: 0s
25696/26561 [============================>.] - ETA: 0s
26561/26561 [==============================] - 1s 52us/step

acc: 65.57%
NeuralNet Accuracy: 65.57% (21.48%) with {'loss': 'mse', 'optimizer': 'adam', 'num_hidden': 3, 'activation': 'relu', 'batch_size': 512, 'val_score': 65.56600168248268, 'train_score': 69.17661232634313, 'max_epochs': 100, 'hidden_layer_width': 64}
MAX: 66.3991439313
=================================================
{   'activation': 'tanh',
    'batch_size': 64,
    'hidden_layer_width': 16,
    'loss': 'mse',
    'max_epochs': 100,
    'num_hidden': 1,
    'optimizer': 'adam'}

   32/13083 [..............................] - ETA: 0s
 1120/13083 [=>............................] - ETA: 0s
 2080/13083 [===>..........................] - ETA: 0s
 2624/13083 [=====>........................] - ETA: 0s
 2944/13083 [=====>........................] - ETA: 0s
 3264/13083 [======>.......................] - ETA: 0s
 3584/13083 [=======>......................] - ETA: 0s
 3936/13083 [========>.....................] - ETA: 0s
 4928/13083 [==========>...................] - ETA: 0s
 5568/13083 [===========>..................] - ETA: 0s
 5856/13083 [============>.................] - ETA: 0s
 6112/13083 [=============>................] - ETA: 0s
 6720/13083 [==============>...............] - ETA: 0s
 7328/13083 [===============>..............] - ETA: 0s
 8320/13083 [==================>...........] - ETA: 0s
 9312/13083 [====================>.........] - ETA: 0s
10304/13083 [======================>.......] - ETA: 0s
11296/13083 [========================>.....] - ETA: 0s
12288/13083 [===========================>..] - ETA: 0s
13083/13083 [==============================] - 1s 75us/step

   32/26561 [..............................] - ETA: 1s
 1056/26561 [>.............................] - ETA: 1s
 2080/26561 [=>............................] - ETA: 1s
 3008/26561 [==>...........................] - ETA: 1s
 3872/26561 [===>..........................] - ETA: 1s
 4768/26561 [====>.........................] - ETA: 1s
 5792/26561 [=====>........................] - ETA: 1s
 6528/26561 [======>.......................] - ETA: 1s
 7584/26561 [=======>......................] - ETA: 1s
 8672/26561 [========>.....................] - ETA: 0s
 9728/26561 [=========>....................] - ETA: 0s
10784/26561 [===========>..................] - ETA: 0s
11872/26561 [============>.................] - ETA: 0s
12960/26561 [=============>................] - ETA: 0s
14048/26561 [==============>...............] - ETA: 0s
15136/26561 [================>.............] - ETA: 0s
16224/26561 [=================>............] - ETA: 0s
17312/26561 [==================>...........] - ETA: 0s
18400/26561 [===================>..........] - ETA: 0s
19488/26561 [=====================>........] - ETA: 0s
20576/26561 [======================>.......] - ETA: 0s
21664/26561 [=======================>......] - ETA: 0s
22752/26561 [========================>.....] - ETA: 0s
23840/26561 [=========================>....] - ETA: 0s
24928/26561 [===========================>..] - ETA: 0s
25920/26561 [============================>.] - ETA: 0s
26560/26561 [============================>.] - ETA: 0s
26561/26561 [==============================] - 1s 50us/step

acc: 65.41%
NeuralNet Accuracy: 65.41% (21.60%) with {'loss': 'mse', 'optimizer': 'adam', 'num_hidden': 1, 'activation': 'tanh', 'batch_size': 64, 'val_score': 65.40548803745621, 'train_score': 67.4259252287188, 'max_epochs': 100, 'hidden_layer_width': 16}
MAX: 66.3991439313
=================================================
{   'activation': 'tanh',
    'batch_size': 512,
    'hidden_layer_width': 16,
    'loss': 'mse',
    'max_epochs': 100,
    'num_hidden': 1,
    'optimizer': 'adam'}

   32/13083 [..............................] - ETA: 0s
 1088/13083 [=>............................] - ETA: 0s
 2176/13083 [===>..........................] - ETA: 0s
 3136/13083 [======>.......................] - ETA: 0s
 3520/13083 [=======>......................] - ETA: 0s
 3840/13083 [=======>......................] - ETA: 0s
 4192/13083 [========>.....................] - ETA: 0s
 4512/13083 [=========>....................] - ETA: 0s
 4992/13083 [==========>...................] - ETA: 0s
 6016/13083 [============>.................] - ETA: 0s
 6496/13083 [=============>................] - ETA: 0s
 6784/13083 [==============>...............] - ETA: 0s
 7104/13083 [===============>..............] - ETA: 0s
 7744/13083 [================>.............] - ETA: 0s
 8352/13083 [==================>...........] - ETA: 0s
 9344/13083 [====================>.........] - ETA: 0s
10336/13083 [======================>.......] - ETA: 0s
11328/13083 [========================>.....] - ETA: 0s
12320/13083 [===========================>..] - ETA: 0s
13083/13083 [==============================] - 1s 74us/step

   32/26561 [..............................] - ETA: 1s
  832/26561 [..............................] - ETA: 1s
 1792/26561 [=>............................] - ETA: 1s
 2816/26561 [==>...........................] - ETA: 1s
 3744/26561 [===>..........................] - ETA: 1s
 4576/26561 [====>.........................] - ETA: 1s
 5472/26561 [=====>........................] - ETA: 1s
 6496/26561 [======>.......................] - ETA: 1s
 7360/26561 [=======>......................] - ETA: 1s
 8352/26561 [========>.....................] - ETA: 1s
 9440/26561 [=========>....................] - ETA: 0s
10528/26561 [==========>...................] - ETA: 0s
11616/26561 [============>.................] - ETA: 0s
12672/26561 [=============>................] - ETA: 0s
13728/26561 [==============>...............] - ETA: 0s
14816/26561 [===============>..............] - ETA: 0s
15904/26561 [================>.............] - ETA: 0s
16992/26561 [==================>...........] - ETA: 0s
18080/26561 [===================>..........] - ETA: 0s
19168/26561 [====================>.........] - ETA: 0s
20192/26561 [=====================>........] - ETA: 0s
21248/26561 [======================>.......] - ETA: 0s
22336/26561 [========================>.....] - ETA: 0s
23424/26561 [=========================>....] - ETA: 0s
24512/26561 [==========================>...] - ETA: 0s
25568/26561 [===========================>..] - ETA: 0s
26561/26561 [==============================] - 1s 50us/step

acc: 65.30%
NeuralNet Accuracy: 65.30% (21.64%) with {'loss': 'mse', 'optimizer': 'adam', 'num_hidden': 1, 'activation': 'tanh', 'batch_size': 512, 'val_score': 65.29847894168307, 'train_score': 66.5524641391514, 'max_epochs': 100, 'hidden_layer_width': 16}
MAX: 66.3991439313
=================================================
{   'activation': 'tanh',
    'batch_size': 64,
    'hidden_layer_width': 32,
    'loss': 'mse',
    'max_epochs': 100,
    'num_hidden': 1,
    'optimizer': 'adam'}

   32/13083 [..............................] - ETA: 1s
 1120/13083 [=>............................] - ETA: 0s
 2208/13083 [====>.........................] - ETA: 0s
 3296/13083 [======>.......................] - ETA: 0s
 4384/13083 [=========>....................] - ETA: 0s
 5472/13083 [===========>..................] - ETA: 0s
 6560/13083 [==============>...............] - ETA: 0s
 7648/13083 [================>.............] - ETA: 0s
 8736/13083 [===================>..........] - ETA: 0s
 9792/13083 [=====================>........] - ETA: 0s
10880/13083 [=======================>......] - ETA: 0s
11968/13083 [==========================>...] - ETA: 0s
13056/13083 [============================>.] - ETA: 0s
13083/13083 [==============================] - 1s 47us/step

   32/26561 [..............................] - ETA: 1s
  992/26561 [>.............................] - ETA: 1s
 1472/26561 [>.............................] - ETA: 1s
 1824/26561 [=>............................] - ETA: 2s
 2144/26561 [=>............................] - ETA: 2s
 2432/26561 [=>............................] - ETA: 2s
 2848/26561 [==>...........................] - ETA: 2s
 3808/26561 [===>..........................] - ETA: 2s
 4320/26561 [===>..........................] - ETA: 2s
 4576/26561 [====>.........................] - ETA: 2s
 4960/26561 [====>.........................] - ETA: 2s
 5728/26561 [=====>........................] - ETA: 2s
 6336/26561 [======>.......................] - ETA: 2s
 7296/26561 [=======>......................] - ETA: 1s
 8256/26561 [========>.....................] - ETA: 1s
 9216/26561 [=========>....................] - ETA: 1s
10176/26561 [==========>...................] - ETA: 1s
11168/26561 [===========>..................] - ETA: 1s
11968/26561 [============>.................] - ETA: 1s
12960/26561 [=============>................] - ETA: 1s
13952/26561 [==============>...............] - ETA: 0s
14880/26561 [===============>..............] - ETA: 0s
15744/26561 [================>.............] - ETA: 0s
16640/26561 [=================>............] - ETA: 0s
17568/26561 [==================>...........] - ETA: 0s
18464/26561 [===================>..........] - ETA: 0s
19552/26561 [=====================>........] - ETA: 0s
20640/26561 [======================>.......] - ETA: 0s
21728/26561 [=======================>......] - ETA: 0s
22816/26561 [========================>.....] - ETA: 0s
23904/26561 [=========================>....] - ETA: 0s
24992/26561 [===========================>..] - ETA: 0s
26080/26561 [============================>.] - ETA: 0s
26561/26561 [==============================] - 2s 63us/step

acc: 65.73%
NeuralNet Accuracy: 65.73% (21.56%) with {'loss': 'mse', 'optimizer': 'adam', 'num_hidden': 1, 'activation': 'tanh', 'batch_size': 64, 'val_score': 65.72651532477562, 'train_score': 67.77229772975414, 'max_epochs': 100, 'hidden_layer_width': 32}
MAX: 66.3991439313
=================================================
{   'activation': 'tanh',
    'batch_size': 512,
    'hidden_layer_width': 32,
    'loss': 'mse',
    'max_epochs': 100,
    'num_hidden': 1,
    'optimizer': 'adam'}

   32/13083 [..............................] - ETA: 1s
 1024/13083 [=>............................] - ETA: 0s
 2016/13083 [===>..........................] - ETA: 0s
 3008/13083 [=====>........................] - ETA: 0s
 3968/13083 [========>.....................] - ETA: 0s
 4896/13083 [==========>...................] - ETA: 0s
 5920/13083 [============>.................] - ETA: 0s
 6944/13083 [==============>...............] - ETA: 0s
 7808/13083 [================>.............] - ETA: 0s
 8704/13083 [==================>...........] - ETA: 0s
 9696/13083 [=====================>........] - ETA: 0s
10592/13083 [=======================>......] - ETA: 0s
11424/13083 [=========================>....] - ETA: 0s
12512/13083 [===========================>..] - ETA: 0s
13083/13083 [==============================] - 1s 54us/step

   32/26561 [..............................] - ETA: 1s
 1120/26561 [>.............................] - ETA: 1s
 2208/26561 [=>............................] - ETA: 1s
 3296/26561 [==>...........................] - ETA: 1s
 4384/26561 [===>..........................] - ETA: 1s
 5472/26561 [=====>........................] - ETA: 0s
 6560/26561 [======>.......................] - ETA: 0s
 7648/26561 [=======>......................] - ETA: 0s
 8736/26561 [========>.....................] - ETA: 0s
 9792/26561 [==========>...................] - ETA: 0s
10880/26561 [===========>..................] - ETA: 0s
11968/26561 [============>.................] - ETA: 0s
13056/26561 [=============>................] - ETA: 0s
14144/26561 [==============>...............] - ETA: 0s
15232/26561 [================>.............] - ETA: 0s
16320/26561 [=================>............] - ETA: 0s
17280/26561 [==================>...........] - ETA: 0s
17696/26561 [==================>...........] - ETA: 0s
18016/26561 [===================>..........] - ETA: 0s
18368/26561 [===================>..........] - ETA: 0s
18688/26561 [====================>.........] - ETA: 0s
19104/26561 [====================>.........] - ETA: 0s
20064/26561 [=====================>........] - ETA: 0s
20640/26561 [======================>.......] - ETA: 0s
20928/26561 [======================>.......] - ETA: 0s
21312/26561 [=======================>......] - ETA: 0s
21984/26561 [=======================>......] - ETA: 0s
22624/26561 [========================>.....] - ETA: 0s
23616/26561 [=========================>....] - ETA: 0s
24608/26561 [==========================>...] - ETA: 0s
25600/26561 [===========================>..] - ETA: 0s
26561/26561 [==============================] - 2s 60us/step

acc: 65.53%
NeuralNet Accuracy: 65.53% (21.49%) with {'loss': 'mse', 'optimizer': 'adam', 'num_hidden': 1, 'activation': 'tanh', 'batch_size': 512, 'val_score': 65.52778414691123, 'train_score': 67.7082941154324, 'max_epochs': 100, 'hidden_layer_width': 32}
MAX: 66.3991439313
=================================================
{   'activation': 'tanh',
    'batch_size': 64,
    'hidden_layer_width': 64,
    'loss': 'mse',
    'max_epochs': 100,
    'num_hidden': 1,
    'optimizer': 'adam'}

   32/13083 [..............................] - ETA: 1s
 1056/13083 [=>............................] - ETA: 0s
 2112/13083 [===>..........................] - ETA: 0s
 3168/13083 [======>.......................] - ETA: 0s
 4160/13083 [========>.....................] - ETA: 0s
 5248/13083 [===========>..................] - ETA: 0s
 6336/13083 [=============>................] - ETA: 0s
 7424/13083 [================>.............] - ETA: 0s
 8512/13083 [==================>...........] - ETA: 0s
 9600/13083 [=====================>........] - ETA: 0s
10688/13083 [=======================>......] - ETA: 0s
11776/13083 [==========================>...] - ETA: 0s
12864/13083 [============================>.] - ETA: 0s
13083/13083 [==============================] - 1s 48us/step

   32/26561 [..............................] - ETA: 1s
  800/26561 [..............................] - ETA: 1s
 1088/26561 [>.............................] - ETA: 2s
 1408/26561 [>.............................] - ETA: 2s
 1728/26561 [>.............................] - ETA: 3s
 2080/26561 [=>............................] - ETA: 3s
 2752/26561 [==>...........................] - ETA: 2s
 3744/26561 [===>..........................] - ETA: 2s
 4096/26561 [===>..........................] - ETA: 2s
 4416/26561 [===>..........................] - ETA: 2s
 4736/26561 [====>.........................] - ETA: 2s
 5536/26561 [=====>........................] - ETA: 2s
 6272/26561 [======>.......................] - ETA: 2s
 7264/26561 [=======>......................] - ETA: 1s
 8256/26561 [========>.....................] - ETA: 1s
 9248/26561 [=========>....................] - ETA: 1s
10240/26561 [==========>...................] - ETA: 1s
11104/26561 [===========>..................] - ETA: 1s
12064/26561 [============>.................] - ETA: 1s
13056/26561 [=============>................] - ETA: 1s
14080/26561 [==============>...............] - ETA: 0s
14880/26561 [===============>..............] - ETA: 0s
15808/26561 [================>.............] - ETA: 0s
16832/26561 [==================>...........] - ETA: 0s
17760/26561 [===================>..........] - ETA: 0s
18624/26561 [====================>.........] - ETA: 0s
19712/26561 [=====================>........] - ETA: 0s
20800/26561 [======================>.......] - ETA: 0s
21888/26561 [=======================>......] - ETA: 0s
22976/26561 [========================>.....] - ETA: 0s
24032/26561 [==========================>...] - ETA: 0s
25088/26561 [===========================>..] - ETA: 0s
26144/26561 [============================>.] - ETA: 0s
26561/26561 [==============================] - 2s 63us/step

acc: 65.50%
NeuralNet Accuracy: 65.50% (21.48%) with {'loss': 'mse', 'optimizer': 'adam', 'num_hidden': 1, 'activation': 'tanh', 'batch_size': 64, 'val_score': 65.49721011635835, 'train_score': 68.16384925266368, 'max_epochs': 100, 'hidden_layer_width': 64}
MAX: 66.3991439313
=================================================
{   'activation': 'tanh',
    'batch_size': 512,
    'hidden_layer_width': 64,
    'loss': 'mse',
    'max_epochs': 100,
    'num_hidden': 1,
    'optimizer': 'adam'}

   32/13083 [..............................] - ETA: 1s
 1088/13083 [=>............................] - ETA: 0s
 2144/13083 [===>..........................] - ETA: 0s
 3200/13083 [======>.......................] - ETA: 0s
 4256/13083 [========>.....................] - ETA: 0s
 5312/13083 [===========>..................] - ETA: 0s
 6400/13083 [=============>................] - ETA: 0s
 7488/13083 [================>.............] - ETA: 0s
 8544/13083 [==================>...........] - ETA: 0s
 9600/13083 [=====================>........] - ETA: 0s
10688/13083 [=======================>......] - ETA: 0s
11776/13083 [==========================>...] - ETA: 0s
12832/13083 [============================>.] - ETA: 0s
13083/13083 [==============================] - 1s 48us/step

   32/26561 [..............................] - ETA: 1s
  992/26561 [>.............................] - ETA: 1s
 1632/26561 [>.............................] - ETA: 1s
 1952/26561 [=>............................] - ETA: 2s
 2304/26561 [=>............................] - ETA: 2s
 2624/26561 [=>............................] - ETA: 2s
 3008/26561 [==>...........................] - ETA: 2s
 4000/26561 [===>..........................] - ETA: 2s
 4576/26561 [====>.........................] - ETA: 2s
 4896/26561 [====>.........................] - ETA: 2s
 5184/26561 [====>.........................] - ETA: 2s
 5760/26561 [=====>........................] - ETA: 2s
 6368/26561 [======>.......................] - ETA: 2s
 7360/26561 [=======>......................] - ETA: 1s
 8352/26561 [========>.....................] - ETA: 1s
 9344/26561 [=========>....................] - ETA: 1s
10336/26561 [==========>...................] - ETA: 1s
11328/26561 [===========>..................] - ETA: 1s
12128/26561 [============>.................] - ETA: 1s
13120/26561 [=============>................] - ETA: 1s
14144/26561 [==============>...............] - ETA: 0s
15072/26561 [================>.............] - ETA: 0s
15904/26561 [================>.............] - ETA: 0s
16800/26561 [=================>............] - ETA: 0s
17728/26561 [===================>..........] - ETA: 0s
18496/26561 [===================>..........] - ETA: 0s
19520/26561 [=====================>........] - ETA: 0s
20576/26561 [======================>.......] - ETA: 0s
21632/26561 [=======================>......] - ETA: 0s
22592/26561 [========================>.....] - ETA: 0s
23616/26561 [=========================>....] - ETA: 0s
24640/26561 [==========================>...] - ETA: 0s
25664/26561 [===========================>..] - ETA: 0s
26561/26561 [==============================] - 2s 64us/step

acc: 65.80%
NeuralNet Accuracy: 65.80% (21.52%) with {'loss': 'mse', 'optimizer': 'adam', 'num_hidden': 1, 'activation': 'tanh', 'batch_size': 512, 'val_score': 65.79530688315496, 'train_score': 67.45980949512443, 'max_epochs': 100, 'hidden_layer_width': 64}
MAX: 66.3991439313
=================================================
{   'activation': 'tanh',
    'batch_size': 64,
    'hidden_layer_width': 16,
    'loss': 'mse',
    'max_epochs': 100,
    'num_hidden': 2,
    'optimizer': 'adam'}

   32/13083 [..............................] - ETA: 1s
  992/13083 [=>............................] - ETA: 0s
 1952/13083 [===>..........................] - ETA: 0s
 2912/13083 [=====>........................] - ETA: 0s
 3712/13083 [=======>......................] - ETA: 0s
 4704/13083 [=========>....................] - ETA: 0s
 5696/13083 [============>.................] - ETA: 0s
 6624/13083 [==============>...............] - ETA: 0s
 7456/13083 [================>.............] - ETA: 0s
 8288/13083 [==================>...........] - ETA: 0s
 9184/13083 [====================>.........] - ETA: 0s
 9952/13083 [=====================>........] - ETA: 0s
10976/13083 [========================>.....] - ETA: 0s
11968/13083 [==========================>...] - ETA: 0s
12960/13083 [============================>.] - ETA: 0s
13083/13083 [==============================] - 1s 55us/step

   32/26561 [..............................] - ETA: 1s
 1024/26561 [>.............................] - ETA: 1s
 2016/26561 [=>............................] - ETA: 1s
 3040/26561 [==>...........................] - ETA: 1s
 4064/26561 [===>..........................] - ETA: 1s
 5088/26561 [====>.........................] - ETA: 1s
 6112/26561 [=====>........................] - ETA: 1s
 7136/26561 [=======>......................] - ETA: 0s
 8192/26561 [========>.....................] - ETA: 0s
 9216/26561 [=========>....................] - ETA: 0s
10240/26561 [==========>...................] - ETA: 0s
11296/26561 [===========>..................] - ETA: 0s
12320/26561 [============>.................] - ETA: 0s
13376/26561 [==============>...............] - ETA: 0s
14400/26561 [===============>..............] - ETA: 0s
15328/26561 [================>.............] - ETA: 0s
15712/26561 [================>.............] - ETA: 0s
16032/26561 [=================>............] - ETA: 0s
16384/26561 [=================>............] - ETA: 0s
16704/26561 [=================>............] - ETA: 0s
17152/26561 [==================>...........] - ETA: 0s
18112/26561 [===================>..........] - ETA: 0s
18560/26561 [===================>..........] - ETA: 0s
18848/26561 [====================>.........] - ETA: 0s
19200/26561 [====================>.........] - ETA: 0s
19936/26561 [=====================>........] - ETA: 0s
20544/26561 [======================>.......] - ETA: 0s
21504/26561 [=======================>......] - ETA: 0s
22464/26561 [========================>.....] - ETA: 0s
23424/26561 [=========================>....] - ETA: 0s
24384/26561 [==========================>...] - ETA: 0s
25344/26561 [===========================>..] - ETA: 0s
26144/26561 [============================>.] - ETA: 0s
26561/26561 [==============================] - 2s 63us/step

acc: 65.11%
NeuralNet Accuracy: 65.11% (21.71%) with {'loss': 'mse', 'optimizer': 'adam', 'num_hidden': 2, 'activation': 'tanh', 'batch_size': 64, 'val_score': 65.1073912661037, 'train_score': 66.51481495425624, 'max_epochs': 100, 'hidden_layer_width': 16}
MAX: 66.3991439313
=================================================
{   'activation': 'tanh',
    'batch_size': 512,
    'hidden_layer_width': 16,
    'loss': 'mse',
    'max_epochs': 100,
    'num_hidden': 2,
    'optimizer': 'adam'}

   32/13083 [..............................] - ETA: 1s
  608/13083 [>.............................] - ETA: 1s
  928/13083 [=>............................] - ETA: 1s
 1248/13083 [=>............................] - ETA: 1s
 1568/13083 [==>...........................] - ETA: 1s
 1920/13083 [===>..........................] - ETA: 1s
 2816/13083 [=====>........................] - ETA: 1s
 3392/13083 [======>.......................] - ETA: 1s
 3680/13083 [=======>......................] - ETA: 1s
 3936/13083 [========>.....................] - ETA: 1s
 4512/13083 [=========>....................] - ETA: 0s
 5056/13083 [==========>...................] - ETA: 0s
 5952/13083 [============>.................] - ETA: 0s
 6848/13083 [==============>...............] - ETA: 0s
 7776/13083 [================>.............] - ETA: 0s
 8736/13083 [===================>..........] - ETA: 0s
 9728/13083 [=====================>........] - ETA: 0s
10528/13083 [=======================>......] - ETA: 0s
11520/13083 [=========================>....] - ETA: 0s
12512/13083 [===========================>..] - ETA: 0s
13083/13083 [==============================] - 1s 77us/step

   32/26561 [..............................] - ETA: 1s
  864/26561 [..............................] - ETA: 1s
 1728/26561 [>.............................] - ETA: 1s
 2720/26561 [==>...........................] - ETA: 1s
 3456/26561 [==>...........................] - ETA: 1s
 4448/26561 [====>.........................] - ETA: 1s
 5472/26561 [=====>........................] - ETA: 1s
 6528/26561 [======>.......................] - ETA: 1s
 7520/26561 [=======>......................] - ETA: 1s
 8512/26561 [========>.....................] - ETA: 0s
 9504/26561 [=========>....................] - ETA: 0s
10496/26561 [==========>...................] - ETA: 0s
11488/26561 [===========>..................] - ETA: 0s
12512/26561 [=============>................] - ETA: 0s
13536/26561 [==============>...............] - ETA: 0s
14560/26561 [===============>..............] - ETA: 0s
15584/26561 [================>.............] - ETA: 0s
16608/26561 [=================>............] - ETA: 0s
17632/26561 [==================>...........] - ETA: 0s
18656/26561 [====================>.........] - ETA: 0s
19680/26561 [=====================>........] - ETA: 0s
20704/26561 [======================>.......] - ETA: 0s
21728/26561 [=======================>......] - ETA: 0s
22432/26561 [========================>.....] - ETA: 0s
22752/26561 [========================>.....] - ETA: 0s
23104/26561 [=========================>....] - ETA: 0s
23392/26561 [=========================>....] - ETA: 0s
23712/26561 [=========================>....] - ETA: 0s
24512/26561 [==========================>...] - ETA: 0s
25248/26561 [===========================>..] - ETA: 0s
25568/26561 [===========================>..] - ETA: 0s
25888/26561 [============================>.] - ETA: 0s
26400/26561 [============================>.] - ETA: 0s
26561/26561 [==============================] - 2s 62us/step

acc: 65.54%
NeuralNet Accuracy: 65.54% (21.62%) with {'loss': 'mse', 'optimizer': 'adam', 'num_hidden': 2, 'activation': 'tanh', 'batch_size': 512, 'val_score': 65.54307116514899, 'train_score': 67.06072813523588, 'max_epochs': 100, 'hidden_layer_width': 16}
MAX: 66.3991439313
=================================================
{   'activation': 'tanh',
    'batch_size': 64,
    'hidden_layer_width': 32,
    'loss': 'mse',
    'max_epochs': 100,
    'num_hidden': 2,
    'optimizer': 'adam'}

   32/13083 [..............................] - ETA: 0s
 1024/13083 [=>............................] - ETA: 0s
 2048/13083 [===>..........................] - ETA: 0s
 3040/13083 [=====>........................] - ETA: 0s
 4064/13083 [========>.....................] - ETA: 0s
 5088/13083 [==========>...................] - ETA: 0s
 6112/13083 [=============>................] - ETA: 0s
 7136/13083 [===============>..............] - ETA: 0s
 8160/13083 [=================>............] - ETA: 0s
 9088/13083 [===================>..........] - ETA: 0s
 9504/13083 [====================>.........] - ETA: 0s
 9792/13083 [=====================>........] - ETA: 0s
10112/13083 [======================>.......] - ETA: 0s
10400/13083 [======================>.......] - ETA: 0s
10912/13083 [========================>.....] - ETA: 0s
11872/13083 [==========================>...] - ETA: 0s
12224/13083 [===========================>..] - ETA: 0s
12544/13083 [===========================>..] - ETA: 0s
12896/13083 [============================>.] - ETA: 0s
13083/13083 [==============================] - 1s 73us/step

   32/26561 [..............................] - ETA: 1s
  704/26561 [..............................] - ETA: 2s
 1600/26561 [>.............................] - ETA: 1s
 2496/26561 [=>............................] - ETA: 1s
 3424/26561 [==>...........................] - ETA: 1s
 4352/26561 [===>..........................] - ETA: 1s
 5280/26561 [====>.........................] - ETA: 1s
 6048/26561 [=====>........................] - ETA: 1s
 6976/26561 [======>.......................] - ETA: 1s
 7904/26561 [=======>......................] - ETA: 1s
 8768/26561 [========>.....................] - ETA: 1s
 9536/26561 [=========>....................] - ETA: 1s
10368/26561 [==========>...................] - ETA: 0s
11328/26561 [===========>..................] - ETA: 0s
12096/26561 [============>.................] - ETA: 0s
13056/26561 [=============>................] - ETA: 0s
14080/26561 [==============>...............] - ETA: 0s
15104/26561 [================>.............] - ETA: 0s
16128/26561 [=================>............] - ETA: 0s
17152/26561 [==================>...........] - ETA: 0s
18176/26561 [===================>..........] - ETA: 0s
19136/26561 [====================>.........] - ETA: 0s
20096/26561 [=====================>........] - ETA: 0s
21120/26561 [======================>.......] - ETA: 0s
22144/26561 [========================>.....] - ETA: 0s
23168/26561 [=========================>....] - ETA: 0s
24192/26561 [==========================>...] - ETA: 0s
25216/26561 [===========================>..] - ETA: 0s
26240/26561 [============================>.] - ETA: 0s
26561/26561 [==============================] - 1s 54us/step

acc: 65.49%
NeuralNet Accuracy: 65.49% (21.64%) with {'loss': 'mse', 'optimizer': 'adam', 'num_hidden': 2, 'activation': 'tanh', 'batch_size': 64, 'val_score': 65.48956661270653, 'train_score': 67.51251835397764, 'max_epochs': 100, 'hidden_layer_width': 32}
MAX: 66.3991439313
=================================================
{   'activation': 'tanh',
    'batch_size': 512,
    'hidden_layer_width': 32,
    'loss': 'mse',
    'max_epochs': 100,
    'num_hidden': 2,
    'optimizer': 'adam'}

   32/13083 [..............................] - ETA: 1s
  864/13083 [>.............................] - ETA: 0s
 1856/13083 [===>..........................] - ETA: 0s
 2560/13083 [====>.........................] - ETA: 0s
 3552/13083 [=======>......................] - ETA: 0s
 4576/13083 [=========>....................] - ETA: 0s
 5600/13083 [===========>..................] - ETA: 0s
 6624/13083 [==============>...............] - ETA: 0s
 7648/13083 [================>.............] - ETA: 0s
 8672/13083 [==================>...........] - ETA: 0s
 9696/13083 [=====================>........] - ETA: 0s
10720/13083 [=======================>......] - ETA: 0s
11744/13083 [=========================>....] - ETA: 0s
12768/13083 [============================>.] - ETA: 0s
13083/13083 [==============================] - 1s 52us/step

   32/26561 [..............................] - ETA: 1s
 1056/26561 [>.............................] - ETA: 1s
 2080/26561 [=>............................] - ETA: 1s
 3072/26561 [==>...........................] - ETA: 1s
 4064/26561 [===>..........................] - ETA: 1s
 5088/26561 [====>.........................] - ETA: 1s
 6080/26561 [=====>........................] - ETA: 1s
 7104/26561 [=======>......................] - ETA: 0s
 8032/26561 [========>.....................] - ETA: 0s
 8448/26561 [========>.....................] - ETA: 0s
 8736/26561 [========>.....................] - ETA: 1s
 9024/26561 [=========>....................] - ETA: 1s
 9344/26561 [=========>....................] - ETA: 1s
 9728/26561 [=========>....................] - ETA: 1s
10656/26561 [===========>..................] - ETA: 1s
11104/26561 [===========>..................] - ETA: 1s
11424/26561 [===========>..................] - ETA: 1s
11744/26561 [============>.................] - ETA: 1s
12384/26561 [============>.................] - ETA: 1s
12928/26561 [=============>................] - ETA: 1s
13824/26561 [==============>...............] - ETA: 0s
14752/26561 [===============>..............] - ETA: 0s
15648/26561 [================>.............] - ETA: 0s
16544/26561 [=================>............] - ETA: 0s
17472/26561 [==================>...........] - ETA: 0s
18272/26561 [===================>..........] - ETA: 0s
19264/26561 [====================>.........] - ETA: 0s
20192/26561 [=====================>........] - ETA: 0s
21056/26561 [======================>.......] - ETA: 0s
21824/26561 [=======================>......] - ETA: 0s
22656/26561 [========================>.....] - ETA: 0s
23616/26561 [=========================>....] - ETA: 0s
24320/26561 [==========================>...] - ETA: 0s
25312/26561 [===========================>..] - ETA: 0s
26336/26561 [============================>.] - ETA: 0s
26561/26561 [==============================] - 2s 66us/step

acc: 65.43%
NeuralNet Accuracy: 65.43% (21.63%) with {'loss': 'mse', 'optimizer': 'adam', 'num_hidden': 2, 'activation': 'tanh', 'batch_size': 512, 'val_score': 65.42841856116814, 'train_score': 67.55769737585182, 'max_epochs': 100, 'hidden_layer_width': 32}
MAX: 66.3991439313
=================================================
{   'activation': 'tanh',
    'batch_size': 64,
    'hidden_layer_width': 64,
    'loss': 'mse',
    'max_epochs': 100,
    'num_hidden': 2,
    'optimizer': 'adam'}

   32/13083 [..............................] - ETA: 1s
  704/13083 [>.............................] - ETA: 0s
 1568/13083 [==>...........................] - ETA: 0s
 2400/13083 [====>.........................] - ETA: 0s
 3168/13083 [======>.......................] - ETA: 0s
 3872/13083 [=======>......................] - ETA: 0s
 4608/13083 [=========>....................] - ETA: 0s
 5472/13083 [===========>..................] - ETA: 0s
 6112/13083 [=============>................] - ETA: 0s
 6976/13083 [==============>...............] - ETA: 0s
 7840/13083 [================>.............] - ETA: 0s
 8704/13083 [==================>...........] - ETA: 0s
 9568/13083 [====================>.........] - ETA: 0s
10432/13083 [======================>.......] - ETA: 0s
11328/13083 [========================>.....] - ETA: 0s
12224/13083 [===========================>..] - ETA: 0s
13083/13083 [==============================] - 1s 63us/step

   32/26561 [..............................] - ETA: 1s
 1056/26561 [>.............................] - ETA: 1s
 2080/26561 [=>............................] - ETA: 1s
 3104/26561 [==>...........................] - ETA: 1s
 4128/26561 [===>..........................] - ETA: 1s
 5120/26561 [====>.........................] - ETA: 1s
 6144/26561 [=====>........................] - ETA: 1s
 7168/26561 [=======>......................] - ETA: 0s
 8192/26561 [========>.....................] - ETA: 0s
 9216/26561 [=========>....................] - ETA: 0s
10208/26561 [==========>...................] - ETA: 0s
10816/26561 [===========>..................] - ETA: 0s
11136/26561 [===========>..................] - ETA: 0s
11456/26561 [===========>..................] - ETA: 0s
11776/26561 [============>.................] - ETA: 0s
12064/26561 [============>.................] - ETA: 0s
12864/26561 [=============>................] - ETA: 0s
13600/26561 [==============>...............] - ETA: 0s
13888/26561 [==============>...............] - ETA: 0s
14144/26561 [==============>...............] - ETA: 0s
14496/26561 [===============>..............] - ETA: 0s
15168/26561 [================>.............] - ETA: 0s
15968/26561 [=================>............] - ETA: 0s
16768/26561 [=================>............] - ETA: 0s
17664/26561 [==================>...........] - ETA: 0s
18592/26561 [===================>..........] - ETA: 0s
19552/26561 [=====================>........] - ETA: 0s
20320/26561 [=====================>........] - ETA: 0s
21280/26561 [=======================>......] - ETA: 0s
22240/26561 [========================>.....] - ETA: 0s
23104/26561 [=========================>....] - ETA: 0s
23904/26561 [=========================>....] - ETA: 0s
24736/26561 [==========================>...] - ETA: 0s
25600/26561 [===========================>..] - ETA: 0s
26272/26561 [============================>.] - ETA: 0s
26561/26561 [==============================] - 2s 67us/step

acc: 64.90%
NeuralNet Accuracy: 64.90% (21.76%) with {'loss': 'mse', 'optimizer': 'adam', 'num_hidden': 2, 'activation': 'tanh', 'batch_size': 64, 'val_score': 64.90101659051014, 'train_score': 66.6089379164941, 'max_epochs': 100, 'hidden_layer_width': 64}
MAX: 66.3991439313
=================================================
{   'activation': 'tanh',
    'batch_size': 512,
    'hidden_layer_width': 64,
    'loss': 'mse',
    'max_epochs': 100,
    'num_hidden': 2,
    'optimizer': 'adam'}

   32/13083 [..............................] - ETA: 1s
  896/13083 [=>............................] - ETA: 0s
 1824/13083 [===>..........................] - ETA: 0s
 2624/13083 [=====>........................] - ETA: 0s
 3584/13083 [=======>......................] - ETA: 0s
 4544/13083 [=========>....................] - ETA: 0s
 5440/13083 [===========>..................] - ETA: 0s
 6272/13083 [=============>................] - ETA: 0s
 7104/13083 [===============>..............] - ETA: 0s
 7968/13083 [=================>............] - ETA: 0s
 8768/13083 [===================>..........] - ETA: 0s
 9792/13083 [=====================>........] - ETA: 0s
10816/13083 [=======================>......] - ETA: 0s
11840/13083 [==========================>...] - ETA: 0s
12864/13083 [============================>.] - ETA: 0s
13083/13083 [==============================] - 1s 56us/step

   32/26561 [..............................] - ETA: 1s
 1056/26561 [>.............................] - ETA: 1s
 2048/26561 [=>............................] - ETA: 1s
 3072/26561 [==>...........................] - ETA: 1s
 4096/26561 [===>..........................] - ETA: 1s
 5120/26561 [====>.........................] - ETA: 1s
 6112/26561 [=====>........................] - ETA: 1s
 7136/26561 [=======>......................] - ETA: 0s
 8160/26561 [========>.....................] - ETA: 0s
 9184/26561 [=========>....................] - ETA: 0s
10208/26561 [==========>...................] - ETA: 0s
11232/26561 [===========>..................] - ETA: 0s
12256/26561 [============>.................] - ETA: 0s
13280/26561 [=============>................] - ETA: 0s
14016/26561 [==============>...............] - ETA: 0s
14304/26561 [===============>..............] - ETA: 0s
14624/26561 [===============>..............] - ETA: 0s
14912/26561 [===============>..............] - ETA: 0s
15232/26561 [================>.............] - ETA: 0s
15808/26561 [================>.............] - ETA: 0s
16736/26561 [=================>............] - ETA: 0s
17024/26561 [==================>...........] - ETA: 0s
17344/26561 [==================>...........] - ETA: 0s
17632/26561 [==================>...........] - ETA: 0s
18496/26561 [===================>..........] - ETA: 0s
19200/26561 [====================>.........] - ETA: 0s
20160/26561 [=====================>........] - ETA: 0s
21152/26561 [======================>.......] - ETA: 0s
22112/26561 [=======================>......] - ETA: 0s
23104/26561 [=========================>....] - ETA: 0s
23904/26561 [=========================>....] - ETA: 0s
24864/26561 [===========================>..] - ETA: 0s
25856/26561 [============================>.] - ETA: 0s
26561/26561 [==============================] - 2s 63us/step

acc: 65.40%
NeuralNet Accuracy: 65.40% (21.54%) with {'loss': 'mse', 'optimizer': 'adam', 'num_hidden': 2, 'activation': 'tanh', 'batch_size': 512, 'val_score': 65.39784453198203, 'train_score': 67.88524528443959, 'max_epochs': 100, 'hidden_layer_width': 64}
MAX: 66.3991439313
=================================================
{   'activation': 'tanh',
    'batch_size': 64,
    'hidden_layer_width': 16,
    'loss': 'mse',
    'max_epochs': 100,
    'num_hidden': 3,
    'optimizer': 'adam'}

   32/13083 [..............................] - ETA: 1s
  928/13083 [=>............................] - ETA: 0s
 1824/13083 [===>..........................] - ETA: 0s
 2656/13083 [=====>........................] - ETA: 0s
 3424/13083 [======>.......................] - ETA: 0s
 4224/13083 [========>.....................] - ETA: 0s
 5120/13083 [==========>...................] - ETA: 0s
 5888/13083 [============>.................] - ETA: 0s
 6880/13083 [==============>...............] - ETA: 0s
 7840/13083 [================>.............] - ETA: 0s
 8800/13083 [===================>..........] - ETA: 0s
 9792/13083 [=====================>........] - ETA: 0s
10784/13083 [=======================>......] - ETA: 0s
11776/13083 [==========================>...] - ETA: 0s
12736/13083 [============================>.] - ETA: 0s
13083/13083 [==============================] - 1s 56us/step

   32/26561 [..............................] - ETA: 1s
 1024/26561 [>.............................] - ETA: 1s
 2016/26561 [=>............................] - ETA: 1s
 3008/26561 [==>...........................] - ETA: 1s
 3968/26561 [===>..........................] - ETA: 1s
 4960/26561 [====>.........................] - ETA: 1s
 5920/26561 [=====>........................] - ETA: 1s
 6912/26561 [======>.......................] - ETA: 1s
 7904/26561 [=======>......................] - ETA: 0s
 8896/26561 [=========>....................] - ETA: 0s
 9888/26561 [==========>...................] - ETA: 0s
10688/26561 [===========>..................] - ETA: 0s
11040/26561 [===========>..................] - ETA: 0s
11328/26561 [===========>..................] - ETA: 0s
11648/26561 [============>.................] - ETA: 0s
11936/26561 [============>.................] - ETA: 0s
12704/26561 [=============>................] - ETA: 0s
13408/26561 [==============>...............] - ETA: 0s
13728/26561 [==============>...............] - ETA: 0s
14048/26561 [==============>...............] - ETA: 0s
14592/26561 [===============>..............] - ETA: 0s
15232/26561 [================>.............] - ETA: 0s
16128/26561 [=================>............] - ETA: 0s
17056/26561 [==================>...........] - ETA: 0s
17984/26561 [===================>..........] - ETA: 0s
18912/26561 [====================>.........] - ETA: 0s
19840/26561 [=====================>........] - ETA: 0s
20576/26561 [======================>.......] - ETA: 0s
21504/26561 [=======================>......] - ETA: 0s
22432/26561 [========================>.....] - ETA: 0s
23296/26561 [=========================>....] - ETA: 0s
24096/26561 [==========================>...] - ETA: 0s
24928/26561 [===========================>..] - ETA: 0s
25888/26561 [============================>.] - ETA: 0s
26560/26561 [============================>.] - ETA: 0s
26561/26561 [==============================] - 2s 66us/step

acc: 65.24%
NeuralNet Accuracy: 65.24% (21.67%) with {'loss': 'mse', 'optimizer': 'adam', 'num_hidden': 3, 'activation': 'tanh', 'batch_size': 64, 'val_score': 65.2449743937965, 'train_score': 66.6089379164941, 'max_epochs': 100, 'hidden_layer_width': 16}
MAX: 66.3991439313
=================================================
{   'activation': 'tanh',
    'batch_size': 512,
    'hidden_layer_width': 16,
    'loss': 'mse',
    'max_epochs': 100,
    'num_hidden': 3,
    'optimizer': 'adam'}

   32/13083 [..............................] - ETA: 1s
  832/13083 [>.............................] - ETA: 0s
 1696/13083 [==>...........................] - ETA: 0s
 2560/13083 [====>.........................] - ETA: 0s
 3424/13083 [======>.......................] - ETA: 0s
 4320/13083 [========>.....................] - ETA: 0s
 5056/13083 [==========>...................] - ETA: 0s
 5952/13083 [============>.................] - ETA: 0s
 6848/13083 [==============>...............] - ETA: 0s
 7680/13083 [================>.............] - ETA: 0s
 8384/13083 [==================>...........] - ETA: 0s
 9184/13083 [====================>.........] - ETA: 0s
10112/13083 [======================>.......] - ETA: 0s
10848/13083 [=======================>......] - ETA: 0s
11744/13083 [=========================>....] - ETA: 0s
12736/13083 [============================>.] - ETA: 0s
13083/13083 [==============================] - 1s 60us/step

   32/26561 [..............................] - ETA: 1s
 1024/26561 [>.............................] - ETA: 1s
 2016/26561 [=>............................] - ETA: 1s
 3008/26561 [==>...........................] - ETA: 1s
 4000/26561 [===>..........................] - ETA: 1s
 4992/26561 [====>.........................] - ETA: 1s
 5984/26561 [=====>........................] - ETA: 1s
 6976/26561 [======>.......................] - ETA: 1s
 7968/26561 [=======>......................] - ETA: 0s
 8960/26561 [=========>....................] - ETA: 0s
 9952/26561 [==========>...................] - ETA: 0s
10944/26561 [===========>..................] - ETA: 0s
11904/26561 [============>.................] - ETA: 0s
12864/26561 [=============>................] - ETA: 0s
13856/26561 [==============>...............] - ETA: 0s
14848/26561 [===============>..............] - ETA: 0s
15712/26561 [================>.............] - ETA: 0s
16096/26561 [=================>............] - ETA: 0s
16384/26561 [=================>............] - ETA: 0s
16672/26561 [=================>............] - ETA: 0s
16960/26561 [==================>...........] - ETA: 0s
17312/26561 [==================>...........] - ETA: 0s
18208/26561 [===================>..........] - ETA: 0s
18752/26561 [====================>.........] - ETA: 0s
19008/26561 [====================>.........] - ETA: 0s
19264/26561 [====================>.........] - ETA: 0s
19808/26561 [=====================>........] - ETA: 0s
20352/26561 [=====================>........] - ETA: 0s
21184/26561 [======================>.......] - ETA: 0s
22016/26561 [=======================>......] - ETA: 0s
22880/26561 [========================>.....] - ETA: 0s
23744/26561 [=========================>....] - ETA: 0s
24608/26561 [==========================>...] - ETA: 0s
25312/26561 [===========================>..] - ETA: 0s
26208/26561 [============================>.] - ETA: 0s
26561/26561 [==============================] - 2s 67us/step

acc: 65.03%
NeuralNet Accuracy: 65.03% (21.78%) with {'loss': 'mse', 'optimizer': 'adam', 'num_hidden': 3, 'activation': 'tanh', 'batch_size': 512, 'val_score': 65.03095620225021, 'train_score': 66.33409886675953, 'max_epochs': 100, 'hidden_layer_width': 16}
MAX: 66.3991439313
=================================================
{   'activation': 'tanh',
    'batch_size': 64,
    'hidden_layer_width': 32,
    'loss': 'mse',
    'max_epochs': 100,
    'num_hidden': 3,
    'optimizer': 'adam'}

   32/13083 [..............................] - ETA: 1s
  992/13083 [=>............................] - ETA: 0s
 1952/13083 [===>..........................] - ETA: 0s
 2912/13083 [=====>........................] - ETA: 0s
 3872/13083 [=======>......................] - ETA: 0s
 4832/13083 [==========>...................] - ETA: 0s
 5792/13083 [============>.................] - ETA: 0s
 6784/13083 [==============>...............] - ETA: 0s
 7744/13083 [================>.............] - ETA: 0s
 8704/13083 [==================>...........] - ETA: 0s
 9664/13083 [=====================>........] - ETA: 0s
10656/13083 [=======================>......] - ETA: 0s
11616/13083 [=========================>....] - ETA: 0s
12608/13083 [===========================>..] - ETA: 0s
13083/13083 [==============================] - 1s 53us/step

   32/26561 [..............................] - ETA: 1s
 1024/26561 [>.............................] - ETA: 1s
 1984/26561 [=>............................] - ETA: 1s
 2944/26561 [==>...........................] - ETA: 1s
 3904/26561 [===>..........................] - ETA: 1s
 4608/26561 [====>.........................] - ETA: 1s
 4928/26561 [====>.........................] - ETA: 1s
 5248/26561 [====>.........................] - ETA: 1s
 5536/26561 [=====>........................] - ETA: 1s
 5824/26561 [=====>........................] - ETA: 1s
 6592/26561 [======>.......................] - ETA: 1s
 7328/26561 [=======>......................] - ETA: 1s
 7584/26561 [=======>......................] - ETA: 1s
 7872/26561 [=======>......................] - ETA: 1s
 8160/26561 [========>.....................] - ETA: 1s
 8896/26561 [=========>....................] - ETA: 1s
 9600/26561 [=========>....................] - ETA: 1s
10464/26561 [==========>...................] - ETA: 1s
11328/26561 [===========>..................] - ETA: 1s
12192/26561 [============>.................] - ETA: 1s
13056/26561 [=============>................] - ETA: 1s
13760/26561 [==============>...............] - ETA: 1s
14624/26561 [===============>..............] - ETA: 0s
15488/26561 [================>.............] - ETA: 0s
16352/26561 [=================>............] - ETA: 0s
17056/26561 [==================>...........] - ETA: 0s
17856/26561 [===================>..........] - ETA: 0s
18752/26561 [====================>.........] - ETA: 0s
19552/26561 [=====================>........] - ETA: 0s
20288/26561 [=====================>........] - ETA: 0s
21248/26561 [======================>.......] - ETA: 0s
22176/26561 [========================>.....] - ETA: 0s
23168/26561 [=========================>....] - ETA: 0s
24160/26561 [==========================>...] - ETA: 0s
25152/26561 [===========================>..] - ETA: 0s
26112/26561 [============================>.] - ETA: 0s
26561/26561 [==============================] - 2s 69us/step

acc: 65.54%
NeuralNet Accuracy: 65.54% (21.56%) with {'loss': 'mse', 'optimizer': 'adam', 'num_hidden': 3, 'activation': 'tanh', 'batch_size': 64, 'val_score': 65.54307116514899, 'train_score': 67.85512593652348, 'max_epochs': 100, 'hidden_layer_width': 32}
MAX: 66.3991439313
=================================================
{   'activation': 'tanh',
    'batch_size': 512,
    'hidden_layer_width': 32,
    'loss': 'mse',
    'max_epochs': 100,
    'num_hidden': 3,
    'optimizer': 'adam'}

   32/13083 [..............................] - ETA: 0s
 1024/13083 [=>............................] - ETA: 0s
 2016/13083 [===>..........................] - ETA: 0s
 3008/13083 [=====>........................] - ETA: 0s
 4000/13083 [========>.....................] - ETA: 0s
 4960/13083 [==========>...................] - ETA: 0s
 5536/13083 [===========>..................] - ETA: 0s
 5856/13083 [============>.................] - ETA: 0s
 6176/13083 [=============>................] - ETA: 0s
 6496/13083 [=============>................] - ETA: 0s
 6848/13083 [==============>...............] - ETA: 0s
 7616/13083 [================>.............] - ETA: 0s
 8128/13083 [=================>............] - ETA: 0s
 8448/13083 [==================>...........] - ETA: 0s
 8800/13083 [===================>..........] - ETA: 0s
 9376/13083 [====================>.........] - ETA: 0s
 9888/13083 [=====================>........] - ETA: 0s
10752/13083 [=======================>......] - ETA: 0s
11648/13083 [=========================>....] - ETA: 0s
12544/13083 [===========================>..] - ETA: 0s
13083/13083 [==============================] - 1s 78us/step

   32/26561 [..............................] - ETA: 1s
  896/26561 [>.............................] - ETA: 1s
 1600/26561 [>.............................] - ETA: 1s
 2496/26561 [=>............................] - ETA: 1s
 3392/26561 [==>...........................] - ETA: 1s
 4224/26561 [===>..........................] - ETA: 1s
 4992/26561 [====>.........................] - ETA: 1s
 5792/26561 [=====>........................] - ETA: 1s
 6720/26561 [======>.......................] - ETA: 1s
 7424/26561 [=======>......................] - ETA: 1s
 8352/26561 [========>.....................] - ETA: 1s
 9312/26561 [=========>....................] - ETA: 1s
10272/26561 [==========>...................] - ETA: 0s
11232/26561 [===========>..................] - ETA: 0s
12192/26561 [============>.................] - ETA: 0s
13152/26561 [=============>................] - ETA: 0s
14112/26561 [==============>...............] - ETA: 0s
15104/26561 [================>.............] - ETA: 0s
16064/26561 [=================>............] - ETA: 0s
17056/26561 [==================>...........] - ETA: 0s
18048/26561 [===================>..........] - ETA: 0s
19008/26561 [====================>.........] - ETA: 0s
19968/26561 [=====================>........] - ETA: 0s
20960/26561 [======================>.......] - ETA: 0s
21920/26561 [=======================>......] - ETA: 0s
22912/26561 [========================>.....] - ETA: 0s
23872/26561 [=========================>....] - ETA: 0s
24832/26561 [===========================>..] - ETA: 0s
25472/26561 [===========================>..] - ETA: 0s
25760/26561 [============================>.] - ETA: 0s
26048/26561 [============================>.] - ETA: 0s
26336/26561 [============================>.] - ETA: 0s
26561/26561 [==============================] - 2s 62us/step

acc: 65.31%
NeuralNet Accuracy: 65.31% (21.64%) with {'loss': 'mse', 'optimizer': 'adam', 'num_hidden': 3, 'activation': 'tanh', 'batch_size': 512, 'val_score': 65.30612245307988, 'train_score': 67.87771544746056, 'max_epochs': 100, 'hidden_layer_width': 32}
MAX: 66.3991439313
=================================================
{   'activation': 'tanh',
    'batch_size': 64,
    'hidden_layer_width': 64,
    'loss': 'mse',
    'max_epochs': 100,
    'num_hidden': 3,
    'optimizer': 'adam'}

   32/13083 [..............................] - ETA: 1s
  480/13083 [>.............................] - ETA: 1s
  768/13083 [>.............................] - ETA: 1s
 1120/13083 [=>............................] - ETA: 1s
 1856/13083 [===>..........................] - ETA: 1s
 2432/13083 [====>.........................] - ETA: 1s
 3296/13083 [======>.......................] - ETA: 0s
 4160/13083 [========>.....................] - ETA: 0s
 5056/13083 [==========>...................] - ETA: 0s
 5920/13083 [============>.................] - ETA: 0s
 6784/13083 [==============>...............] - ETA: 0s
 7488/13083 [================>.............] - ETA: 0s
 8320/13083 [==================>...........] - ETA: 0s
 9152/13083 [===================>..........] - ETA: 0s
 9920/13083 [=====================>........] - ETA: 0s
10624/13083 [=======================>......] - ETA: 0s
11424/13083 [=========================>....] - ETA: 0s
12320/13083 [===========================>..] - ETA: 0s
12992/13083 [============================>.] - ETA: 0s
13083/13083 [==============================] - 1s 71us/step

   32/26561 [..............................] - ETA: 1s
  992/26561 [>.............................] - ETA: 1s
 1952/26561 [=>............................] - ETA: 1s
 2912/26561 [==>...........................] - ETA: 1s
 3840/26561 [===>..........................] - ETA: 1s
 4736/26561 [====>.........................] - ETA: 1s
 5600/26561 [=====>........................] - ETA: 1s
 6464/26561 [======>.......................] - ETA: 1s
 7328/26561 [=======>......................] - ETA: 1s
 8288/26561 [========>.....................] - ETA: 1s
 9248/26561 [=========>....................] - ETA: 0s
10176/26561 [==========>...................] - ETA: 0s
11104/26561 [===========>..................] - ETA: 0s
12032/26561 [============>.................] - ETA: 0s
12960/26561 [=============>................] - ETA: 0s
13888/26561 [==============>...............] - ETA: 0s
14816/26561 [===============>..............] - ETA: 0s
15744/26561 [================>.............] - ETA: 0s
16608/26561 [=================>............] - ETA: 0s
17056/26561 [==================>...........] - ETA: 0s
17344/26561 [==================>...........] - ETA: 0s
17632/26561 [==================>...........] - ETA: 0s
17888/26561 [===================>..........] - ETA: 0s
18240/26561 [===================>..........] - ETA: 0s
19136/26561 [====================>.........] - ETA: 0s
19648/26561 [=====================>........] - ETA: 0s
19840/26561 [=====================>........] - ETA: 0s
20160/26561 [=====================>........] - ETA: 0s
20768/26561 [======================>.......] - ETA: 0s
21344/26561 [=======================>......] - ETA: 0s
22208/26561 [========================>.....] - ETA: 0s
23040/26561 [=========================>....] - ETA: 0s
23904/26561 [=========================>....] - ETA: 0s
24800/26561 [===========================>..] - ETA: 0s
25664/26561 [===========================>..] - ETA: 0s
26368/26561 [============================>.] - ETA: 0s
26561/26561 [==============================] - 2s 68us/step

acc: 65.51%
NeuralNet Accuracy: 65.51% (21.61%) with {'loss': 'mse', 'optimizer': 'adam', 'num_hidden': 3, 'activation': 'tanh', 'batch_size': 64, 'val_score': 65.51249713004023, 'train_score': 67.40333571778172, 'max_epochs': 100, 'hidden_layer_width': 64}
MAX: 66.3991439313
=================================================
{   'activation': 'tanh',
    'batch_size': 512,
    'hidden_layer_width': 64,
    'loss': 'mse',
    'max_epochs': 100,
    'num_hidden': 3,
    'optimizer': 'adam'}

   32/13083 [..............................] - ETA: 0s
  992/13083 [=>............................] - ETA: 0s
 1920/13083 [===>..........................] - ETA: 0s
 2848/13083 [=====>........................] - ETA: 0s
 3808/13083 [=======>......................] - ETA: 0s
 4768/13083 [=========>....................] - ETA: 0s
 5408/13083 [===========>..................] - ETA: 0s
 5696/13083 [============>.................] - ETA: 0s
 5984/13083 [============>.................] - ETA: 0s
 6272/13083 [=============>................] - ETA: 0s
 6560/13083 [==============>...............] - ETA: 0s
 7200/13083 [===============>..............] - ETA: 0s
 8032/13083 [=================>............] - ETA: 0s
 8352/13083 [==================>...........] - ETA: 0s
 8672/13083 [==================>...........] - ETA: 0s
 8960/13083 [===================>..........] - ETA: 0s
 9664/13083 [=====================>........] - ETA: 0s
10400/13083 [======================>.......] - ETA: 0s
11264/13083 [========================>.....] - ETA: 0s
12096/13083 [==========================>...] - ETA: 0s
12960/13083 [============================>.] - ETA: 0s
13083/13083 [==============================] - 1s 80us/step

   32/26561 [..............................] - ETA: 1s
  896/26561 [>.............................] - ETA: 1s
 1600/26561 [>.............................] - ETA: 1s
 2464/26561 [=>............................] - ETA: 1s
 3296/26561 [==>...........................] - ETA: 1s
 4064/26561 [===>..........................] - ETA: 1s
 4800/26561 [====>.........................] - ETA: 1s
 5568/26561 [=====>........................] - ETA: 1s
 6432/26561 [======>.......................] - ETA: 1s
 7136/26561 [=======>......................] - ETA: 1s
 8000/26561 [========>.....................] - ETA: 1s
 8960/26561 [=========>....................] - ETA: 1s
 9920/26561 [==========>...................] - ETA: 1s
10880/26561 [===========>..................] - ETA: 0s
11840/26561 [============>.................] - ETA: 0s
12800/26561 [=============>................] - ETA: 0s
13760/26561 [==============>...............] - ETA: 0s
14720/26561 [===============>..............] - ETA: 0s
15680/26561 [================>.............] - ETA: 0s
16640/26561 [=================>............] - ETA: 0s
17600/26561 [==================>...........] - ETA: 0s
18528/26561 [===================>..........] - ETA: 0s
19456/26561 [====================>.........] - ETA: 0s
20384/26561 [======================>.......] - ETA: 0s
21312/26561 [=======================>......] - ETA: 0s
22240/26561 [========================>.....] - ETA: 0s
23168/26561 [=========================>....] - ETA: 0s
24128/26561 [==========================>...] - ETA: 0s
24736/26561 [==========================>...] - ETA: 0s
25024/26561 [===========================>..] - ETA: 0s
25312/26561 [===========================>..] - ETA: 0s
25600/26561 [===========================>..] - ETA: 0s
25888/26561 [============================>.] - ETA: 0s
26528/26561 [============================>.] - ETA: 0s
26561/26561 [==============================] - 2s 64us/step

acc: 65.67%
NeuralNet Accuracy: 65.67% (21.49%) with {'loss': 'mse', 'optimizer': 'adam', 'num_hidden': 3, 'activation': 'tanh', 'batch_size': 512, 'val_score': 65.66536727004811, 'train_score': 67.97560332818794, 'max_epochs': 100, 'hidden_layer_width': 64}
MAX: 66.3991439313
=================================================
{   'activation': 'relu',
    'batch_size': 64,
    'hidden_layer_width': 16,
    'loss': 'binary_crossentropy',
    'max_epochs': 100,
    'num_hidden': 1,
    'optimizer': 'rmsprop'}

   32/13083 [..............................] - ETA: 1s
  864/13083 [>.............................] - ETA: 0s
 1696/13083 [==>...........................] - ETA: 0s
 2400/13083 [====>.........................] - ETA: 0s
 3232/13083 [======>.......................] - ETA: 0s
 4064/13083 [========>.....................] - ETA: 0s
 4896/13083 [==========>...................] - ETA: 0s
 5632/13083 [===========>..................] - ETA: 0s
 6400/13083 [=============>................] - ETA: 0s
 7200/13083 [===============>..............] - ETA: 0s
 7936/13083 [=================>............] - ETA: 0s
 8864/13083 [===================>..........] - ETA: 0s
 9792/13083 [=====================>........] - ETA: 0s
10720/13083 [=======================>......] - ETA: 0s
11648/13083 [=========================>....] - ETA: 0s
12576/13083 [===========================>..] - ETA: 0s
13083/13083 [==============================] - 1s 61us/step

   32/26561 [..............................] - ETA: 1s
  960/26561 [>.............................] - ETA: 1s
 1888/26561 [=>............................] - ETA: 1s
 2816/26561 [==>...........................] - ETA: 1s
 3744/26561 [===>..........................] - ETA: 1s
 4672/26561 [====>.........................] - ETA: 1s
 5600/26561 [=====>........................] - ETA: 1s
 6528/26561 [======>.......................] - ETA: 1s
 7456/26561 [=======>......................] - ETA: 1s
 8384/26561 [========>.....................] - ETA: 0s
 9312/26561 [=========>....................] - ETA: 0s
10240/26561 [==========>...................] - ETA: 0s
11136/26561 [===========>..................] - ETA: 0s
11712/26561 [============>.................] - ETA: 0s
12000/26561 [============>.................] - ETA: 0s
12288/26561 [============>.................] - ETA: 0s
12576/26561 [=============>................] - ETA: 0s
12896/26561 [=============>................] - ETA: 0s
13728/26561 [==============>...............] - ETA: 0s
14240/26561 [===============>..............] - ETA: 0s
14496/26561 [===============>..............] - ETA: 0s
14816/26561 [===============>..............] - ETA: 0s
15488/26561 [================>.............] - ETA: 0s
16032/26561 [=================>............] - ETA: 0s
16864/26561 [==================>...........] - ETA: 0s
17696/26561 [==================>...........] - ETA: 0s
18528/26561 [===================>..........] - ETA: 0s
19360/26561 [====================>.........] - ETA: 0s
20192/26561 [=====================>........] - ETA: 0s
20960/26561 [======================>.......] - ETA: 0s
21792/26561 [=======================>......] - ETA: 0s
22624/26561 [========================>.....] - ETA: 0s
23360/26561 [=========================>....] - ETA: 0s
24128/26561 [==========================>...] - ETA: 0s
24896/26561 [===========================>..] - ETA: 0s
25664/26561 [===========================>..] - ETA: 0s
26400/26561 [============================>.] - ETA: 0s
26561/26561 [==============================] - 2s 70us/step

acc: 66.14%
NeuralNet Accuracy: 66.14% (61.59%) with {'loss': 'binary_crossentropy', 'optimizer': 'rmsprop', 'num_hidden': 1, 'activation': 'relu', 'batch_size': 64, 'val_score': 66.1392646987422, 'train_score': 67.87395052897105, 'max_epochs': 100, 'hidden_layer_width': 16}
MAX: 66.3991439313
=================================================
{   'activation': 'relu',
    'batch_size': 512,
    'hidden_layer_width': 16,
    'loss': 'binary_crossentropy',
    'max_epochs': 100,
    'num_hidden': 1,
    'optimizer': 'rmsprop'}

   32/13083 [..............................] - ETA: 1s
  800/13083 [>.............................] - ETA: 0s
 1504/13083 [==>...........................] - ETA: 0s
 2432/13083 [====>.........................] - ETA: 0s
 3360/13083 [======>.......................] - ETA: 0s
 4288/13083 [========>.....................] - ETA: 0s
 5216/13083 [==========>...................] - ETA: 0s
 6144/13083 [=============>................] - ETA: 0s
 7072/13083 [===============>..............] - ETA: 0s
 8000/13083 [=================>............] - ETA: 0s
 8896/13083 [===================>..........] - ETA: 0s
 9824/13083 [=====================>........] - ETA: 0s
10752/13083 [=======================>......] - ETA: 0s
11648/13083 [=========================>....] - ETA: 0s
12544/13083 [===========================>..] - ETA: 0s
13083/13083 [==============================] - 1s 57us/step

   32/26561 [..............................] - ETA: 1s
  896/26561 [>.............................] - ETA: 1s
 1728/26561 [>.............................] - ETA: 1s
 2592/26561 [=>............................] - ETA: 1s
 3488/26561 [==>...........................] - ETA: 1s
 4320/26561 [===>..........................] - ETA: 1s
 4832/26561 [====>.........................] - ETA: 1s
 5120/26561 [====>.........................] - ETA: 1s
 5408/26561 [=====>........................] - ETA: 1s
 5728/26561 [=====>........................] - ETA: 1s
 6080/26561 [=====>........................] - ETA: 1s
 6912/26561 [======>.......................] - ETA: 1s
 7328/26561 [=======>......................] - ETA: 1s
 7584/26561 [=======>......................] - ETA: 1s
 7872/26561 [=======>......................] - ETA: 1s
 8416/26561 [========>.....................] - ETA: 1s
 8928/26561 [=========>....................] - ETA: 1s
 9728/26561 [=========>....................] - ETA: 1s
10528/26561 [==========>...................] - ETA: 1s
11328/26561 [===========>..................] - ETA: 1s
12128/26561 [============>.................] - ETA: 1s
12960/26561 [=============>................] - ETA: 1s
13632/26561 [==============>...............] - ETA: 1s
14432/26561 [===============>..............] - ETA: 1s
15232/26561 [================>.............] - ETA: 0s
16000/26561 [=================>............] - ETA: 0s
16672/26561 [=================>............] - ETA: 0s
17408/26561 [==================>...........] - ETA: 0s
18272/26561 [===================>..........] - ETA: 0s
18912/26561 [====================>.........] - ETA: 0s
19808/26561 [=====================>........] - ETA: 0s
20736/26561 [======================>.......] - ETA: 0s
21664/26561 [=======================>......] - ETA: 0s
22592/26561 [========================>.....] - ETA: 0s
23520/26561 [=========================>....] - ETA: 0s
24416/26561 [==========================>...] - ETA: 0s
25312/26561 [===========================>..] - ETA: 0s
26240/26561 [============================>.] - ETA: 0s
26561/26561 [==============================] - 2s 72us/step

acc: 65.93%
NeuralNet Accuracy: 65.93% (61.52%) with {'loss': 'binary_crossentropy', 'optimizer': 'rmsprop', 'num_hidden': 1, 'activation': 'relu', 'batch_size': 512, 'val_score': 65.92524649808415, 'train_score': 67.59158164225745, 'max_epochs': 100, 'hidden_layer_width': 16}
MAX: 66.3991439313
=================================================
{   'activation': 'relu',
    'batch_size': 64,
    'hidden_layer_width': 32,
    'loss': 'binary_crossentropy',
    'max_epochs': 100,
    'num_hidden': 1,
    'optimizer': 'rmsprop'}

   32/13083 [..............................] - ETA: 1s
  896/13083 [=>............................] - ETA: 0s
 1792/13083 [===>..........................] - ETA: 0s
 2720/13083 [=====>........................] - ETA: 0s
 3648/13083 [=======>......................] - ETA: 0s
 4576/13083 [=========>....................] - ETA: 0s
 5504/13083 [===========>..................] - ETA: 0s
 6432/13083 [=============>................] - ETA: 0s
 7360/13083 [===============>..............] - ETA: 0s
 8288/13083 [==================>...........] - ETA: 0s
 9216/13083 [====================>.........] - ETA: 0s
10048/13083 [======================>.......] - ETA: 0s
10528/13083 [=======================>......] - ETA: 0s
10784/13083 [=======================>......] - ETA: 0s
11072/13083 [========================>.....] - ETA: 0s
11360/13083 [=========================>....] - ETA: 0s
11680/13083 [=========================>....] - ETA: 0s
12512/13083 [===========================>..] - ETA: 0s
12960/13083 [============================>.] - ETA: 0s
13083/13083 [==============================] - 1s 73us/step

   32/26561 [..............................] - ETA: 4s
  320/26561 [..............................] - ETA: 4s
  608/26561 [..............................] - ETA: 4s
 1280/26561 [>.............................] - ETA: 3s
 2016/26561 [=>............................] - ETA: 2s
 2848/26561 [==>...........................] - ETA: 2s
 3680/26561 [===>..........................] - ETA: 2s
 4512/26561 [====>.........................] - ETA: 1s
 5344/26561 [=====>........................] - ETA: 1s
 6016/26561 [=====>........................] - ETA: 1s
 6848/26561 [======>.......................] - ETA: 1s
 7680/26561 [=======>......................] - ETA: 1s
 8448/26561 [========>.....................] - ETA: 1s
 9152/26561 [=========>....................] - ETA: 1s
 9888/26561 [==========>...................] - ETA: 1s
10752/26561 [===========>..................] - ETA: 1s
11520/26561 [============>.................] - ETA: 1s
12288/26561 [============>.................] - ETA: 1s
13216/26561 [=============>................] - ETA: 0s
14144/26561 [==============>...............] - ETA: 0s
15072/26561 [================>.............] - ETA: 0s
16000/26561 [=================>............] - ETA: 0s
16928/26561 [==================>...........] - ETA: 0s
17856/26561 [===================>..........] - ETA: 0s
18784/26561 [====================>.........] - ETA: 0s
19712/26561 [=====================>........] - ETA: 0s
20640/26561 [======================>.......] - ETA: 0s
21568/26561 [=======================>......] - ETA: 0s
22496/26561 [========================>.....] - ETA: 0s
23424/26561 [=========================>....] - ETA: 0s
24352/26561 [==========================>...] - ETA: 0s
25280/26561 [===========================>..] - ETA: 0s
26208/26561 [============================>.] - ETA: 0s
26561/26561 [==============================] - 2s 63us/step

acc: 65.95%
NeuralNet Accuracy: 65.95% (61.57%) with {'loss': 'binary_crossentropy', 'optimizer': 'rmsprop', 'num_hidden': 1, 'activation': 'relu', 'batch_size': 64, 'val_score': 65.94817702771874, 'train_score': 68.60810963442641, 'max_epochs': 100, 'hidden_layer_width': 32}
MAX: 66.3991439313
=================================================
{   'activation': 'relu',
    'batch_size': 512,
    'hidden_layer_width': 32,
    'loss': 'binary_crossentropy',
    'max_epochs': 100,
    'num_hidden': 1,
    'optimizer': 'rmsprop'}

   32/13083 [..............................] - ETA: 2s
  352/13083 [..............................] - ETA: 2s
  640/13083 [>.............................] - ETA: 2s
  960/13083 [=>............................] - ETA: 2s
 1792/13083 [===>..........................] - ETA: 1s
 2272/13083 [====>.........................] - ETA: 1s
 2528/13083 [====>.........................] - ETA: 1s
 2816/13083 [=====>........................] - ETA: 1s
 3328/13083 [======>.......................] - ETA: 1s
 3872/13083 [=======>......................] - ETA: 1s
 4672/13083 [=========>....................] - ETA: 0s
 5472/13083 [===========>..................] - ETA: 0s
 6272/13083 [=============>................] - ETA: 0s
 7104/13083 [===============>..............] - ETA: 0s
 7936/13083 [=================>............] - ETA: 0s
 8608/13083 [==================>...........] - ETA: 0s
 9440/13083 [====================>.........] - ETA: 0s
10272/13083 [======================>.......] - ETA: 0s
11072/13083 [========================>.....] - ETA: 0s
11808/13083 [==========================>...] - ETA: 0s
12576/13083 [===========================>..] - ETA: 0s
13083/13083 [==============================] - 1s 81us/step

   32/26561 [..............................] - ETA: 1s
  800/26561 [..............................] - ETA: 1s
 1600/26561 [>.............................] - ETA: 1s
 2528/26561 [=>............................] - ETA: 1s
 3456/26561 [==>...........................] - ETA: 1s
 4384/26561 [===>..........................] - ETA: 1s
 5280/26561 [====>.........................] - ETA: 1s
 6176/26561 [=====>........................] - ETA: 1s
 7008/26561 [======>.......................] - ETA: 1s
 7872/26561 [=======>......................] - ETA: 1s
 8768/26561 [========>.....................] - ETA: 1s
 9696/26561 [=========>....................] - ETA: 0s
10624/26561 [==========>...................] - ETA: 0s
11552/26561 [============>.................] - ETA: 0s
12480/26561 [=============>................] - ETA: 0s
13408/26561 [==============>...............] - ETA: 0s
14336/26561 [===============>..............] - ETA: 0s
15264/26561 [================>.............] - ETA: 0s
16192/26561 [=================>............] - ETA: 0s
17120/26561 [==================>...........] - ETA: 0s
17792/26561 [===================>..........] - ETA: 0s
18080/26561 [===================>..........] - ETA: 0s
18368/26561 [===================>..........] - ETA: 0s
18688/26561 [====================>.........] - ETA: 0s
18976/26561 [====================>.........] - ETA: 0s
19712/26561 [=====================>........] - ETA: 0s
20352/26561 [=====================>........] - ETA: 0s
20608/26561 [======================>.......] - ETA: 0s
20896/26561 [======================>.......] - ETA: 0s
21280/26561 [=======================>......] - ETA: 0s
21888/26561 [=======================>......] - ETA: 0s
22656/26561 [========================>.....] - ETA: 0s
23488/26561 [=========================>....] - ETA: 0s
24320/26561 [==========================>...] - ETA: 0s
25152/26561 [===========================>..] - ETA: 0s
25984/26561 [============================>.] - ETA: 0s
26561/26561 [==============================] - 2s 69us/step

acc: 66.01%
NeuralNet Accuracy: 66.01% (61.67%) with {'loss': 'binary_crossentropy', 'optimizer': 'rmsprop', 'num_hidden': 1, 'activation': 'relu', 'batch_size': 512, 'val_score': 66.00932507470124, 'train_score': 68.14878957870563, 'max_epochs': 100, 'hidden_layer_width': 32}
MAX: 66.3991439313
=================================================
{   'activation': 'relu',
    'batch_size': 64,
    'hidden_layer_width': 64,
    'loss': 'binary_crossentropy',
    'max_epochs': 100,
    'num_hidden': 1,
    'optimizer': 'rmsprop'}

   32/13083 [..............................] - ETA: 1s
  768/13083 [>.............................] - ETA: 0s
 1632/13083 [==>...........................] - ETA: 0s
 2400/13083 [====>.........................] - ETA: 0s
 3232/13083 [======>.......................] - ETA: 0s
 4160/13083 [========>.....................] - ETA: 0s
 5088/13083 [==========>...................] - ETA: 0s
 6016/13083 [============>.................] - ETA: 0s
 6944/13083 [==============>...............] - ETA: 0s
 7872/13083 [=================>............] - ETA: 0s
 8800/13083 [===================>..........] - ETA: 0s
 9728/13083 [=====================>........] - ETA: 0s
10656/13083 [=======================>......] - ETA: 0s
11552/13083 [=========================>....] - ETA: 0s
12448/13083 [===========================>..] - ETA: 0s
13083/13083 [==============================] - 1s 58us/step

   32/26561 [..............................] - ETA: 1s
  960/26561 [>.............................] - ETA: 1s
 1888/26561 [=>............................] - ETA: 1s
 2816/26561 [==>...........................] - ETA: 1s
 3744/26561 [===>..........................] - ETA: 1s
 4672/26561 [====>.........................] - ETA: 1s
 5568/26561 [=====>........................] - ETA: 1s
 6272/26561 [======>.......................] - ETA: 1s
 6560/26561 [======>.......................] - ETA: 1s
 6848/26561 [======>.......................] - ETA: 1s
 7104/26561 [=======>......................] - ETA: 1s
 7392/26561 [=======>......................] - ETA: 1s
 7968/26561 [=======>......................] - ETA: 1s
 8800/26561 [========>.....................] - ETA: 1s
 9088/26561 [=========>....................] - ETA: 1s
 9376/26561 [=========>....................] - ETA: 1s
 9696/26561 [=========>....................] - ETA: 1s
10336/26561 [==========>...................] - ETA: 1s
11072/26561 [===========>..................] - ETA: 1s
11904/26561 [============>.................] - ETA: 1s
12736/26561 [=============>................] - ETA: 1s
13568/26561 [==============>...............] - ETA: 1s
14400/26561 [===============>..............] - ETA: 0s
15072/26561 [================>.............] - ETA: 0s
15904/26561 [================>.............] - ETA: 0s
16736/26561 [=================>............] - ETA: 0s
17536/26561 [==================>...........] - ETA: 0s
18240/26561 [===================>..........] - ETA: 0s
18976/26561 [====================>.........] - ETA: 0s
19808/26561 [=====================>........] - ETA: 0s
20544/26561 [======================>.......] - ETA: 0s
21248/26561 [======================>.......] - ETA: 0s
22176/26561 [========================>.....] - ETA: 0s
23104/26561 [=========================>....] - ETA: 0s
24032/26561 [==========================>...] - ETA: 0s
24928/26561 [===========================>..] - ETA: 0s
25856/26561 [============================>.] - ETA: 0s
26561/26561 [==============================] - 2s 71us/step

acc: 66.15%
NeuralNet Accuracy: 66.15% (61.91%) with {'loss': 'binary_crossentropy', 'optimizer': 'rmsprop', 'num_hidden': 1, 'activation': 'relu', 'batch_size': 64, 'val_score': 66.15455171242407, 'train_score': 68.69093784119575, 'max_epochs': 100, 'hidden_layer_width': 64}
MAX: 66.3991439313
=================================================
{   'activation': 'relu',
    'batch_size': 512,
    'hidden_layer_width': 64,
    'loss': 'binary_crossentropy',
    'max_epochs': 100,
    'num_hidden': 1,
    'optimizer': 'rmsprop'}

   32/13083 [..............................] - ETA: 1s
  480/13083 [>.............................] - ETA: 1s
  800/13083 [>.............................] - ETA: 1s
 1088/13083 [=>............................] - ETA: 1s
 1792/13083 [===>..........................] - ETA: 1s
 2336/13083 [====>.........................] - ETA: 1s
 3168/13083 [======>.......................] - ETA: 1s
 4000/13083 [========>.....................] - ETA: 0s
 4832/13083 [==========>...................] - ETA: 0s
 5664/13083 [===========>..................] - ETA: 0s
 6496/13083 [=============>................] - ETA: 0s
 7296/13083 [===============>..............] - ETA: 0s
 8128/13083 [=================>............] - ETA: 0s
 8960/13083 [===================>..........] - ETA: 0s
 9664/13083 [=====================>........] - ETA: 0s
10432/13083 [======================>.......] - ETA: 0s
11296/13083 [========================>.....] - ETA: 0s
12064/13083 [==========================>...] - ETA: 0s
12768/13083 [============================>.] - ETA: 0s
13083/13083 [==============================] - 1s 74us/step

   32/26561 [..............................] - ETA: 1s
  960/26561 [>.............................] - ETA: 1s
 1856/26561 [=>............................] - ETA: 1s
 2688/26561 [==>...........................] - ETA: 1s
 3552/26561 [===>..........................] - ETA: 1s
 4416/26561 [===>..........................] - ETA: 1s
 5280/26561 [====>.........................] - ETA: 1s
 6144/26561 [=====>........................] - ETA: 1s
 7008/26561 [======>.......................] - ETA: 1s
 7936/26561 [=======>......................] - ETA: 1s
 8864/26561 [=========>....................] - ETA: 1s
 9760/26561 [==========>...................] - ETA: 0s
10688/26561 [===========>..................] - ETA: 0s
11616/26561 [============>.................] - ETA: 0s
12512/26561 [=============>................] - ETA: 0s
13440/26561 [==============>...............] - ETA: 0s
14336/26561 [===============>..............] - ETA: 0s
15168/26561 [================>.............] - ETA: 0s
15712/26561 [================>.............] - ETA: 0s
16000/26561 [=================>............] - ETA: 0s
16288/26561 [=================>............] - ETA: 0s
16544/26561 [=================>............] - ETA: 0s
16864/26561 [==================>...........] - ETA: 0s
17664/26561 [==================>...........] - ETA: 0s
18144/26561 [===================>..........] - ETA: 0s
18464/26561 [===================>..........] - ETA: 0s
18752/26561 [====================>.........] - ETA: 0s
19264/26561 [====================>.........] - ETA: 0s
19808/26561 [=====================>........] - ETA: 0s
20640/26561 [======================>.......] - ETA: 0s
21472/26561 [=======================>......] - ETA: 0s
22304/26561 [========================>.....] - ETA: 0s
23168/26561 [=========================>....] - ETA: 0s
24032/26561 [==========================>...] - ETA: 0s
24736/26561 [==========================>...] - ETA: 0s
25600/26561 [===========================>..] - ETA: 0s
26464/26561 [============================>.] - ETA: 0s
26561/26561 [==============================] - 2s 70us/step

acc: 65.68%
NeuralNet Accuracy: 65.68% (62.05%) with {'loss': 'binary_crossentropy', 'optimizer': 'rmsprop', 'num_hidden': 1, 'activation': 'relu', 'batch_size': 512, 'val_score': 65.68065428054088, 'train_score': 69.51545499039946, 'max_epochs': 100, 'hidden_layer_width': 64}
MAX: 66.3991439313
=================================================
{   'activation': 'relu',
    'batch_size': 64,
    'hidden_layer_width': 16,
    'loss': 'binary_crossentropy',
    'max_epochs': 100,
    'num_hidden': 2,
    'optimizer': 'rmsprop'}

   32/13083 [..............................] - ETA: 1s
  864/13083 [>.............................] - ETA: 0s
 1760/13083 [===>..........................] - ETA: 0s
 2624/13083 [=====>........................] - ETA: 0s
 3520/13083 [=======>......................] - ETA: 0s
 4416/13083 [=========>....................] - ETA: 0s
 5312/13083 [===========>..................] - ETA: 0s
 6208/13083 [=============>................] - ETA: 0s
 7104/13083 [===============>..............] - ETA: 0s
 8000/13083 [=================>............] - ETA: 0s
 8896/13083 [===================>..........] - ETA: 0s
 9760/13083 [=====================>........] - ETA: 0s
10656/13083 [=======================>......] - ETA: 0s
11552/13083 [=========================>....] - ETA: 0s
12448/13083 [===========================>..] - ETA: 0s
12992/13083 [============================>.] - ETA: 0s
13083/13083 [==============================] - 1s 60us/step

   32/26561 [..............................] - ETA: 7s
  288/26561 [..............................] - ETA: 5s
  576/26561 [..............................] - ETA: 5s
  864/26561 [..............................] - ETA: 4s
 1216/26561 [>.............................] - ETA: 4s
 2048/26561 [=>............................] - ETA: 3s
 2464/26561 [=>............................] - ETA: 3s
 2720/26561 [==>...........................] - ETA: 3s
 3008/26561 [==>...........................] - ETA: 3s
 3616/26561 [===>..........................] - ETA: 3s
 4128/26561 [===>..........................] - ETA: 2s
 4896/26561 [====>.........................] - ETA: 2s
 5664/26561 [=====>........................] - ETA: 2s
 6432/26561 [======>.......................] - ETA: 2s
 7200/26561 [=======>......................] - ETA: 1s
 8000/26561 [========>.....................] - ETA: 1s
 8672/26561 [========>.....................] - ETA: 1s
 9504/26561 [=========>....................] - ETA: 1s
10336/26561 [==========>...................] - ETA: 1s
11104/26561 [===========>..................] - ETA: 1s
11808/26561 [============>.................] - ETA: 1s
12512/26561 [=============>................] - ETA: 1s
13344/26561 [==============>...............] - ETA: 1s
13984/26561 [==============>...............] - ETA: 1s
14848/26561 [===============>..............] - ETA: 0s
15744/26561 [================>.............] - ETA: 0s
16640/26561 [=================>............] - ETA: 0s
17536/26561 [==================>...........] - ETA: 0s
18432/26561 [===================>..........] - ETA: 0s
19328/26561 [====================>.........] - ETA: 0s
20224/26561 [=====================>........] - ETA: 0s
21120/26561 [======================>.......] - ETA: 0s
22016/26561 [=======================>......] - ETA: 0s
22912/26561 [========================>.....] - ETA: 0s
23808/26561 [=========================>....] - ETA: 0s
24672/26561 [==========================>...] - ETA: 0s
25536/26561 [===========================>..] - ETA: 0s
26400/26561 [============================>.] - ETA: 0s
26561/26561 [==============================] - 2s 72us/step

acc: 66.14%
NeuralNet Accuracy: 66.14% (61.83%) with {'loss': 'binary_crossentropy', 'optimizer': 'rmsprop', 'num_hidden': 2, 'activation': 'relu', 'batch_size': 64, 'val_score': 66.1392646987422, 'train_score': 67.88901020292911, 'max_epochs': 100, 'hidden_layer_width': 16}
MAX: 66.3991439313
=================================================
{   'activation': 'relu',
    'batch_size': 512,
    'hidden_layer_width': 16,
    'loss': 'binary_crossentropy',
    'max_epochs': 100,
    'num_hidden': 2,
    'optimizer': 'rmsprop'}

   32/13083 [..............................] - ETA: 0s
  928/13083 [=>............................] - ETA: 0s
 1824/13083 [===>..........................] - ETA: 0s
 2720/13083 [=====>........................] - ETA: 0s
 3616/13083 [=======>......................] - ETA: 0s
 4512/13083 [=========>....................] - ETA: 0s
 5408/13083 [===========>..................] - ETA: 0s
 6272/13083 [=============>................] - ETA: 0s
 7168/13083 [===============>..............] - ETA: 0s
 7808/13083 [================>.............] - ETA: 0s
 8096/13083 [=================>............] - ETA: 0s
 8352/13083 [==================>...........] - ETA: 0s
 8608/13083 [==================>...........] - ETA: 0s
 8896/13083 [===================>..........] - ETA: 0s
 9568/13083 [====================>.........] - ETA: 0s
10208/13083 [======================>.......] - ETA: 0s
10464/13083 [======================>.......] - ETA: 0s
10688/13083 [=======================>......] - ETA: 0s
11200/13083 [========================>.....] - ETA: 0s
11712/13083 [=========================>....] - ETA: 0s
12512/13083 [===========================>..] - ETA: 0s
13083/13083 [==============================] - 1s 82us/step

   32/26561 [..............................] - ETA: 1s
  864/26561 [..............................] - ETA: 1s
 1696/26561 [>.............................] - ETA: 1s
 2528/26561 [=>............................] - ETA: 1s
 3200/26561 [==>...........................] - ETA: 1s
 4032/26561 [===>..........................] - ETA: 1s
 4864/26561 [====>.........................] - ETA: 1s
 5632/26561 [=====>........................] - ETA: 1s
 6336/26561 [======>.......................] - ETA: 1s
 7040/26561 [======>.......................] - ETA: 1s
 7872/26561 [=======>......................] - ETA: 1s
 8480/26561 [========>.....................] - ETA: 1s
 9344/26561 [=========>....................] - ETA: 1s
10240/26561 [==========>...................] - ETA: 1s
11136/26561 [===========>..................] - ETA: 0s
12032/26561 [============>.................] - ETA: 0s
12896/26561 [=============>................] - ETA: 0s
13792/26561 [==============>...............] - ETA: 0s
14688/26561 [===============>..............] - ETA: 0s
15584/26561 [================>.............] - ETA: 0s
16480/26561 [=================>............] - ETA: 0s
17344/26561 [==================>...........] - ETA: 0s
18240/26561 [===================>..........] - ETA: 0s
19136/26561 [====================>.........] - ETA: 0s
20032/26561 [=====================>........] - ETA: 0s
20928/26561 [======================>.......] - ETA: 0s
21824/26561 [=======================>......] - ETA: 0s
22720/26561 [========================>.....] - ETA: 0s
23552/26561 [=========================>....] - ETA: 0s
24448/26561 [==========================>...] - ETA: 0s
24992/26561 [===========================>..] - ETA: 0s
25280/26561 [===========================>..] - ETA: 0s
25568/26561 [===========================>..] - ETA: 0s
25824/26561 [============================>.] - ETA: 0s
26144/26561 [============================>.] - ETA: 0s
26561/26561 [==============================] - 2s 67us/step

acc: 65.04%
NeuralNet Accuracy: 65.04% (62.11%) with {'loss': 'binary_crossentropy', 'optimizer': 'rmsprop', 'num_hidden': 2, 'activation': 'relu', 'batch_size': 512, 'val_score': 65.03859971364705, 'train_score': 67.07578780919393, 'max_epochs': 100, 'hidden_layer_width': 16}
MAX: 66.3991439313
=================================================
{   'activation': 'relu',
    'batch_size': 64,
    'hidden_layer_width': 32,
    'loss': 'binary_crossentropy',
    'max_epochs': 100,
    'num_hidden': 2,
    'optimizer': 'rmsprop'}

   32/13083 [..............................] - ETA: 1s
  928/13083 [=>............................] - ETA: 0s
 1824/13083 [===>..........................] - ETA: 0s
 2720/13083 [=====>........................] - ETA: 0s
 3616/13083 [=======>......................] - ETA: 0s
 4512/13083 [=========>....................] - ETA: 0s
 5408/13083 [===========>..................] - ETA: 0s
 6304/13083 [=============>................] - ETA: 0s
 7168/13083 [===============>..............] - ETA: 0s
 8064/13083 [=================>............] - ETA: 0s
 8960/13083 [===================>..........] - ETA: 0s
 9856/13083 [=====================>........] - ETA: 0s
10752/13083 [=======================>......] - ETA: 0s
11648/13083 [=========================>....] - ETA: 0s
12544/13083 [===========================>..] - ETA: 0s
13083/13083 [==============================] - 1s 57us/step

   32/26561 [..............................] - ETA: 4s
  576/26561 [..............................] - ETA: 2s
  864/26561 [..............................] - ETA: 3s
 1152/26561 [>.............................] - ETA: 3s
 1408/26561 [>.............................] - ETA: 3s
 1728/26561 [>.............................] - ETA: 3s
 2464/26561 [=>............................] - ETA: 3s
 2976/26561 [==>...........................] - ETA: 2s
 3264/26561 [==>...........................] - ETA: 3s
 3520/26561 [==>...........................] - ETA: 3s
 4032/26561 [===>..........................] - ETA: 2s
 4576/26561 [====>.........................] - ETA: 2s
 5376/26561 [=====>........................] - ETA: 2s
 6144/26561 [=====>........................] - ETA: 2s
 6944/26561 [======>.......................] - ETA: 2s
 7744/26561 [=======>......................] - ETA: 1s
 8576/26561 [========>.....................] - ETA: 1s
 9248/26561 [=========>....................] - ETA: 1s
10080/26561 [==========>...................] - ETA: 1s
10912/26561 [===========>..................] - ETA: 1s
11680/26561 [============>.................] - ETA: 1s
12384/26561 [============>.................] - ETA: 1s
13120/26561 [=============>................] - ETA: 1s
13952/26561 [==============>...............] - ETA: 1s
14624/26561 [===============>..............] - ETA: 1s
15520/26561 [================>.............] - ETA: 0s
16384/26561 [=================>............] - ETA: 0s
17280/26561 [==================>...........] - ETA: 0s
18176/26561 [===================>..........] - ETA: 0s
19072/26561 [====================>.........] - ETA: 0s
19968/26561 [=====================>........] - ETA: 0s
20864/26561 [======================>.......] - ETA: 0s
21760/26561 [=======================>......] - ETA: 0s
22624/26561 [========================>.....] - ETA: 0s
23520/26561 [=========================>....] - ETA: 0s
24416/26561 [==========================>...] - ETA: 0s
25312/26561 [===========================>..] - ETA: 0s
26208/26561 [============================>.] - ETA: 0s
26561/26561 [==============================] - 2s 73us/step

acc: 66.06%
NeuralNet Accuracy: 66.06% (61.72%) with {'loss': 'binary_crossentropy', 'optimizer': 'rmsprop', 'num_hidden': 2, 'activation': 'relu', 'batch_size': 64, 'val_score': 66.06282962577693, 'train_score': 68.46127781333534, 'max_epochs': 100, 'hidden_layer_width': 32}
MAX: 66.3991439313
=================================================
{   'activation': 'relu',
    'batch_size': 512,
    'hidden_layer_width': 32,
    'loss': 'binary_crossentropy',
    'max_epochs': 100,
    'num_hidden': 2,
    'optimizer': 'rmsprop'}

   32/13083 [..............................] - ETA: 1s
  736/13083 [>.............................] - ETA: 0s
  992/13083 [=>............................] - ETA: 1s
 1216/13083 [=>............................] - ETA: 1s
 1504/13083 [==>...........................] - ETA: 1s
 2144/13083 [===>..........................] - ETA: 1s
 2816/13083 [=====>........................] - ETA: 1s
 3616/13083 [=======>......................] - ETA: 0s
 4448/13083 [=========>....................] - ETA: 0s
 5280/13083 [===========>..................] - ETA: 0s
 6112/13083 [=============>................] - ETA: 0s
 6784/13083 [==============>...............] - ETA: 0s
 7616/13083 [================>.............] - ETA: 0s
 8448/13083 [==================>...........] - ETA: 0s
 9216/13083 [====================>.........] - ETA: 0s
 9888/13083 [=====================>........] - ETA: 0s
10624/13083 [=======================>......] - ETA: 0s
11456/13083 [=========================>....] - ETA: 0s
12064/13083 [==========================>...] - ETA: 0s
12928/13083 [============================>.] - ETA: 0s
13083/13083 [==============================] - 1s 76us/step

   32/26561 [..............................] - ETA: 2s
  896/26561 [>.............................] - ETA: 1s
 1792/26561 [=>............................] - ETA: 1s
 2656/26561 [=>............................] - ETA: 1s
 3552/26561 [===>..........................] - ETA: 1s
 4448/26561 [====>.........................] - ETA: 1s
 5344/26561 [=====>........................] - ETA: 1s
 6240/26561 [======>.......................] - ETA: 1s
 7136/26561 [=======>......................] - ETA: 1s
 8032/26561 [========>.....................] - ETA: 1s
 8928/26561 [=========>....................] - ETA: 1s
 9824/26561 [==========>...................] - ETA: 0s
10688/26561 [===========>..................] - ETA: 0s
11584/26561 [============>.................] - ETA: 0s
12480/26561 [=============>................] - ETA: 0s
13344/26561 [==============>...............] - ETA: 0s
14176/26561 [===============>..............] - ETA: 0s
14912/26561 [===============>..............] - ETA: 0s
15200/26561 [================>.............] - ETA: 0s
15456/26561 [================>.............] - ETA: 0s
15744/26561 [================>.............] - ETA: 0s
16032/26561 [=================>............] - ETA: 0s
16704/26561 [=================>............] - ETA: 0s
17504/26561 [==================>...........] - ETA: 0s
17760/26561 [===================>..........] - ETA: 0s
18080/26561 [===================>..........] - ETA: 0s
18400/26561 [===================>..........] - ETA: 0s
19072/26561 [====================>.........] - ETA: 0s
19776/26561 [=====================>........] - ETA: 0s
20576/26561 [======================>.......] - ETA: 0s
21376/26561 [=======================>......] - ETA: 0s
22176/26561 [========================>.....] - ETA: 0s
22976/26561 [========================>.....] - ETA: 0s
23648/26561 [=========================>....] - ETA: 0s
24480/26561 [==========================>...] - ETA: 0s
25312/26561 [===========================>..] - ETA: 0s
26080/26561 [============================>.] - ETA: 0s
26561/26561 [==============================] - 2s 71us/step

acc: 65.75%
NeuralNet Accuracy: 65.75% (62.12%) with {'loss': 'binary_crossentropy', 'optimizer': 'rmsprop', 'num_hidden': 2, 'activation': 'relu', 'batch_size': 512, 'val_score': 65.74944584210932, 'train_score': 68.28056172583864, 'max_epochs': 100, 'hidden_layer_width': 32}
MAX: 66.3991439313
=================================================
{   'activation': 'relu',
    'batch_size': 64,
    'hidden_layer_width': 64,
    'loss': 'binary_crossentropy',
    'max_epochs': 100,
    'num_hidden': 2,
    'optimizer': 'rmsprop'}

   32/13083 [..............................] - ETA: 1s
  864/13083 [>.............................] - ETA: 0s
 1696/13083 [==>...........................] - ETA: 0s
 2496/13083 [====>.........................] - ETA: 0s
 3008/13083 [=====>........................] - ETA: 0s
 3264/13083 [======>.......................] - ETA: 0s
 3552/13083 [=======>......................] - ETA: 0s
 3872/13083 [=======>......................] - ETA: 0s
 4192/13083 [========>.....................] - ETA: 0s
 4992/13083 [==========>...................] - ETA: 0s
 5504/13083 [===========>..................] - ETA: 0s
 5760/13083 [============>.................] - ETA: 0s
 6016/13083 [============>.................] - ETA: 0s
 6688/13083 [==============>...............] - ETA: 0s
 7200/13083 [===============>..............] - ETA: 0s
 8000/13083 [=================>............] - ETA: 0s
 8800/13083 [===================>..........] - ETA: 0s
 9600/13083 [=====================>........] - ETA: 0s
10400/13083 [======================>.......] - ETA: 0s
11200/13083 [========================>.....] - ETA: 0s
11840/13083 [==========================>...] - ETA: 0s
12640/13083 [===========================>..] - ETA: 0s
13083/13083 [==============================] - 1s 86us/step

   32/26561 [..............................] - ETA: 1s
  768/26561 [..............................] - ETA: 1s
 1408/26561 [>.............................] - ETA: 1s
 2144/26561 [=>............................] - ETA: 1s
 2976/26561 [==>...........................] - ETA: 1s
 3680/26561 [===>..........................] - ETA: 1s
 4512/26561 [====>.........................] - ETA: 1s
 5376/26561 [=====>........................] - ETA: 1s
 6240/26561 [======>.......................] - ETA: 1s
 7104/26561 [=======>......................] - ETA: 1s
 8000/26561 [========>.....................] - ETA: 1s
 8896/26561 [=========>....................] - ETA: 1s
 9792/26561 [==========>...................] - ETA: 1s
10656/26561 [===========>..................] - ETA: 1s
11552/26561 [============>.................] - ETA: 0s
12416/26561 [=============>................] - ETA: 0s
13312/26561 [==============>...............] - ETA: 0s
14176/26561 [===============>..............] - ETA: 0s
15072/26561 [================>.............] - ETA: 0s
15968/26561 [=================>............] - ETA: 0s
16864/26561 [==================>...........] - ETA: 0s
17760/26561 [===================>..........] - ETA: 0s
18624/26561 [====================>.........] - ETA: 0s
19520/26561 [=====================>........] - ETA: 0s
20064/26561 [=====================>........] - ETA: 0s
20352/26561 [=====================>........] - ETA: 0s
20608/26561 [======================>.......] - ETA: 0s
20896/26561 [======================>.......] - ETA: 0s
21216/26561 [======================>.......] - ETA: 0s
21984/26561 [=======================>......] - ETA: 0s
22528/26561 [========================>.....] - ETA: 0s
22816/26561 [========================>.....] - ETA: 0s
23136/26561 [=========================>....] - ETA: 0s
23648/26561 [=========================>....] - ETA: 0s
24160/26561 [==========================>...] - ETA: 0s
24928/26561 [===========================>..] - ETA: 0s
25728/26561 [============================>.] - ETA: 0s
26528/26561 [============================>.] - ETA: 0s
26561/26561 [==============================] - 2s 72us/step

acc: 65.34%
NeuralNet Accuracy: 65.34% (62.15%) with {'loss': 'binary_crossentropy', 'optimizer': 'rmsprop', 'num_hidden': 2, 'activation': 'relu', 'batch_size': 64, 'val_score': 65.33669648044365, 'train_score': 69.3723880877979, 'max_epochs': 100, 'hidden_layer_width': 64}
MAX: 66.3991439313
=================================================
{   'activation': 'relu',
    'batch_size': 512,
    'hidden_layer_width': 64,
    'loss': 'binary_crossentropy',
    'max_epochs': 100,
    'num_hidden': 2,
    'optimizer': 'rmsprop'}

   32/13083 [..............................] - ETA: 1s
  832/13083 [>.............................] - ETA: 0s
 1632/13083 [==>...........................] - ETA: 0s
 2368/13083 [====>.........................] - ETA: 0s
 3168/13083 [======>.......................] - ETA: 0s
 4000/13083 [========>.....................] - ETA: 0s
 4832/13083 [==========>...................] - ETA: 0s
 5504/13083 [===========>..................] - ETA: 0s
 6240/13083 [=============>................] - ETA: 0s
 7072/13083 [===============>..............] - ETA: 0s
 7808/13083 [================>.............] - ETA: 0s
 8576/13083 [==================>...........] - ETA: 0s
 9472/13083 [====================>.........] - ETA: 0s
10336/13083 [======================>.......] - ETA: 0s
11232/13083 [========================>.....] - ETA: 0s
12064/13083 [==========================>...] - ETA: 0s
12896/13083 [============================>.] - ETA: 0s
13083/13083 [==============================] - 1s 64us/step

   32/26561 [..............................] - ETA: 1s
  864/26561 [..............................] - ETA: 1s
 1696/26561 [>.............................] - ETA: 1s
 2592/26561 [=>............................] - ETA: 1s
 3488/26561 [==>...........................] - ETA: 1s
 4384/26561 [===>..........................] - ETA: 1s
 5280/26561 [====>.........................] - ETA: 1s
 6176/26561 [=====>........................] - ETA: 1s
 7072/26561 [======>.......................] - ETA: 1s
 7968/26561 [=======>......................] - ETA: 1s
 8864/26561 [=========>....................] - ETA: 1s
 9760/26561 [==========>...................] - ETA: 0s
10560/26561 [==========>...................] - ETA: 0s
11040/26561 [===========>..................] - ETA: 0s
11296/26561 [===========>..................] - ETA: 0s
11584/26561 [============>.................] - ETA: 1s
11840/26561 [============>.................] - ETA: 1s
12160/26561 [============>.................] - ETA: 1s
12960/26561 [=============>................] - ETA: 0s
13408/26561 [==============>...............] - ETA: 0s
13632/26561 [==============>...............] - ETA: 0s
13888/26561 [==============>...............] - ETA: 0s
14464/26561 [===============>..............] - ETA: 0s
14976/26561 [===============>..............] - ETA: 0s
15744/26561 [================>.............] - ETA: 0s
16544/26561 [=================>............] - ETA: 0s
17344/26561 [==================>...........] - ETA: 0s
18144/26561 [===================>..........] - ETA: 0s
18944/26561 [====================>.........] - ETA: 0s
19616/26561 [=====================>........] - ETA: 0s
20416/26561 [======================>.......] - ETA: 0s
21248/26561 [======================>.......] - ETA: 0s
22016/26561 [=======================>......] - ETA: 0s
22688/26561 [========================>.....] - ETA: 0s
23392/26561 [=========================>....] - ETA: 0s
24192/26561 [==========================>...] - ETA: 0s
24864/26561 [===========================>..] - ETA: 0s
25728/26561 [============================>.] - ETA: 0s
26560/26561 [============================>.] - ETA: 0s
26561/26561 [==============================] - 2s 74us/step

acc: 65.54%
NeuralNet Accuracy: 65.54% (61.88%) with {'loss': 'binary_crossentropy', 'optimizer': 'rmsprop', 'num_hidden': 2, 'activation': 'relu', 'batch_size': 512, 'val_score': 65.53542765830804, 'train_score': 68.92059786905614, 'max_epochs': 100, 'hidden_layer_width': 64}
MAX: 66.3991439313
=================================================
{   'activation': 'relu',
    'batch_size': 64,
    'hidden_layer_width': 16,
    'loss': 'binary_crossentropy',
    'max_epochs': 100,
    'num_hidden': 3,
    'optimizer': 'rmsprop'}

   32/13083 [..............................] - ETA: 1s
  896/13083 [=>............................] - ETA: 0s
 1760/13083 [===>..........................] - ETA: 0s
 2592/13083 [====>.........................] - ETA: 0s
 3456/13083 [======>.......................] - ETA: 0s
 4224/13083 [========>.....................] - ETA: 0s
 4576/13083 [=========>....................] - ETA: 0s
 4832/13083 [==========>...................] - ETA: 0s
 5088/13083 [==========>...................] - ETA: 0s
 5344/13083 [===========>..................] - ETA: 0s
 5792/13083 [============>.................] - ETA: 0s
 6560/13083 [==============>...............] - ETA: 0s
 6816/13083 [==============>...............] - ETA: 0s
 7072/13083 [===============>..............] - ETA: 0s
 7296/13083 [===============>..............] - ETA: 0s
 8000/13083 [=================>............] - ETA: 0s
 8704/13083 [==================>...........] - ETA: 0s
 9504/13083 [====================>.........] - ETA: 0s
10304/13083 [======================>.......] - ETA: 0s
11104/13083 [========================>.....] - ETA: 0s
11904/13083 [==========================>...] - ETA: 0s
12544/13083 [===========================>..] - ETA: 0s
13083/13083 [==============================] - 1s 87us/step

   32/26561 [..............................] - ETA: 1s
  832/26561 [..............................] - ETA: 1s
 1632/26561 [>.............................] - ETA: 1s
 2336/26561 [=>............................] - ETA: 1s
 3040/26561 [==>...........................] - ETA: 1s
 3840/26561 [===>..........................] - ETA: 1s
 4544/26561 [====>.........................] - ETA: 1s
 5216/26561 [====>.........................] - ETA: 1s
 6080/26561 [=====>........................] - ETA: 1s
 6944/26561 [======>.......................] - ETA: 1s
 7808/26561 [=======>......................] - ETA: 1s
 8672/26561 [========>.....................] - ETA: 1s
 9536/26561 [=========>....................] - ETA: 1s
10400/26561 [==========>...................] - ETA: 1s
11264/26561 [===========>..................] - ETA: 0s
12128/26561 [============>.................] - ETA: 0s
12992/26561 [=============>................] - ETA: 0s
13856/26561 [==============>...............] - ETA: 0s
14720/26561 [===============>..............] - ETA: 0s
15584/26561 [================>.............] - ETA: 0s
16448/26561 [=================>............] - ETA: 0s
17312/26561 [==================>...........] - ETA: 0s
18176/26561 [===================>..........] - ETA: 0s
19040/26561 [====================>.........] - ETA: 0s
19904/26561 [=====================>........] - ETA: 0s
20672/26561 [======================>.......] - ETA: 0s
20960/26561 [======================>.......] - ETA: 0s
21248/26561 [======================>.......] - ETA: 0s
21568/26561 [=======================>......] - ETA: 0s
21856/26561 [=======================>......] - ETA: 0s
22496/26561 [========================>.....] - ETA: 0s
23136/26561 [=========================>....] - ETA: 0s
23424/26561 [=========================>....] - ETA: 0s
23680/26561 [=========================>....] - ETA: 0s
24064/26561 [==========================>...] - ETA: 0s
24672/26561 [==========================>...] - ETA: 0s
25376/26561 [===========================>..] - ETA: 0s
26144/26561 [============================>.] - ETA: 0s
26561/26561 [==============================] - 2s 73us/step
